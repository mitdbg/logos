{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LOGos","text":"<p>Utilizing system logs to perform causal analysis.</p>"},{"location":"#demo","title":"Demo","text":"<p>You can find a quick demo of the LOGos API at demo.ipynb. </p>"},{"location":"#documentation","title":"Documentation","text":"<p>To view the documentation, run <code>mkdocs serve</code> from the root of this repo and open the corresponding page. </p> <p>You might need to install the following packages: <code>pip install mkdocs-material mkdocs-gen-files mkdocs-literate-nav markdown_include pymdown-extensions markdown mkdocs-pymdownx Pygments mkdocs-jupyter mkdocstrings-python mkdocstrings mdx_include</code></p>"},{"location":"#openai-integration","title":"OpenAI integration","text":"<p>Yo use the LLM-powered capabilites of LOGos, please add a <code>.env</code> file to the root of this repo and define <code>OPENAI_API_KEY</code> appropriately.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>logos<ul> <li>aggimp<ul> <li>agg_funcs</li> <li>imp_funcs</li> </ul> </li> <li>aggregate_selector</li> <li>ate_calculator</li> <li>candidate_cause_ranker</li> <li>causal_discoverer</li> <li>causal_unit_suggester</li> <li>clustering_params</li> <li>drain</li> <li>edge_occurrence_tree</li> <li>edge_state_matrix</li> <li>graph_renderer</li> <li>interactive_causal_graph_refiner</li> <li>logos</li> <li>pickler</li> <li>printer</li> <li>pruner</li> <li>regression</li> <li>tag_utils</li> <li>types</li> <li>variable_name<ul> <li>parsed_variable_name</li> <li>prepared_variable_name</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/logos/","title":"Index","text":""},{"location":"reference/logos/aggregate_selector/","title":"AggregateSelector","text":""},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.AggregateSelector","title":"<code>AggregateSelector</code>","text":"Source code in <code>src/logos/aggregate_selector.py</code> <pre><code>class AggregateSelector:\n    DEFAULT_AGGREGATES = {\n        \"num\": [\n            \"mean\",\n            \"max\",\n            \"min\",\n        ],\n        \"str\": [\n            \"last\",\n            \"mode\",\n            \"first\",\n        ],\n    }\n\n    def _entropy(col: pd.Series) -&gt; float:\n        \"\"\"\n        Calculates the entropy of a column.\n\n        Parameters:\n            col: The column for which to calculate the entropy.\n\n        Returns:\n            The entropy of `col`.\n        \"\"\"\n\n        rel_value_counts = col.value_counts(normalize=True)\n        if rel_value_counts.empty:\n            return 0\n        return -np.sum(rel_value_counts * np.log2(rel_value_counts))\n\n    def find_uninformative_aggregates(\n        prepared_log: pd.DataFrame, parsed_variables: pd.DataFrame, causal_unit_var: str\n    ) -&gt; list[str]:\n        \"\"\"\n        Find aggregates that are uninformative for each column in `prepared_log`.\n        Aggregates are uninformative unless they maximize the empirical entropy across causal units.\n\n        Parameters:\n            prepared_log: The prepared log.\n            parsed_variables: The parsed variables.\n            causal_unit_var: The name of the causal unit variable.\n\n        Returns:\n            A list of uninformative aggregates for `prepared_log`.\n        \"\"\"\n\n        drop_list = []\n\n        for row in parsed_variables.itertuples():\n            aggs = row.Aggregates\n            if len(aggs) == 0 or row.Name == causal_unit_var:\n                continue\n\n            vars = [f\"{row.Name}+{agg}\" for agg in aggs]\n            best_var = f\"{row.Name}+{AggregateSelector.DEFAULT_AGGREGATES[row.Type][0]}\"\n            max_entropy = -np.inf\n\n            for var in vars:\n                entropy = AggregateSelector._entropy(prepared_log[var])\n\n                if entropy &gt; max_entropy:\n                    best_var = var\n                    max_entropy = entropy\n\n            drop_list.extend([var for var in vars if var != best_var])\n\n        return drop_list\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.AggregateSelector._entropy","title":"<code>_entropy(col)</code>","text":"<p>Calculates the entropy of a column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column for which to calculate the entropy.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The entropy of <code>col</code>.</p> Source code in <code>src/logos/aggregate_selector.py</code> <pre><code>def _entropy(col: pd.Series) -&gt; float:\n    \"\"\"\n    Calculates the entropy of a column.\n\n    Parameters:\n        col: The column for which to calculate the entropy.\n\n    Returns:\n        The entropy of `col`.\n    \"\"\"\n\n    rel_value_counts = col.value_counts(normalize=True)\n    if rel_value_counts.empty:\n        return 0\n    return -np.sum(rel_value_counts * np.log2(rel_value_counts))\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.AggregateSelector.find_uninformative_aggregates","title":"<code>find_uninformative_aggregates(prepared_log, parsed_variables, causal_unit_var)</code>","text":"<p>Find aggregates that are uninformative for each column in <code>prepared_log</code>. Aggregates are uninformative unless they maximize the empirical entropy across causal units.</p> <p>Parameters:</p> Name Type Description Default <code>prepared_log</code> <code>DataFrame</code> <p>The prepared log.</p> required <code>parsed_variables</code> <code>DataFrame</code> <p>The parsed variables.</p> required <code>causal_unit_var</code> <code>str</code> <p>The name of the causal unit variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of uninformative aggregates for <code>prepared_log</code>.</p> Source code in <code>src/logos/aggregate_selector.py</code> <pre><code>def find_uninformative_aggregates(\n    prepared_log: pd.DataFrame, parsed_variables: pd.DataFrame, causal_unit_var: str\n) -&gt; list[str]:\n    \"\"\"\n    Find aggregates that are uninformative for each column in `prepared_log`.\n    Aggregates are uninformative unless they maximize the empirical entropy across causal units.\n\n    Parameters:\n        prepared_log: The prepared log.\n        parsed_variables: The parsed variables.\n        causal_unit_var: The name of the causal unit variable.\n\n    Returns:\n        A list of uninformative aggregates for `prepared_log`.\n    \"\"\"\n\n    drop_list = []\n\n    for row in parsed_variables.itertuples():\n        aggs = row.Aggregates\n        if len(aggs) == 0 or row.Name == causal_unit_var:\n            continue\n\n        vars = [f\"{row.Name}+{agg}\" for agg in aggs]\n        best_var = f\"{row.Name}+{AggregateSelector.DEFAULT_AGGREGATES[row.Type][0]}\"\n        max_entropy = -np.inf\n\n        for var in vars:\n            entropy = AggregateSelector._entropy(prepared_log[var])\n\n            if entropy &gt; max_entropy:\n                best_var = var\n                max_entropy = entropy\n\n        drop_list.extend([var for var in vars if var != best_var])\n\n    return drop_list\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.mean","title":"<code>mean(x)</code>","text":"<p>Calculates the mean of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mean will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mean of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def mean(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mean of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mean will be calculated.\n\n    Returns:\n        The mean of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mean(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.min","title":"<code>min(x)</code>","text":"<p>Calculates the minimum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the minimum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The minimum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def min(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the minimum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the minimum will be calculated.\n\n    Returns:\n        The minimum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.min(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.max","title":"<code>max(x)</code>","text":"<p>Calculates the maximum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the maximum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The maximum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def max(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the maximum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the maximum will be calculated.\n\n    Returns:\n        The maximum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.max(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.median","title":"<code>median(x)</code>","text":"<p>Calculates the median of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the median will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The median of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def median(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the median of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the median will be calculated.\n\n    Returns:\n        The median of the series, or None if the series is all NA.\n    \"\"\"\n    return x.median(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.mode","title":"<code>mode(x)</code>","text":"<p>Calculates the mode of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mode will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mode of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def mode(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mode of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mode will be calculated.\n\n    Returns:\n        The mode of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mode(dropna=True)[0] if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.std","title":"<code>std(x)</code>","text":"<p>Calculates the standard deviation of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the standard deviation will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The standard deviation of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def std(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the standard deviation of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the standard deviation will be calculated.\n\n    Returns:\n        The standard deviation of the series, or None if the series is all NA.\n    \"\"\"\n    return x.std(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.last","title":"<code>last(x)</code>","text":"<p>Returns the last non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the last non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The last non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def last(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the last non-NA value in a series.\n\n    Parameters:\n        x: The series for which the last non-NA value will be returned.\n\n    Returns:\n        The last non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().tail(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.first","title":"<code>first(x)</code>","text":"<p>Returns the first non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the first non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The first non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def first(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the first non-NA value in a series.\n\n    Parameters:\n        x: The series for which the first non-NA value will be returned.\n\n    Returns:\n        The first non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().head(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggregate_selector/#logos.aggregate_selector.sum","title":"<code>sum(x)</code>","text":"<p>Calculates the sum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the sum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The sum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def sum(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the sum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the sum will be calculated.\n\n    Returns:\n        The sum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.sum(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/ate_calculator/","title":"ATECalculator","text":""},{"location":"reference/logos/ate_calculator/#logos.ate_calculator.ATECalculator","title":"<code>ATECalculator</code>","text":"<p>A class to calculate ATEs and determine the impact of adding/removing/reversing DAG edges on these calculations.</p> Source code in <code>src/logos/ate_calculator.py</code> <pre><code>class ATECalculator:\n    \"\"\"\n    A class to calculate ATEs and determine the impact of adding/removing/reversing DAG edges\n    on these calculations.\n    \"\"\"\n\n    @staticmethod\n    def get_ate_and_confidence(\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        treatment: str,\n        outcome: str,\n        confounder: Optional[str] = None,\n        graph: Optional[nx.DiGraph] = None,\n        calculate_p_value: bool = True,\n        calculate_std_error: bool = True,\n        get_estimand: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Calculate the ATE of `treatment` on `outcome`, alongside confidence measures.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n                causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n            graph: The graph to be used for causal analysis. If not specified, a two-node graph with just\n                `treatment` and `outcome` is used.\n            calculate_p_value: Whether to calculate the P-value of the ATE.\n            calculate_std_error: Whether to calculate the standard error of the ATE.\n            get_estimand: Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.\n\n        Returns:\n            A dictionary containing the ATE of `treatment` on `outcome`, alongside confidence measures. If\n            `get_estimand` is True, the estimand used to calculate the ATE is also returned.\n        \"\"\"\n\n        # If the user provided the tag of any variable, retrieve their names\n        treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n        outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n        if confounder is not None:\n            confounder = TagUtils.name_of(vars, confounder, \"prepared\")\n\n        # Should the effects be calculated based on the current partial causal graph,\n        # some other graph provided as a function parameter,\n        # or on an ad-hoc subset relevant for the question at hand?\n        if graph is None:\n            graph = nx.DiGraph()\n            graph.add_node(treatment)\n            graph.add_node(outcome)\n            graph.add_edge(treatment, outcome)\n\n            if confounder is not None:\n                graph.add_node(confounder)\n                graph.add_edge(confounder, outcome)\n                graph.add_edge(confounder, treatment)\n\n        # Use dowhy to get the ATE, P-value and standard error.\n        with open(\"/dev/null\", \"w+\") as f:\n            try:\n                with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                    model = CausalModel(\n                        data=data[list(graph.nodes)],\n                        treatment=treatment,\n                        outcome=outcome,\n                        graph=nx.nx_pydot.to_pydot(graph).to_string(),\n                    )\n                    identified_estimand = model.identify_effect(\n                        proceed_when_unidentifiable=True\n                    )\n                    estimate = model.estimate_effect(\n                        identified_estimand,\n                        method_name=\"backdoor.linear_regression\",\n                        test_significance=True,\n                    )\n                    p_value = (\n                        estimate.test_stat_significance()[\"p_value\"].astype(float)[0]\n                        if calculate_p_value\n                        else None\n                    )\n                    stderr = (\n                        estimate.get_standard_error() if calculate_std_error else None\n                    )\n                    d = {\n                        \"ATE\": float(estimate.value),\n                        \"P-value\": p_value,\n                        \"Standard Error\": stderr,\n                    }\n                    if get_estimand:\n                        d[\"Estimand\"] = identified_estimand\n                    return d\n            except:\n                raise ValueError\n</code></pre>"},{"location":"reference/logos/ate_calculator/#logos.ate_calculator.ATECalculator.get_ate_and_confidence","title":"<code>get_ate_and_confidence(data, vars, treatment, outcome, confounder=None, graph=None, calculate_p_value=True, calculate_std_error=True, get_estimand=False)</code>  <code>staticmethod</code>","text":"<p>Calculate the ATE of <code>treatment</code> on <code>outcome</code>, alongside confidence measures.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>confounder</code> <code>Optional[str]</code> <p>The name or tag of a confounder variable. If specified, overrides the current partial causal graph in favor of a three-node graph with <code>treatment</code>, <code>outcome</code> and <code>confounder</code>.</p> <code>None</code> <code>graph</code> <code>Optional[DiGraph]</code> <p>The graph to be used for causal analysis. If not specified, a two-node graph with just <code>treatment</code> and <code>outcome</code> is used.</p> <code>None</code> <code>calculate_p_value</code> <code>bool</code> <p>Whether to calculate the P-value of the ATE.</p> <code>True</code> <code>calculate_std_error</code> <code>bool</code> <p>Whether to calculate the standard error of the ATE.</p> <code>True</code> <code>get_estimand</code> <code>bool</code> <p>Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the ATE of <code>treatment</code> on <code>outcome</code>, alongside confidence measures. If</p> <code>dict[str, Any]</code> <p><code>get_estimand</code> is True, the estimand used to calculate the ATE is also returned.</p> Source code in <code>src/logos/ate_calculator.py</code> <pre><code>@staticmethod\ndef get_ate_and_confidence(\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    treatment: str,\n    outcome: str,\n    confounder: Optional[str] = None,\n    graph: Optional[nx.DiGraph] = None,\n    calculate_p_value: bool = True,\n    calculate_std_error: bool = True,\n    get_estimand: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Calculate the ATE of `treatment` on `outcome`, alongside confidence measures.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n            causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n        graph: The graph to be used for causal analysis. If not specified, a two-node graph with just\n            `treatment` and `outcome` is used.\n        calculate_p_value: Whether to calculate the P-value of the ATE.\n        calculate_std_error: Whether to calculate the standard error of the ATE.\n        get_estimand: Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.\n\n    Returns:\n        A dictionary containing the ATE of `treatment` on `outcome`, alongside confidence measures. If\n        `get_estimand` is True, the estimand used to calculate the ATE is also returned.\n    \"\"\"\n\n    # If the user provided the tag of any variable, retrieve their names\n    treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n    outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n    if confounder is not None:\n        confounder = TagUtils.name_of(vars, confounder, \"prepared\")\n\n    # Should the effects be calculated based on the current partial causal graph,\n    # some other graph provided as a function parameter,\n    # or on an ad-hoc subset relevant for the question at hand?\n    if graph is None:\n        graph = nx.DiGraph()\n        graph.add_node(treatment)\n        graph.add_node(outcome)\n        graph.add_edge(treatment, outcome)\n\n        if confounder is not None:\n            graph.add_node(confounder)\n            graph.add_edge(confounder, outcome)\n            graph.add_edge(confounder, treatment)\n\n    # Use dowhy to get the ATE, P-value and standard error.\n    with open(\"/dev/null\", \"w+\") as f:\n        try:\n            with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                model = CausalModel(\n                    data=data[list(graph.nodes)],\n                    treatment=treatment,\n                    outcome=outcome,\n                    graph=nx.nx_pydot.to_pydot(graph).to_string(),\n                )\n                identified_estimand = model.identify_effect(\n                    proceed_when_unidentifiable=True\n                )\n                estimate = model.estimate_effect(\n                    identified_estimand,\n                    method_name=\"backdoor.linear_regression\",\n                    test_significance=True,\n                )\n                p_value = (\n                    estimate.test_stat_significance()[\"p_value\"].astype(float)[0]\n                    if calculate_p_value\n                    else None\n                )\n                stderr = (\n                    estimate.get_standard_error() if calculate_std_error else None\n                )\n                d = {\n                    \"ATE\": float(estimate.value),\n                    \"P-value\": p_value,\n                    \"Standard Error\": stderr,\n                }\n                if get_estimand:\n                    d[\"Estimand\"] = identified_estimand\n                return d\n        except:\n            raise ValueError\n</code></pre>"},{"location":"reference/logos/candidate_cause_ranker/","title":"CandidateCauseRanker","text":""},{"location":"reference/logos/candidate_cause_ranker/#logos.candidate_cause_ranker.CandidateCauseRanker","title":"<code>CandidateCauseRanker</code>","text":"Source code in <code>src/logos/candidate_cause_ranker.py</code> <pre><code>class CandidateCauseRanker:\n    COLUMN_ORDER = [\n        \"Candidate\",\n        \"Candidate Tag\",\n        \"Target Tag\",\n        \"Slope\",\n        \"P-value\",\n        \"Candidate-&gt;Target Edge Status\",\n        \"Target-&gt;Candidate Edge Status\",\n    ]\n\n    INTERNAL_COLUMN_ORDER = [\n        \"Candidate\",\n        \"Candidate Tag\",\n        \"Target Tag\",\n        \"Slope\",\n        \"P-value\",\n    ]\n\n    @staticmethod\n    def rank(\n        data: pd.DataFrame,\n        data_tags_df: pd.DataFrame = None,\n        target_name: Optional[str] = None,\n        ignore: Optional[list[str]] = None,\n        method: CandidateCauseRankerMethod = CandidateCauseRankerMethod.LOGOS,\n        prune_candidates: bool = True,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n        model: str = \"gpt-4o-mini-2024-07-18\",\n        gpt_log_path: Optional[str] = None,\n    ) -&gt; Tuple[pd.DataFrame, list[str]]:\n        \"\"\"\n        Present the user with candidate causal graph neighbors for `target`. If no `target`\n        is specified, the most recent suggestion of `suggest_next_exploration()` is used, if any.\n        If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n        Parameters:\n            data: The data based on which to explore candidate causes.\n            data_tags_df: A dataframe containing tags for the data.\n            target_name: The name of the target variable.\n            ignore: A list of variables to ignore.\n            method: The method to use for ranking candidate causes.\n            prune_candidates: Whether to prune the candidate causes using Lasso regression. Only\n                applies if `method` is `CandidateCauseRankerMethod.LOGOS`.\n            lasso_alpha: The alpha parameter to be used for Lasso regression. Only applies if\n                `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n            lasso_max_iter: The maximum number of iterations to be used for Lasso regression. Only\n                applies if `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n            model: The model to use for the langmodel method. Only applies if the method is\n                `CandidateCauseRankerMethod.LANGMODEL`.\n            gpt_log_path: The path to the log file for the prompt and reply. Only applies if the\n                method is `CandidateCauseRankerMethod.LANGMODEL`.\n\n        Returns:\n            results_df: A dataframe containing the candidate causal graph neighbors for `target`\n            pruned: A list of pruned candidate causes, if any.\n        \"\"\"\n        if ignore is None:\n            ignore = []\n        non_ignore = [col for col in data.columns if col not in ignore]\n\n        if method == CandidateCauseRankerMethod.LOGOS:\n            return CandidateCauseRanker._rank_logos(\n                data[non_ignore],\n                data_tags_df,\n                target_name,\n                prune_candidates,\n                lasso_alpha,\n                lasso_max_iter,\n            )\n        elif method == CandidateCauseRankerMethod.REGRESSION:\n            return CandidateCauseRanker._rank_regression(data[non_ignore], data_tags_df, target_name)\n        elif method == CandidateCauseRankerMethod.LANGMODEL:\n            return CandidateCauseRanker._rank_langmodel(\n                data[non_ignore], data_tags_df, target_name, model, gpt_log_path\n            )\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n    @staticmethod\n    def _rank_logos(\n        data: pd.DataFrame,\n        data_tags_df: pd.DataFrame = None,\n        target_name: Optional[str] = None,\n        prune_candidates: bool = True,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    ) -&gt; Tuple[pd.DataFrame, list[str]]:\n        \"\"\"\n        Implement `rank()` for the LOGOS method.\n\n        Parameters:\n            data: The data based on which to explore candidate causes.\n            data_tags_df: A dataframe containing tags for the data.\n            target_name: The name of the target variable.\n            prune_candidates: Whether to prune the candidate causes using Lasso regression.\n            lasso_alpha: The alpha parameter to be used for Lasso regression.\n            lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n        Returns:\n            results_df: A dataframe containing the candidate causal graph neighbors for `target`\n            pruned: A list of pruned candidate causes, if any.\n        \"\"\"\n\n        # Use Lasso to get a pruned list of neighbors\n        candidates = (\n            Pruner.prune_with_lasso(\n                data,\n                [target_name],\n                alpha=lasso_alpha,\n                max_iter=lasso_max_iter,\n            )\n            if prune_candidates\n            else [c for c in data.columns if c != target_name]\n        )\n        Printer.printv(f\"Candidates: {candidates}\")\n\n        # Stop if there are no candidates\n        if len(candidates) == 0:\n            print(\"No candidates found.\")\n            return pd.DataFrame(columns=CandidateCauseRanker.COLUMN_ORDER), pruned\n\n        # For each candidate, calculate the slope and p-value of a linear regression with target (in parallel)\n        num_processors = multiprocessing.cpu_count()\n        with multiprocessing.Pool(processes=num_processors) as pool:\n            results = pool.starmap(\n                Regression.ols,\n                [(col, data[col], data[target_name]) for col in candidates],\n            )\n\n        # Prepare return values.\n        result_df = (\n            pd.DataFrame(results)\n            .sort_values(by=\"P-value\", ascending=True)\n            .reset_index(drop=True)\n        )\n        result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n        result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n            lambda x: TagUtils.tag_of(data_tags_df, x, \"prepared\")\n        )\n        result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n        pruned = set(data.columns) - set(candidates) - set([target_name])\n\n        return result_df, pruned\n\n    @staticmethod\n    def _rank_regression(\n        data: pd.DataFrame,\n        data_tags_df: pd.DataFrame = None,\n        target_name: Optional[str] = None,\n    ) -&gt; Tuple[pd.DataFrame, list[str]]:\n        \"\"\"\n        Implement `rank()` for the REGRESSION method.\n\n        Parameters:\n            data: The data based on which to explore candidate causes.\n            data_tags_df: A dataframe containing tags for the data.\n            target_name: The name of the target variable.\n\n        Returns:\n            results_df: A dataframe containing the candidate causal graph neighbors for `target`\n            pruned: A list of pruned candidate causes, if any. #TODO: Prune based on cutoff?\n        \"\"\"\n\n        candidates = [c for c in data.columns if c != target_name]\n        result_df = Regression.multi_ols(\n            candidates, data[candidates], data[target_name]\n        )\n        result_df = (\n            result_df.sort_values(by=\"Absolute Normalized Slope\", ascending=False)\n            .drop(columns=[\"Normalized Slope\", \"Absolute Normalized Slope\"])\n            .reset_index(drop=True)\n        )\n\n        result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n        result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n            lambda x: TagUtils.tag_of(data_tags_df, x, \"prepared\")\n        )\n        result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n        return result_df, []\n\n    @staticmethod\n    def _rank_langmodel(\n        data: pd.DataFrame,\n        data_tags_df: pd.DataFrame = None,\n        target_name: Optional[str] = None,\n        model: str = \"gpt-4o-mini-2024-07-18\",\n        gpt_log_path: Optional[str] = None,\n    ) -&gt; Tuple[pd.DataFrame, list[str]]:\n        \"\"\"\n        Implement `rank()` for the LANGMODEL method.\n\n        Parameters:\n            data: The data based on which to explore candidate causes.\n            data_tags_df: A dataframe containing tags for the data.\n            target_name: The name of the target variable.\n            model: The model to use for the langmodel method.\n            gpt_log_path: The path to the log file for the prompt and reply.\n\n        Returns:\n            results_df: A dataframe containing the candidate causal graph neighbors for `target`\n            pruned: A list of pruned candidate causes, if any.\n        \"\"\"\n\n        client = OpenAI()\n\n        target_tag = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n        num_samples_per_var = 3\n\n        if gpt_log_path == None:\n            gpt_log_path = f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n        with open(gpt_log_path, \"w+\") as f:\n\n            # Define the messages to send to the model\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant for causal reasoning.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Below is a list of variable names and some example distinct values for each. \"\"\"\n                    f\"\"\"The lists are not sorted in compatible ways, so that elements in the same position may not correspond to the same entity. \"\"\"\n                    f\"\"\"I want you to assess the likelihood of each of these variables as a cause for variable '{target_tag}' and return them as a ranked list. \"\"\"\n                    \"\"\"I understand that you may think this is speculative, but I want you to do your best to come up with such a list ALWAYS. \"\"\"\n                    \"\"\"I will interpret any results you give me knowing that you may not be sure about them. \"\"\"\n                    \"\"\"Only return the ranked answers, one per line, preceded by a number and a period. Rank as many of the given variables as you see fit,\"\"\"\n                    f\"\"\" except '{target_tag}' itself. Do not return any other text before or after the list.\"\"\"\n                    \"\"\"Here are the variables: \"\"\"\n                    f\"\"\"{', '.join([f'{TagUtils.tag_of(data_tags_df, v, \"prepared\")}: [{\", \".join(str(x) for x in data[v].unique().tolist()[:num_samples_per_var])}]' for v in data.columns])}\"\"\",\n                },\n            ]\n\n            reply = (\n                client.chat.completions.create(model=model, messages=messages)\n                .choices[0]\n                .message.content\n            )\n\n            # Log the messages and the reply\n            f.write(f\"{datetime.now()}\\n\")\n            f.write(\"Messages:\\n\")\n            for message in messages:\n                f.write(f\"{message['role']}: {message['content']}\\n\")\n            f.write(\"----------------\\n\")\n            f.write(f\"Reply: {reply}\\n\\n\")\n            f.write(\"================\\n\")\n            f.flush()\n            f.close()\n\n        # Combat hallucinations\n        reply_rows = reply.split(\"\\n\")\n        reply_rows = [row for row in reply_rows if row.strip() != \"\" and row[0].isdigit()]\n        possibly_candidate_tags = [\".\".join(row.split(\".\")[1:]).split(':')[0].strip() for row in reply_rows]\n        candidate_tags = [tag for tag in possibly_candidate_tags if tag in data_tags_df['Tag'].values]\n\n        d = {\n            \"Candidate Tag\": candidate_tags,\n            \"Slope\": [None for _ in range(len(candidate_tags))],\n            \"P-value\": [None for _ in range(len(candidate_tags))],\n        }\n        result_df = pd.DataFrame(d)\n        result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n        result_df[\"Candidate\"] = result_df[\"Candidate Tag\"].apply(\n            lambda x: TagUtils.name_of(data_tags_df, x.split(':')[0], \"prepared\")\n        )\n        result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n        pruned = set(data.columns) - set(result_df[\"Candidate\"]) - set([target_name])\n\n        return result_df, pruned\n</code></pre>"},{"location":"reference/logos/candidate_cause_ranker/#logos.candidate_cause_ranker.CandidateCauseRanker.rank","title":"<code>rank(data, data_tags_df=None, target_name=None, ignore=None, method=CandidateCauseRankerMethod.LOGOS, prune_candidates=True, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER, model='gpt-4o-mini-2024-07-18', gpt_log_path=None)</code>  <code>staticmethod</code>","text":"<p>Present the user with candidate causal graph neighbors for <code>target</code>. If no <code>target</code> is specified, the most recent suggestion of <code>suggest_next_exploration()</code> is used, if any. If <code>ignore</code> is specified, the variables in <code>ignore</code> are not considered as candidate causes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data based on which to explore candidate causes.</p> required <code>data_tags_df</code> <code>DataFrame</code> <p>A dataframe containing tags for the data.</p> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable.</p> <code>None</code> <code>ignore</code> <code>Optional[list[str]]</code> <p>A list of variables to ignore.</p> <code>None</code> <code>method</code> <code>CandidateCauseRankerMethod</code> <p>The method to use for ranking candidate causes.</p> <code>LOGOS</code> <code>prune_candidates</code> <code>bool</code> <p>Whether to prune the candidate causes using Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code>.</p> <code>True</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code> and <code>prune_candidates</code> is True.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code> and <code>prune_candidates</code> is True.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>model</code> <code>str</code> <p>The model to use for the langmodel method. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>'gpt-4o-mini-2024-07-18'</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the log file for the prompt and reply. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>A dataframe containing the candidate causal graph neighbors for <code>target</code></p> <code>pruned</code> <code>list[str]</code> <p>A list of pruned candidate causes, if any.</p> Source code in <code>src/logos/candidate_cause_ranker.py</code> <pre><code>@staticmethod\ndef rank(\n    data: pd.DataFrame,\n    data_tags_df: pd.DataFrame = None,\n    target_name: Optional[str] = None,\n    ignore: Optional[list[str]] = None,\n    method: CandidateCauseRankerMethod = CandidateCauseRankerMethod.LOGOS,\n    prune_candidates: bool = True,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    model: str = \"gpt-4o-mini-2024-07-18\",\n    gpt_log_path: Optional[str] = None,\n) -&gt; Tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Present the user with candidate causal graph neighbors for `target`. If no `target`\n    is specified, the most recent suggestion of `suggest_next_exploration()` is used, if any.\n    If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n    Parameters:\n        data: The data based on which to explore candidate causes.\n        data_tags_df: A dataframe containing tags for the data.\n        target_name: The name of the target variable.\n        ignore: A list of variables to ignore.\n        method: The method to use for ranking candidate causes.\n        prune_candidates: Whether to prune the candidate causes using Lasso regression. Only\n            applies if `method` is `CandidateCauseRankerMethod.LOGOS`.\n        lasso_alpha: The alpha parameter to be used for Lasso regression. Only applies if\n            `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n        lasso_max_iter: The maximum number of iterations to be used for Lasso regression. Only\n            applies if `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n        model: The model to use for the langmodel method. Only applies if the method is\n            `CandidateCauseRankerMethod.LANGMODEL`.\n        gpt_log_path: The path to the log file for the prompt and reply. Only applies if the\n            method is `CandidateCauseRankerMethod.LANGMODEL`.\n\n    Returns:\n        results_df: A dataframe containing the candidate causal graph neighbors for `target`\n        pruned: A list of pruned candidate causes, if any.\n    \"\"\"\n    if ignore is None:\n        ignore = []\n    non_ignore = [col for col in data.columns if col not in ignore]\n\n    if method == CandidateCauseRankerMethod.LOGOS:\n        return CandidateCauseRanker._rank_logos(\n            data[non_ignore],\n            data_tags_df,\n            target_name,\n            prune_candidates,\n            lasso_alpha,\n            lasso_max_iter,\n        )\n    elif method == CandidateCauseRankerMethod.REGRESSION:\n        return CandidateCauseRanker._rank_regression(data[non_ignore], data_tags_df, target_name)\n    elif method == CandidateCauseRankerMethod.LANGMODEL:\n        return CandidateCauseRanker._rank_langmodel(\n            data[non_ignore], data_tags_df, target_name, model, gpt_log_path\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n</code></pre>"},{"location":"reference/logos/candidate_cause_ranker/#logos.candidate_cause_ranker.CandidateCauseRanker._rank_logos","title":"<code>_rank_logos(data, data_tags_df=None, target_name=None, prune_candidates=True, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER)</code>  <code>staticmethod</code>","text":"<p>Implement <code>rank()</code> for the LOGOS method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data based on which to explore candidate causes.</p> required <code>data_tags_df</code> <code>DataFrame</code> <p>A dataframe containing tags for the data.</p> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable.</p> <code>None</code> <code>prune_candidates</code> <code>bool</code> <p>Whether to prune the candidate causes using Lasso regression.</p> <code>True</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for Lasso regression.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for Lasso regression.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>A dataframe containing the candidate causal graph neighbors for <code>target</code></p> <code>pruned</code> <code>list[str]</code> <p>A list of pruned candidate causes, if any.</p> Source code in <code>src/logos/candidate_cause_ranker.py</code> <pre><code>@staticmethod\ndef _rank_logos(\n    data: pd.DataFrame,\n    data_tags_df: pd.DataFrame = None,\n    target_name: Optional[str] = None,\n    prune_candidates: bool = True,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n) -&gt; Tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Implement `rank()` for the LOGOS method.\n\n    Parameters:\n        data: The data based on which to explore candidate causes.\n        data_tags_df: A dataframe containing tags for the data.\n        target_name: The name of the target variable.\n        prune_candidates: Whether to prune the candidate causes using Lasso regression.\n        lasso_alpha: The alpha parameter to be used for Lasso regression.\n        lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n    Returns:\n        results_df: A dataframe containing the candidate causal graph neighbors for `target`\n        pruned: A list of pruned candidate causes, if any.\n    \"\"\"\n\n    # Use Lasso to get a pruned list of neighbors\n    candidates = (\n        Pruner.prune_with_lasso(\n            data,\n            [target_name],\n            alpha=lasso_alpha,\n            max_iter=lasso_max_iter,\n        )\n        if prune_candidates\n        else [c for c in data.columns if c != target_name]\n    )\n    Printer.printv(f\"Candidates: {candidates}\")\n\n    # Stop if there are no candidates\n    if len(candidates) == 0:\n        print(\"No candidates found.\")\n        return pd.DataFrame(columns=CandidateCauseRanker.COLUMN_ORDER), pruned\n\n    # For each candidate, calculate the slope and p-value of a linear regression with target (in parallel)\n    num_processors = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=num_processors) as pool:\n        results = pool.starmap(\n            Regression.ols,\n            [(col, data[col], data[target_name]) for col in candidates],\n        )\n\n    # Prepare return values.\n    result_df = (\n        pd.DataFrame(results)\n        .sort_values(by=\"P-value\", ascending=True)\n        .reset_index(drop=True)\n    )\n    result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n    result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n        lambda x: TagUtils.tag_of(data_tags_df, x, \"prepared\")\n    )\n    result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n    pruned = set(data.columns) - set(candidates) - set([target_name])\n\n    return result_df, pruned\n</code></pre>"},{"location":"reference/logos/candidate_cause_ranker/#logos.candidate_cause_ranker.CandidateCauseRanker._rank_regression","title":"<code>_rank_regression(data, data_tags_df=None, target_name=None)</code>  <code>staticmethod</code>","text":"<p>Implement <code>rank()</code> for the REGRESSION method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data based on which to explore candidate causes.</p> required <code>data_tags_df</code> <code>DataFrame</code> <p>A dataframe containing tags for the data.</p> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>A dataframe containing the candidate causal graph neighbors for <code>target</code></p> <code>pruned</code> <code>list[str]</code> <p>A list of pruned candidate causes, if any. #TODO: Prune based on cutoff?</p> Source code in <code>src/logos/candidate_cause_ranker.py</code> <pre><code>@staticmethod\ndef _rank_regression(\n    data: pd.DataFrame,\n    data_tags_df: pd.DataFrame = None,\n    target_name: Optional[str] = None,\n) -&gt; Tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Implement `rank()` for the REGRESSION method.\n\n    Parameters:\n        data: The data based on which to explore candidate causes.\n        data_tags_df: A dataframe containing tags for the data.\n        target_name: The name of the target variable.\n\n    Returns:\n        results_df: A dataframe containing the candidate causal graph neighbors for `target`\n        pruned: A list of pruned candidate causes, if any. #TODO: Prune based on cutoff?\n    \"\"\"\n\n    candidates = [c for c in data.columns if c != target_name]\n    result_df = Regression.multi_ols(\n        candidates, data[candidates], data[target_name]\n    )\n    result_df = (\n        result_df.sort_values(by=\"Absolute Normalized Slope\", ascending=False)\n        .drop(columns=[\"Normalized Slope\", \"Absolute Normalized Slope\"])\n        .reset_index(drop=True)\n    )\n\n    result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n    result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n        lambda x: TagUtils.tag_of(data_tags_df, x, \"prepared\")\n    )\n    result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n    return result_df, []\n</code></pre>"},{"location":"reference/logos/candidate_cause_ranker/#logos.candidate_cause_ranker.CandidateCauseRanker._rank_langmodel","title":"<code>_rank_langmodel(data, data_tags_df=None, target_name=None, model='gpt-4o-mini-2024-07-18', gpt_log_path=None)</code>  <code>staticmethod</code>","text":"<p>Implement <code>rank()</code> for the LANGMODEL method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data based on which to explore candidate causes.</p> required <code>data_tags_df</code> <code>DataFrame</code> <p>A dataframe containing tags for the data.</p> <code>None</code> <code>target_name</code> <code>Optional[str]</code> <p>The name of the target variable.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for the langmodel method.</p> <code>'gpt-4o-mini-2024-07-18'</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the log file for the prompt and reply.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>A dataframe containing the candidate causal graph neighbors for <code>target</code></p> <code>pruned</code> <code>list[str]</code> <p>A list of pruned candidate causes, if any.</p> Source code in <code>src/logos/candidate_cause_ranker.py</code> <pre><code>@staticmethod\ndef _rank_langmodel(\n    data: pd.DataFrame,\n    data_tags_df: pd.DataFrame = None,\n    target_name: Optional[str] = None,\n    model: str = \"gpt-4o-mini-2024-07-18\",\n    gpt_log_path: Optional[str] = None,\n) -&gt; Tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Implement `rank()` for the LANGMODEL method.\n\n    Parameters:\n        data: The data based on which to explore candidate causes.\n        data_tags_df: A dataframe containing tags for the data.\n        target_name: The name of the target variable.\n        model: The model to use for the langmodel method.\n        gpt_log_path: The path to the log file for the prompt and reply.\n\n    Returns:\n        results_df: A dataframe containing the candidate causal graph neighbors for `target`\n        pruned: A list of pruned candidate causes, if any.\n    \"\"\"\n\n    client = OpenAI()\n\n    target_tag = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n    num_samples_per_var = 3\n\n    if gpt_log_path == None:\n        gpt_log_path = f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n    with open(gpt_log_path, \"w+\") as f:\n\n        # Define the messages to send to the model\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant for causal reasoning.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Below is a list of variable names and some example distinct values for each. \"\"\"\n                f\"\"\"The lists are not sorted in compatible ways, so that elements in the same position may not correspond to the same entity. \"\"\"\n                f\"\"\"I want you to assess the likelihood of each of these variables as a cause for variable '{target_tag}' and return them as a ranked list. \"\"\"\n                \"\"\"I understand that you may think this is speculative, but I want you to do your best to come up with such a list ALWAYS. \"\"\"\n                \"\"\"I will interpret any results you give me knowing that you may not be sure about them. \"\"\"\n                \"\"\"Only return the ranked answers, one per line, preceded by a number and a period. Rank as many of the given variables as you see fit,\"\"\"\n                f\"\"\" except '{target_tag}' itself. Do not return any other text before or after the list.\"\"\"\n                \"\"\"Here are the variables: \"\"\"\n                f\"\"\"{', '.join([f'{TagUtils.tag_of(data_tags_df, v, \"prepared\")}: [{\", \".join(str(x) for x in data[v].unique().tolist()[:num_samples_per_var])}]' for v in data.columns])}\"\"\",\n            },\n        ]\n\n        reply = (\n            client.chat.completions.create(model=model, messages=messages)\n            .choices[0]\n            .message.content\n        )\n\n        # Log the messages and the reply\n        f.write(f\"{datetime.now()}\\n\")\n        f.write(\"Messages:\\n\")\n        for message in messages:\n            f.write(f\"{message['role']}: {message['content']}\\n\")\n        f.write(\"----------------\\n\")\n        f.write(f\"Reply: {reply}\\n\\n\")\n        f.write(\"================\\n\")\n        f.flush()\n        f.close()\n\n    # Combat hallucinations\n    reply_rows = reply.split(\"\\n\")\n    reply_rows = [row for row in reply_rows if row.strip() != \"\" and row[0].isdigit()]\n    possibly_candidate_tags = [\".\".join(row.split(\".\")[1:]).split(':')[0].strip() for row in reply_rows]\n    candidate_tags = [tag for tag in possibly_candidate_tags if tag in data_tags_df['Tag'].values]\n\n    d = {\n        \"Candidate Tag\": candidate_tags,\n        \"Slope\": [None for _ in range(len(candidate_tags))],\n        \"P-value\": [None for _ in range(len(candidate_tags))],\n    }\n    result_df = pd.DataFrame(d)\n    result_df[\"Target Tag\"] = TagUtils.tag_of(data_tags_df, target_name, \"prepared\")\n    result_df[\"Candidate\"] = result_df[\"Candidate Tag\"].apply(\n        lambda x: TagUtils.name_of(data_tags_df, x.split(':')[0], \"prepared\")\n    )\n    result_df = result_df[CandidateCauseRanker.INTERNAL_COLUMN_ORDER]\n\n    pruned = set(data.columns) - set(result_df[\"Candidate\"]) - set([target_name])\n\n    return result_df, pruned\n</code></pre>"},{"location":"reference/logos/causal_discoverer/","title":"CausalDiscoverer","text":""},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer","title":"<code>CausalDiscoverer</code>","text":"<p>Provides various methods for automatic causal discovery based on a dataframe.</p> <p>Within LOGos, the expectation is that the passed dataframe will contain the prepared variables.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>class CausalDiscoverer:\n    \"\"\"\n    Provides various methods for automatic causal discovery based on a dataframe.\n\n    Within LOGos, the expectation is that the passed dataframe will contain the prepared variables.\n    \"\"\"\n\n    @staticmethod\n    def _pgmpy_dag_to_digraph(dag: DAG) -&gt; nx.DiGraph:\n        \"\"\"\n        Converts a pgmpy DAG to a networkx DiGraph.\n\n        Parameters:\n            dag: The pgmpy DAG.\n\n        Returns:\n            The networkx DiGraph.\n        \"\"\"\n\n        return nx.DiGraph(dag.edges())\n\n    @staticmethod\n    def pc(df: pd.DataFrame, max_cond_vars: int = 3) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the PC algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the PC algorithm.\n            max_cond_vars: The maximum number of conditioning variables to use.\n\n        Returns:\n            The causal graph learned by the PC algorithm.\n        \"\"\"\n\n        pc = PC(data=df)\n        model = pc.estimate(variant=\"parallel\", max_cond_vars=max_cond_vars)\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def hill_climb(df: pd.DataFrame) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the hill climb algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the hill climb algorithm.\n\n        Returns:\n            The causal graph learned by the hill climb algorithm.\n        \"\"\"\n\n        scoring_method = K2Score(data=df)\n        hcs = HillClimbSearch(data=df)\n        model = hcs.estimate(\n            scoring_method=scoring_method, max_indegree=4, max_iter=int(1e4)\n        )\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def exhaustive(df: pd.DataFrame) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the exhaustive search algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the exhaustive search algorithm.\n\n        Returns:\n            The causal graph learned by the exhaustive search algorithm.\n        \"\"\"\n\n        scoring_method = K2Score(data=df)\n        exh = ExhaustiveSearch(data=df, complete_samples_only=False)\n        model = exh.estimate()\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def gpt(\n        data_df: pd.DataFrame,\n        model: str = \"gpt-3.5-turbo\",\n        vars_df: Optional[pd.DataFrame] = None,\n    ) -&gt; nx.DiGraph:\n        \"\"\"\n        Consults GPT to determine the causal graph of the variables in the dataframe.\n\n        Parameters:\n            data_df: The dataframe based on which to construct a causal graph.\n            model: The GPT model to use.\n            vars_df: The dataframe containing the variable names and tags.\n\n        Returns:\n            The causal graph learned by consulting GPT.\n        \"\"\"\n\n        # Open a file for logging, with the model and the timestamp in the name\n        log_file = open(\n            f\"/../../evaluation/gpt-logs/{model}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.txt\",\n            \"w\",\n        )\n\n        client = OpenAI()\n        graph = nx.DiGraph()\n\n        for i in tqdm(\n            range(len(data_df.columns)), desc=\"Outer edge-finding loop using GPT...\"\n        ):\n            for j in range(i + 1, len(data_df.columns)):\n                var_a = data_df.columns[i]\n                var_b = data_df.columns[j]\n\n                example_rows = data_df[[var_a, var_b]].dropna().sample(3)\n                examples_a = \", \".join(str(x) for x in example_rows[var_a].tolist())\n                examples_b = \", \".join(str(x) for x in example_rows[var_b].tolist())\n\n                tag_a = (\n                    var_a\n                    if vars_df is None\n                    else TagUtils.get_tag(vars_df, var_a, \"prepared\")\n                )\n                tag_b = (\n                    var_b\n                    if vars_df is None\n                    else TagUtils.get_tag(vars_df, var_b, \"prepared\")\n                )\n\n                # Define the messages to send to the model\n                messages = [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a helpful assistant for causal reasoning.\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"\"\"Which cause-and-effect relationship is more likely? \"\"\"\n                        f\"\"\"A. changing {tag_a} causes a change in {tag_b}. \"\"\"\n                        f\"\"\"B. changing {tag_b} causes a change in {tag_a}. \"\"\"\n                        f\"\"\"C. Neither of the two. \"\"\"\n                        f\"\"\" Here are some example values of {tag_a} : [{examples_a}]\"\"\"\n                        f\"\"\" Here are the corresponding values of {tag_b} : [{examples_b}]\"\"\"\n                        \"\"\"Let's work this out in a step by step way to be sure that we have the right answer. \"\"\"\n                        \"\"\"Then provide your \ufb01nal answer within the tags &lt;Answer&gt;A/B/C&lt;/Answer&gt;.\"\"\",\n                    },\n                ]\n\n                reply = (\n                    client.chat.completions.create(model=model, messages=messages)\n                    .choices[0]\n                    .message.content\n                )\n\n                # Log the messages and the reply\n                log_file.write(f\"{datetime.now()}\\n\")\n                log_file.write(\"Messages:\\n\")\n                for message in messages:\n                    log_file.write(f\"{message['role']}: {message['content']}\\n\")\n                log_file.write(\"----------------\\n\")\n                log_file.write(f\"Reply: {reply}\\n\\n\")\n                log_file.write(\"================\\n\")\n                log_file.flush()\n\n                # Find the part of the reply that contains the answer\n                start_idx = reply.find(\"&lt;Answer&gt;\") + len(\"&lt;Answer&gt;\")\n                end_idx = reply.find(\"&lt;/Answer&gt;\")\n                answer = reply[start_idx:end_idx]\n\n                # Add the edge to the graph\n                if answer == \"A\":\n                    graph.add_edge(var_a, var_b)\n                elif answer == \"B\":\n                    graph.add_edge(var_b, var_a)\n        log_file.close()\n        return graph\n</code></pre>"},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer._pgmpy_dag_to_digraph","title":"<code>_pgmpy_dag_to_digraph(dag)</code>  <code>staticmethod</code>","text":"<p>Converts a pgmpy DAG to a networkx DiGraph.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DAG</code> <p>The pgmpy DAG.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The networkx DiGraph.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>@staticmethod\ndef _pgmpy_dag_to_digraph(dag: DAG) -&gt; nx.DiGraph:\n    \"\"\"\n    Converts a pgmpy DAG to a networkx DiGraph.\n\n    Parameters:\n        dag: The pgmpy DAG.\n\n    Returns:\n        The networkx DiGraph.\n    \"\"\"\n\n    return nx.DiGraph(dag.edges())\n</code></pre>"},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer.pc","title":"<code>pc(df, max_cond_vars=3)</code>  <code>staticmethod</code>","text":"<p>Runs the PC algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the PC algorithm.</p> required <code>max_cond_vars</code> <code>int</code> <p>The maximum number of conditioning variables to use.</p> <code>3</code> <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the PC algorithm.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>@staticmethod\ndef pc(df: pd.DataFrame, max_cond_vars: int = 3) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the PC algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the PC algorithm.\n        max_cond_vars: The maximum number of conditioning variables to use.\n\n    Returns:\n        The causal graph learned by the PC algorithm.\n    \"\"\"\n\n    pc = PC(data=df)\n    model = pc.estimate(variant=\"parallel\", max_cond_vars=max_cond_vars)\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer.hill_climb","title":"<code>hill_climb(df)</code>  <code>staticmethod</code>","text":"<p>Runs the hill climb algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the hill climb algorithm.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the hill climb algorithm.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>@staticmethod\ndef hill_climb(df: pd.DataFrame) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the hill climb algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the hill climb algorithm.\n\n    Returns:\n        The causal graph learned by the hill climb algorithm.\n    \"\"\"\n\n    scoring_method = K2Score(data=df)\n    hcs = HillClimbSearch(data=df)\n    model = hcs.estimate(\n        scoring_method=scoring_method, max_indegree=4, max_iter=int(1e4)\n    )\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer.exhaustive","title":"<code>exhaustive(df)</code>  <code>staticmethod</code>","text":"<p>Runs the exhaustive search algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the exhaustive search algorithm.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the exhaustive search algorithm.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>@staticmethod\ndef exhaustive(df: pd.DataFrame) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the exhaustive search algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the exhaustive search algorithm.\n\n    Returns:\n        The causal graph learned by the exhaustive search algorithm.\n    \"\"\"\n\n    scoring_method = K2Score(data=df)\n    exh = ExhaustiveSearch(data=df, complete_samples_only=False)\n    model = exh.estimate()\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/logos/causal_discoverer/#logos.causal_discoverer.CausalDiscoverer.gpt","title":"<code>gpt(data_df, model='gpt-3.5-turbo', vars_df=None)</code>  <code>staticmethod</code>","text":"<p>Consults GPT to determine the causal graph of the variables in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The dataframe based on which to construct a causal graph.</p> required <code>model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> <code>vars_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing the variable names and tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by consulting GPT.</p> Source code in <code>src/logos/causal_discoverer.py</code> <pre><code>@staticmethod\ndef gpt(\n    data_df: pd.DataFrame,\n    model: str = \"gpt-3.5-turbo\",\n    vars_df: Optional[pd.DataFrame] = None,\n) -&gt; nx.DiGraph:\n    \"\"\"\n    Consults GPT to determine the causal graph of the variables in the dataframe.\n\n    Parameters:\n        data_df: The dataframe based on which to construct a causal graph.\n        model: The GPT model to use.\n        vars_df: The dataframe containing the variable names and tags.\n\n    Returns:\n        The causal graph learned by consulting GPT.\n    \"\"\"\n\n    # Open a file for logging, with the model and the timestamp in the name\n    log_file = open(\n        f\"/../../evaluation/gpt-logs/{model}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.txt\",\n        \"w\",\n    )\n\n    client = OpenAI()\n    graph = nx.DiGraph()\n\n    for i in tqdm(\n        range(len(data_df.columns)), desc=\"Outer edge-finding loop using GPT...\"\n    ):\n        for j in range(i + 1, len(data_df.columns)):\n            var_a = data_df.columns[i]\n            var_b = data_df.columns[j]\n\n            example_rows = data_df[[var_a, var_b]].dropna().sample(3)\n            examples_a = \", \".join(str(x) for x in example_rows[var_a].tolist())\n            examples_b = \", \".join(str(x) for x in example_rows[var_b].tolist())\n\n            tag_a = (\n                var_a\n                if vars_df is None\n                else TagUtils.get_tag(vars_df, var_a, \"prepared\")\n            )\n            tag_b = (\n                var_b\n                if vars_df is None\n                else TagUtils.get_tag(vars_df, var_b, \"prepared\")\n            )\n\n            # Define the messages to send to the model\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant for causal reasoning.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Which cause-and-effect relationship is more likely? \"\"\"\n                    f\"\"\"A. changing {tag_a} causes a change in {tag_b}. \"\"\"\n                    f\"\"\"B. changing {tag_b} causes a change in {tag_a}. \"\"\"\n                    f\"\"\"C. Neither of the two. \"\"\"\n                    f\"\"\" Here are some example values of {tag_a} : [{examples_a}]\"\"\"\n                    f\"\"\" Here are the corresponding values of {tag_b} : [{examples_b}]\"\"\"\n                    \"\"\"Let's work this out in a step by step way to be sure that we have the right answer. \"\"\"\n                    \"\"\"Then provide your \ufb01nal answer within the tags &lt;Answer&gt;A/B/C&lt;/Answer&gt;.\"\"\",\n                },\n            ]\n\n            reply = (\n                client.chat.completions.create(model=model, messages=messages)\n                .choices[0]\n                .message.content\n            )\n\n            # Log the messages and the reply\n            log_file.write(f\"{datetime.now()}\\n\")\n            log_file.write(\"Messages:\\n\")\n            for message in messages:\n                log_file.write(f\"{message['role']}: {message['content']}\\n\")\n            log_file.write(\"----------------\\n\")\n            log_file.write(f\"Reply: {reply}\\n\\n\")\n            log_file.write(\"================\\n\")\n            log_file.flush()\n\n            # Find the part of the reply that contains the answer\n            start_idx = reply.find(\"&lt;Answer&gt;\") + len(\"&lt;Answer&gt;\")\n            end_idx = reply.find(\"&lt;/Answer&gt;\")\n            answer = reply[start_idx:end_idx]\n\n            # Add the edge to the graph\n            if answer == \"A\":\n                graph.add_edge(var_a, var_b)\n            elif answer == \"B\":\n                graph.add_edge(var_b, var_a)\n    log_file.close()\n    return graph\n</code></pre>"},{"location":"reference/logos/causal_unit_suggester/","title":"CausalUnitSuggester","text":""},{"location":"reference/logos/causal_unit_suggester/#logos.causal_unit_suggester.CausalUnitSuggester","title":"<code>CausalUnitSuggester</code>","text":"<p>This class is responsible for suggesting causal units to the user.</p> Source code in <code>src/logos/causal_unit_suggester.py</code> <pre><code>class CausalUnitSuggester:\n    \"\"\"\n    This class is responsible for suggesting causal units to the user.\n    \"\"\"\n\n    @staticmethod\n    def _discretize(col: pd.Series, col_type: str, bins: int = 0) -&gt; pd.Series:\n        \"\"\"\n        Discretize an unsorted `col` based on its type. If `col_type` is 'num', then\n        return labels for each of `bins` equi-depth bins. If `col_type` is 'str,\n        then return a unique label for each unique value. Nulls in `col` are assigned\n        to bin -1.\n\n        Parameters:\n            col: The column to discretize.\n            col_type: The type of the column.\n            bins: The number of bins to use when discretizing the column.\n\n        Returns:\n            A vector of length len(`col`) with the labels of each value in `col`.\n        \"\"\"\n        if col_type == \"num\":\n            return (\n                pd.qcut(col, bins, labels=False, duplicates=\"drop\")\n                .fillna(-1)\n                .astype(int)\n            )\n        elif col_type == \"str\":\n            return pd.factorize(col, use_na_sentinel=True)[0]\n        else:\n            raise ValueError(f\"Unknown column type: {col_type}\")\n\n    @staticmethod\n    def _get_all_discretizations(\n        col: pd.Series, col_type: str, k: int\n    ) -&gt; list[pd.Series]:\n        \"\"\"\n        Return a list of all possible discretizations of `col` based on its type.\n        If `col_type` is 'num', then return discretizations with `k`, `2k` and `10k` bins.\n        If `col_type` is 'str', then return a discretization with a unique label for\n        each unique value in `col`.\n\n        Parameters:\n            col: The column to discretize.\n            col_type: The type of the column.\n            k: A parameter indirectly controlling the number of bins to use when discretizing\n                a numeric column (see above).\n\n        Returns:\n            A list of all desired discretizations of `col`.\n        \"\"\"\n\n        if col_type == \"num\":\n            l = []\n            if len(col) &gt;= k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, k))\n            if len(col) &gt;= 2 * k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, 2 * k))\n            if len(col) &gt;= 10 * k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, 10 * k))\n            return l\n        elif col_type == \"str\":\n            return [CausalUnitSuggester._discretize(col, col_type)]\n        else:\n            raise ValueError(f\"Unknown column type: {col_type}\")\n\n    @staticmethod\n    def _calculate_IUS(df: pd.DataFrame, discretization: pd.Series) -&gt; float:\n        \"\"\"\n        Calculate the Information Utilization Score of `df` if each row belongs\n        to the causal unit specified by `discretization`. The unit labelled -1\n        contails rows with null value for the causal unit column, so the corresponding\n        rows in `df` are ignored.\n\n        Parameters:\n            df: The DataFrame to calculate the Information Utilization Score of.\n            discretization: The causal unit of each row.\n\n        Returns:\n            The Information Utilization Score of `df`.\n        \"\"\"\n\n        grouped = df.groupby(discretization)  # TODO: handle nulls\n        ius = 0\n\n        for group_id, group_data in grouped:\n            if group_id == -1:\n                continue\n            columns_with_non_nulls = group_data.notna().any(axis=0).sum()\n            ius += columns_with_non_nulls * len(group_data)\n\n        return ius / (len(df.columns) * len(df))\n\n    @staticmethod\n    def suggest_causal_unit_defs(\n        data_df: pd.DataFrame,\n        var_df: pd.DataFrame,\n        min_causal_units: int = 4,\n        num_suggestions: int = 10,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Suggest at most `num_suggestions` causal unit definitions for `data_df` based on ius\n        maximization, while returning at least `min_causal_units` causal units. `var_df` provides\n        information on the type of each variable.\n\n        Parameters:\n            data_df: The DataFrame to suggest causal unit definitions for.\n            var_df: A DataFrame with one row for each variable in `data_df` that includes variable type information.\n            min_causal_units: The minimum number of causal units that a suggested definition should create.\n            num_suggestions: The maximum number of causal unit definitions to suggest.\n\n        Returns:\n            A DataFrame with one row for each suggested causal unit definition, or `None` if no suggestions were made.\n        \"\"\"\n\n        list_of_suggestions = []\n\n        for col in data_df.columns:\n            discretizations = CausalUnitSuggester._get_all_discretizations(\n                data_df[col],\n                var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                k=min_causal_units,\n            )\n            for disc in discretizations:\n                # Ensure that the unique values in disc, excluding -1 if it exists, are at least min_causal_units\n                if disc.max() &gt;= (min_causal_units - 1):\n                    list_of_suggestions.append(\n                        {\n                            \"Variable\": col,\n                            \"Type\": var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                            \"Num Units\": disc.max() + 1,\n                            \"IUS\": CausalUnitSuggester._calculate_IUS(data_df, disc),\n                        }\n                    )\n\n        df_of_suggestions = pd.DataFrame(list_of_suggestions)\n        if len(df_of_suggestions) == 0:\n            return None\n        return df_of_suggestions.sort_values(by=[\"IUS\"], ascending=False).head(\n            num_suggestions\n        )\n</code></pre>"},{"location":"reference/logos/causal_unit_suggester/#logos.causal_unit_suggester.CausalUnitSuggester._discretize","title":"<code>_discretize(col, col_type, bins=0)</code>  <code>staticmethod</code>","text":"<p>Discretize an unsorted <code>col</code> based on its type. If <code>col_type</code> is 'num', then return labels for each of <code>bins</code> equi-depth bins. If <code>col_type</code> is 'str, then return a unique label for each unique value. Nulls in <code>col</code> are assigned to bin -1.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column to discretize.</p> required <code>col_type</code> <code>str</code> <p>The type of the column.</p> required <code>bins</code> <code>int</code> <p>The number of bins to use when discretizing the column.</p> <code>0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A vector of length len(<code>col</code>) with the labels of each value in <code>col</code>.</p> Source code in <code>src/logos/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _discretize(col: pd.Series, col_type: str, bins: int = 0) -&gt; pd.Series:\n    \"\"\"\n    Discretize an unsorted `col` based on its type. If `col_type` is 'num', then\n    return labels for each of `bins` equi-depth bins. If `col_type` is 'str,\n    then return a unique label for each unique value. Nulls in `col` are assigned\n    to bin -1.\n\n    Parameters:\n        col: The column to discretize.\n        col_type: The type of the column.\n        bins: The number of bins to use when discretizing the column.\n\n    Returns:\n        A vector of length len(`col`) with the labels of each value in `col`.\n    \"\"\"\n    if col_type == \"num\":\n        return (\n            pd.qcut(col, bins, labels=False, duplicates=\"drop\")\n            .fillna(-1)\n            .astype(int)\n        )\n    elif col_type == \"str\":\n        return pd.factorize(col, use_na_sentinel=True)[0]\n    else:\n        raise ValueError(f\"Unknown column type: {col_type}\")\n</code></pre>"},{"location":"reference/logos/causal_unit_suggester/#logos.causal_unit_suggester.CausalUnitSuggester._get_all_discretizations","title":"<code>_get_all_discretizations(col, col_type, k)</code>  <code>staticmethod</code>","text":"<p>Return a list of all possible discretizations of <code>col</code> based on its type. If <code>col_type</code> is 'num', then return discretizations with <code>k</code>, <code>2k</code> and <code>10k</code> bins. If <code>col_type</code> is 'str', then return a discretization with a unique label for each unique value in <code>col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column to discretize.</p> required <code>col_type</code> <code>str</code> <p>The type of the column.</p> required <code>k</code> <code>int</code> <p>A parameter indirectly controlling the number of bins to use when discretizing a numeric column (see above).</p> required <p>Returns:</p> Type Description <code>list[Series]</code> <p>A list of all desired discretizations of <code>col</code>.</p> Source code in <code>src/logos/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _get_all_discretizations(\n    col: pd.Series, col_type: str, k: int\n) -&gt; list[pd.Series]:\n    \"\"\"\n    Return a list of all possible discretizations of `col` based on its type.\n    If `col_type` is 'num', then return discretizations with `k`, `2k` and `10k` bins.\n    If `col_type` is 'str', then return a discretization with a unique label for\n    each unique value in `col`.\n\n    Parameters:\n        col: The column to discretize.\n        col_type: The type of the column.\n        k: A parameter indirectly controlling the number of bins to use when discretizing\n            a numeric column (see above).\n\n    Returns:\n        A list of all desired discretizations of `col`.\n    \"\"\"\n\n    if col_type == \"num\":\n        l = []\n        if len(col) &gt;= k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, k))\n        if len(col) &gt;= 2 * k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, 2 * k))\n        if len(col) &gt;= 10 * k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, 10 * k))\n        return l\n    elif col_type == \"str\":\n        return [CausalUnitSuggester._discretize(col, col_type)]\n    else:\n        raise ValueError(f\"Unknown column type: {col_type}\")\n</code></pre>"},{"location":"reference/logos/causal_unit_suggester/#logos.causal_unit_suggester.CausalUnitSuggester._calculate_IUS","title":"<code>_calculate_IUS(df, discretization)</code>  <code>staticmethod</code>","text":"<p>Calculate the Information Utilization Score of <code>df</code> if each row belongs to the causal unit specified by <code>discretization</code>. The unit labelled -1 contails rows with null value for the causal unit column, so the corresponding rows in <code>df</code> are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to calculate the Information Utilization Score of.</p> required <code>discretization</code> <code>Series</code> <p>The causal unit of each row.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Information Utilization Score of <code>df</code>.</p> Source code in <code>src/logos/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _calculate_IUS(df: pd.DataFrame, discretization: pd.Series) -&gt; float:\n    \"\"\"\n    Calculate the Information Utilization Score of `df` if each row belongs\n    to the causal unit specified by `discretization`. The unit labelled -1\n    contails rows with null value for the causal unit column, so the corresponding\n    rows in `df` are ignored.\n\n    Parameters:\n        df: The DataFrame to calculate the Information Utilization Score of.\n        discretization: The causal unit of each row.\n\n    Returns:\n        The Information Utilization Score of `df`.\n    \"\"\"\n\n    grouped = df.groupby(discretization)  # TODO: handle nulls\n    ius = 0\n\n    for group_id, group_data in grouped:\n        if group_id == -1:\n            continue\n        columns_with_non_nulls = group_data.notna().any(axis=0).sum()\n        ius += columns_with_non_nulls * len(group_data)\n\n    return ius / (len(df.columns) * len(df))\n</code></pre>"},{"location":"reference/logos/causal_unit_suggester/#logos.causal_unit_suggester.CausalUnitSuggester.suggest_causal_unit_defs","title":"<code>suggest_causal_unit_defs(data_df, var_df, min_causal_units=4, num_suggestions=10)</code>  <code>staticmethod</code>","text":"<p>Suggest at most <code>num_suggestions</code> causal unit definitions for <code>data_df</code> based on ius maximization, while returning at least <code>min_causal_units</code> causal units. <code>var_df</code> provides information on the type of each variable.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The DataFrame to suggest causal unit definitions for.</p> required <code>var_df</code> <code>DataFrame</code> <p>A DataFrame with one row for each variable in <code>data_df</code> that includes variable type information.</p> required <code>min_causal_units</code> <code>int</code> <p>The minimum number of causal units that a suggested definition should create.</p> <code>4</code> <code>num_suggestions</code> <code>int</code> <p>The maximum number of causal unit definitions to suggest.</p> <code>10</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame with one row for each suggested causal unit definition, or <code>None</code> if no suggestions were made.</p> Source code in <code>src/logos/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef suggest_causal_unit_defs(\n    data_df: pd.DataFrame,\n    var_df: pd.DataFrame,\n    min_causal_units: int = 4,\n    num_suggestions: int = 10,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Suggest at most `num_suggestions` causal unit definitions for `data_df` based on ius\n    maximization, while returning at least `min_causal_units` causal units. `var_df` provides\n    information on the type of each variable.\n\n    Parameters:\n        data_df: The DataFrame to suggest causal unit definitions for.\n        var_df: A DataFrame with one row for each variable in `data_df` that includes variable type information.\n        min_causal_units: The minimum number of causal units that a suggested definition should create.\n        num_suggestions: The maximum number of causal unit definitions to suggest.\n\n    Returns:\n        A DataFrame with one row for each suggested causal unit definition, or `None` if no suggestions were made.\n    \"\"\"\n\n    list_of_suggestions = []\n\n    for col in data_df.columns:\n        discretizations = CausalUnitSuggester._get_all_discretizations(\n            data_df[col],\n            var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n            k=min_causal_units,\n        )\n        for disc in discretizations:\n            # Ensure that the unique values in disc, excluding -1 if it exists, are at least min_causal_units\n            if disc.max() &gt;= (min_causal_units - 1):\n                list_of_suggestions.append(\n                    {\n                        \"Variable\": col,\n                        \"Type\": var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                        \"Num Units\": disc.max() + 1,\n                        \"IUS\": CausalUnitSuggester._calculate_IUS(data_df, disc),\n                    }\n                )\n\n    df_of_suggestions = pd.DataFrame(list_of_suggestions)\n    if len(df_of_suggestions) == 0:\n        return None\n    return df_of_suggestions.sort_values(by=[\"IUS\"], ascending=False).head(\n        num_suggestions\n    )\n</code></pre>"},{"location":"reference/logos/clustering_params/","title":"ClusteringParams","text":""},{"location":"reference/logos/clustering_params/#logos.clustering_params.ClusteringParams","title":"<code>ClusteringParams</code>","text":"<p>A class to conveniently hold all the parameters required by the clustering approach to challenging the ATE.</p> Source code in <code>src/logos/clustering_params.py</code> <pre><code>class ClusteringParams:\n    \"\"\"\n    A class to conveniently hold all the parameters required by the clustering\n    approach to challenging the ATE.\n    \"\"\"\n\n    def __init__(\n        self,\n        top_n: int = 10,\n        num_edges: int = 3,\n        ignore_ts: bool = True,\n        var_pruning_method: Optional[str] = None,\n        triangle_n: int = 6,\n        force: bool = False,\n        force_triangle: bool = False,\n        num_clusters: Optional[int] = None,\n        threshold: float = 0,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a ClusteringParams object.\n\n        Parameters:\n            top_n: The number of top edges to identify.\n            num_edges: The maximum number of edges to use when enumerating DAGs.\n            ignore_ts: Whether to ignore timestamp variables.\n            var_pruning_method: The pruning method to use. Can be either \"lasso\" or \"triangle\".\n            triangle_n: The number of variables to use for the triangle method.\n            force: Whether to force recalculation.\n            force_triangle: Whether to force the triangle method to be recalculated, if selected.\n            num_clusters: The number of clusters to use. If None, will try to find the optimal number.\n            threshold: The threshold to use when finding outlier edges.\n\n        \"\"\"\n        self.top_n = top_n\n        self.num_edges = num_edges\n        self.ignore_ts = ignore_ts\n        self.var_pruning_method = var_pruning_method\n        self.triangle_n = triangle_n\n        self.force = force\n        self.force_triangle = force_triangle\n        self.num_clusters = num_clusters\n        self.threshold = threshold\n</code></pre>"},{"location":"reference/logos/clustering_params/#logos.clustering_params.ClusteringParams.__init__","title":"<code>__init__(top_n=10, num_edges=3, ignore_ts=True, var_pruning_method=None, triangle_n=6, force=False, force_triangle=False, num_clusters=None, threshold=0)</code>","text":"<p>Initializes a ClusteringParams object.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of top edges to identify.</p> <code>10</code> <code>num_edges</code> <code>int</code> <p>The maximum number of edges to use when enumerating DAGs.</p> <code>3</code> <code>ignore_ts</code> <code>bool</code> <p>Whether to ignore timestamp variables.</p> <code>True</code> <code>var_pruning_method</code> <code>Optional[str]</code> <p>The pruning method to use. Can be either \"lasso\" or \"triangle\".</p> <code>None</code> <code>triangle_n</code> <code>int</code> <p>The number of variables to use for the triangle method.</p> <code>6</code> <code>force</code> <code>bool</code> <p>Whether to force recalculation.</p> <code>False</code> <code>force_triangle</code> <code>bool</code> <p>Whether to force the triangle method to be recalculated, if selected.</p> <code>False</code> <code>num_clusters</code> <code>Optional[int]</code> <p>The number of clusters to use. If None, will try to find the optimal number.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The threshold to use when finding outlier edges.</p> <code>0</code> Source code in <code>src/logos/clustering_params.py</code> <pre><code>def __init__(\n    self,\n    top_n: int = 10,\n    num_edges: int = 3,\n    ignore_ts: bool = True,\n    var_pruning_method: Optional[str] = None,\n    triangle_n: int = 6,\n    force: bool = False,\n    force_triangle: bool = False,\n    num_clusters: Optional[int] = None,\n    threshold: float = 0,\n) -&gt; None:\n    \"\"\"\n    Initializes a ClusteringParams object.\n\n    Parameters:\n        top_n: The number of top edges to identify.\n        num_edges: The maximum number of edges to use when enumerating DAGs.\n        ignore_ts: Whether to ignore timestamp variables.\n        var_pruning_method: The pruning method to use. Can be either \"lasso\" or \"triangle\".\n        triangle_n: The number of variables to use for the triangle method.\n        force: Whether to force recalculation.\n        force_triangle: Whether to force the triangle method to be recalculated, if selected.\n        num_clusters: The number of clusters to use. If None, will try to find the optimal number.\n        threshold: The threshold to use when finding outlier edges.\n\n    \"\"\"\n    self.top_n = top_n\n    self.num_edges = num_edges\n    self.ignore_ts = ignore_ts\n    self.var_pruning_method = var_pruning_method\n    self.triangle_n = triangle_n\n    self.force = force\n    self.force_triangle = force_triangle\n    self.num_clusters = num_clusters\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/logos/drain/","title":"Drain","text":"<p>Inspired by the LogPAI implementation of the Drain algorithm for log parsing,  available under the MIT license here: https://github.com/HelenGuohx/logbert/blob/main/logparser/Drain.py</p>"},{"location":"reference/logos/drain/#logos.drain.Cluster","title":"<code>Cluster</code>","text":"<p>A cluster in the Drain parse tree.</p> Source code in <code>src/logos/drain.py</code> <pre><code>class Cluster:\n    \"\"\"\n    A cluster in the Drain parse tree.\n    \"\"\"\n\n    def __init__(self, template: str = \"\", message_ids: list[int] = []):\n        \"\"\"\n        Parameters:\n            template : the template of log messages in this cluster\n            message_ids : the list of log message IDs in this cluster\n        \"\"\"\n\n        self.template = template\n        self.message_ids = message_ids\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Cluster.__init__","title":"<code>__init__(template='', message_ids=[])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>template</code> <p>the template of log messages in this cluster</p> <code>''</code> <code>message_ids</code> <p>the list of log message IDs in this cluster</p> <code>[]</code> Source code in <code>src/logos/drain.py</code> <pre><code>def __init__(self, template: str = \"\", message_ids: list[int] = []):\n    \"\"\"\n    Parameters:\n        template : the template of log messages in this cluster\n        message_ids : the list of log message IDs in this cluster\n    \"\"\"\n\n    self.template = template\n    self.message_ids = message_ids\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Node","title":"<code>Node</code>","text":"<p>A node in the Drain parse tree.</p> Source code in <code>src/logos/drain.py</code> <pre><code>class Node:\n    \"\"\"\n    A node in the Drain parse tree.\n    \"\"\"\n\n    def __init__(self, children=None, depth=0, id=None):\n        \"\"\"\n        Parameters:\n            children : the dictionary of children nodes\n            depth : the depth of this node in the tree\n            id : the digit or token that this node represents\n        \"\"\"\n        if children is None:\n            children = dict()\n        self.children = children\n        self.depth = depth\n        self.id = id\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Node.__init__","title":"<code>__init__(children=None, depth=0, id=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>children</code> <p>the dictionary of children nodes</p> <code>None</code> <code>depth</code> <p>the depth of this node in the tree</p> <code>0</code> <code>id</code> <p>the digit or token that this node represents</p> <code>None</code> Source code in <code>src/logos/drain.py</code> <pre><code>def __init__(self, children=None, depth=0, id=None):\n    \"\"\"\n    Parameters:\n        children : the dictionary of children nodes\n        depth : the depth of this node in the tree\n        id : the digit or token that this node represents\n    \"\"\"\n    if children is None:\n        children = dict()\n    self.children = children\n    self.depth = depth\n    self.id = id\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain","title":"<code>Drain</code>","text":"<p>A class implementing the Drain log parsing algorithm.</p> Source code in <code>src/logos/drain.py</code> <pre><code>class Drain:\n    \"\"\"\n    A class implementing the Drain log parsing algorithm.\n    \"\"\"\n\n    def __init__(\n        self,\n        indir: str = \".\",\n        depth: int = 4,\n        st: float = 0.4,\n        max_children: int = 100,\n        rex: dict = {},\n        skip_writeout: bool = False,\n        message_prefix: str = r\".*\",\n    ):\n        \"\"\"\n        Initialize a Drain-based parser.\n\n        Parameters:\n            indir: the input directory stores the input log file name\n            depth: depth of all leaf nodes\n            st: similarity threshold\n            max_children: max number of children of an internal node\n            rex: regular expressions used in preprocessing, provided as a dictionary from field name to field regex\n            skip_writeout: whether to skip writing out the parsed log file, templates and variables.\n            message_prefix: prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.\n        \"\"\"\n        self.indir = indir\n        self.depth = depth - 2\n        self.st = st\n        self.max_children = max_children\n        self.rex = rex\n        self.skip_writeout = skip_writeout\n        self.message_prefix = message_prefix\n\n    def parse(self, filename: str) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Parse a log file.\n\n        Parameters:\n            filename: The name of the log file to parse (without path).\n\n        Returns:\n            A tuple of three dataframes, containing the parsed log file, the parsed log templates,\n            and the parsed variables respectively.\n        \"\"\"\n\n        full_path = os.path.join(self.indir, filename)\n        print(f\"Parsing file: {full_path}\")\n        self.filename = filename\n        self.root = Node()\n        self.cluster_list = []\n        self.logdf = self._to_df(full_path)\n\n        tqdm.pandas(desc=\"Determining template for each line...\")\n        self.logdf.progress_apply(self._parse_message, axis=1)\n\n        return self._postprocess()\n\n    def _to_df(self, log_file: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform a log file into a dataframe.\n\n        Parameters:\n            log_file: The path to the log file.\n\n        Returns:\n            A dataframe containing the log file's lines, tokenized and with regexes replaced.\n        \"\"\"\n\n        log_messages = []\n        linecount = 0\n\n        with open(log_file, \"r\") as f:\n            log_message = \"\"\n\n            for line in tqdm(f.readlines(), desc=\"Reading and tokenizing log lines...\"):\n                line = line.strip()\n\n                if re.match(self.message_prefix, line):\n                    if log_message:\n                        try:\n                            log_messages.append(self._preprocess(log_message))\n                            linecount += 1\n                        except Exception as e:\n                            raise ValueError\n                    log_message = line\n                else:\n                    log_message += \" \" + line\n\n            if log_message:\n                try:\n                    log_messages.append(self._preprocess(log_message))\n                    linecount += 1\n                except Exception as e:\n                    raise ValueError\n\n        logdf = pd.DataFrame(\n            log_messages, columns=[\"Message\", \"Tokenized\", \"Replaced by regex\"]\n        )\n        logdf[\"MsgId\"] = range(len(logdf))\n        return logdf\n\n    def _preprocess(self, msg: str) -&gt; tuple[str, list[str], list[str]]:\n        \"\"\"\n        Preprocess a message of a log file.\n\n        Parameters:\n            msg: The message to preprocess.\n\n        Returns:\n            A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.\n        \"\"\"\n\n        msg = msg.strip()\n\n        regex_matches = []\n        for i, rex in enumerate(self.rex.values()):\n            matches = re.findall(rex, msg)  ##### ASSUMPTION: only 1 match of interest\n            regex_matches.append(matches[0] if matches else \"\")\n            msg = re.sub(rex, \"&lt;*\" + str(i) + \"&gt;\", msg, count=1)\n\n        pattern = r'([=,\\{\\}\\[\\]\\(\\);\"\\'])'  # Add spaces around punctuation\n        msg = re.sub(pattern, r\" \\1 \", msg)\n        pattern = r\"(?&lt;=\\D):|:(?=\\D)\"  # Colons not in timestamps\n        msg = re.sub(pattern, \" : \", msg)\n\n        return (msg, msg.strip().split(), regex_matches)\n\n    def _parse_message(self, msg: pd.Series) -&gt; None:\n        \"\"\"\n        Parse a single log message and add it to the Drain parse tree in the appropriate cluster.\n\n        Parameters:\n            msg: The log message to parse.\n        \"\"\"\n\n        line_id = msg[\"MsgId\"]\n        tokenized = msg[\"Tokenized\"]\n        cluster = self._tree_search(self.root, tokenized)\n\n        if cluster is None:\n            new_cluster = Cluster(template=tokenized, message_ids=[line_id])\n            self.cluster_list.append(new_cluster)\n            self._add_cluster_to_tree(self.root, new_cluster)\n        else:\n            new_template = self._get_updated_template(tokenized, cluster.template)\n            cluster.message_ids.append(line_id)\n            if \" \".join(new_template) != \" \".join(cluster.template):\n                cluster.template = new_template\n\n    def _tree_search(self, root: Node, tokenized: list[str]) -&gt; Optional[Cluster]:\n        \"\"\"\n        Search the Drain parse tree for a cluster matching `tokenized`.\n\n        Parameters:\n            root: The root of the Drain parse tree.\n            tokenized: The tokenized log message to search for.\n\n        Returns:\n            The cluster in the Drain parse tree that matches `tokenized`,\n            or None if no such cluster exists.\n        \"\"\"\n\n        num_toks = len(tokenized)\n        if num_toks not in root.children:\n            return None\n\n        node = root.children[num_toks]\n\n        depth = 1\n        for token in tokenized:\n            if depth &gt;= self.depth or depth &gt; num_toks:\n                break\n            if token in node.children:\n                node = node.children[token]\n            elif \"&lt;*&gt;\" in node.children:\n                node = node.children[\"&lt;*&gt;\"]\n            else:\n                return None\n            depth += 1\n\n        cluster_list = node.children\n        returned_cluster = self._find_cluster(cluster_list, tokenized)\n\n        return returned_cluster\n\n    def _add_cluster_to_tree(self, root: Node, cluster: Cluster) -&gt; None:\n        \"\"\"\n        Add a cluster to the Drain parse tree.\n\n        Parameters:\n            root: The root of the Drain parse tree.\n            cluster: The cluster to add.\n        \"\"\"\n\n        # Add a node to the first layer of the tree representing the length of the log message.\n        length = len(cluster.template)\n        first_layer_node = None\n        if length not in root.children:\n            first_layer_node = Node(depth=1, id=length)\n            root.children[length] = first_layer_node\n        else:\n            first_layer_node = root.children[length]\n\n        # Traverse the tree to add the new cluster.\n        node = first_layer_node\n        depth = 1\n        for token in cluster.template:\n            # If out of depth, add current log cluster to the leaf node\n            if depth &gt;= self.depth or depth &gt; length:\n                if len(node.children) == 0:\n                    node.children = [cluster]\n                else:\n                    node.children.append(cluster)\n                break\n\n            # If token not matched in this layer of existing tree.\n            if token not in node.children:\n                if not any(char.isdigit() for char in token):\n                    if \"&lt;*&gt;\" in node.children:\n                        if len(node.children) &lt; self.max_children:\n                            new_node = Node(depth=depth + 1, id=token)\n                            node.children[token] = new_node\n                            node = new_node\n                        else:\n                            node = node.children[\"&lt;*&gt;\"]\n                    else:\n                        if len(node.children) + 1 &lt; self.max_children:\n                            new_node = Node(depth=depth + 1, id=token)\n                            node.children[token] = new_node\n                            node = new_node\n                        elif len(node.children) + 1 == self.max_children:\n                            new_node = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                            node.children[\"&lt;*&gt;\"] = new_node\n                            node = new_node\n                        else:\n                            node = node.children[\"&lt;*&gt;\"]\n                else:\n                    if \"&lt;*&gt;\" not in node.children:\n                        node.children[\"&lt;*&gt;\"] = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                    node = node.children[\"&lt;*&gt;\"]\n\n            # If the token is matched\n            else:\n                node = node.children[token]\n\n            depth += 1\n\n    def _similarity(self, seq1: list[str], seq2: list[str]) -&gt; tuple[float, int]:\n        \"\"\"\n        Determine the fraction of tokens in `seq1` that are identical to the corresponding token in `seq2`.\n        Also return the number of parameters in `seq1`.\n\n        Parameters:\n            seq1: The first sequence.\n            seq2: The second sequence.\n\n        Returns:\n            A tuple containing the fraction of identical tokens and the number of parameters in `seq1`.\n        \"\"\"\n        assert len(seq1) == len(seq2)\n        matches = 0\n        num_params = 0\n\n        for token1, token2 in zip(seq1, seq2):\n            if token1 == \"&lt;*&gt;\":\n                num_params += 1\n            if token1 == token2:\n                matches += 1\n\n        similarity = float(matches) / len(seq1)\n\n        return similarity, num_params\n\n    def _find_cluster(\n        self, cluster_list: list[Cluster], seq: list[str]\n    ) -&gt; Optional[Cluster]:\n        \"\"\"\n        Find the cluster in `cluster_list` that is most similar to `seq`.\n\n        Parameters:\n            cluster_list: The list of clusters to search.\n            seq: The sequence of tokens to compare to.\n\n        Returns:\n            The cluster in `cluster_list` that is most similar to `seq`,\n            or None if no cluster is sufficiently similar.\n        \"\"\"\n\n        max_similarity = -1\n        max_num_params = -1\n        max_cluster = None\n\n        for cluster in cluster_list:\n            similarity, num_params = self._similarity(cluster.template, seq)\n            if similarity &gt; max_similarity or (\n                similarity == max_similarity and num_params &gt; max_num_params\n            ):\n                max_similarity = similarity\n                max_num_params = num_params\n                max_cluster = cluster\n\n        if max_similarity &gt;= self.st:\n            return max_cluster\n        else:\n            return None\n\n    def _get_updated_template(self, template: list[str], msg: list[str]) -&gt; list[str]:\n        \"\"\"\n        Get the updated template from matching `msg` to `template`.\n\n        Parameters:\n            template: The template to match to.\n            msg: The message to match.\n\n        Returns:\n            The updated template.\n        \"\"\"\n\n        assert len(template) == len(msg)\n        updated_template = []\n\n        for i, word in enumerate(template):\n            if word == msg[i]:\n                updated_template.append(word)\n            else:\n                updated_template.append(\"&lt;*&gt;\")\n\n        return updated_template\n\n    @staticmethod\n    def _preceding_3(parsed_templates: pd.DataFrame, x: str) -&gt; list[str]:\n        \"\"\"\n        Get the 3 tokens preceding the variable `x` in the template.\n\n        Parameters:\n            parsed_templates: The dataframe containing information about the parsed templates.\n            x: The name of the variable.\n\n        Returns:\n            The 3 tokens preceding the variable `x` in the template.\n        \"\"\"\n\n        splitx = x.split(\"_\")\n        if len(splitx) != 2:\n            return []\n        id = splitx[0]\n        position = int(splitx[1])\n        start_position = max(0, position - 3)\n        return (\n            parsed_templates[parsed_templates[\"TemplateId\"] == id][\"TemplateText\"]\n            .values[0]\n            .split()[start_position:position]\n        )\n\n    def _postprocess(\n        self,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        template_id_per_msg = [0] * self.logdf.shape[0]\n        parsed_templates_list = []\n\n        regex_tokens = [\"&lt;*\" + str(i) + \"&gt;\" for i in range(len(self.rex))]\n\n        # Process each cluster to determine template information.\n        for cluster in self.cluster_list:\n            d = {}\n\n            # Determine the template ID\n            d[\"TemplateText\"] = \" \".join(cluster.template)\n            d[\"TemplateId\"] = hashlib.md5(\n                d[\"TemplateText\"].encode(\"utf-8\")\n            ).hexdigest()[0:8]\n\n            # Determine the indices of the variables and regexes in the template.\n            d[\"VariableIndices\"] = [\n                i for i, x in enumerate(cluster.template) if x == \"&lt;*&gt;\"\n            ]\n            d[\"RegexIndices\"] = []\n            for i in regex_tokens:\n                try:\n                    d[\"RegexIndices\"].append(cluster.template.index(str(i)))\n                except:\n                    pass\n\n            # Update the template ID for each log message in the cluster.\n            for i, line_id in enumerate(cluster.message_ids):\n                self.logdf.loc[line_id, \"TemplateId\"] = d[\"TemplateId\"]\n\n                if i == 0:\n                    d[\"TemplateExample\"] = self.logdf.loc[line_id, \"Message\"]\n\n            parsed_templates_list.append(d.copy())\n\n        # Create a dataframe of the parsed templates.\n        self.parsed_templates = pd.DataFrame(parsed_templates_list)\n        template_occurences = dict(self.logdf[\"TemplateId\"].value_counts())\n        self.parsed_templates[\"Occurrences\"] = self.parsed_templates[\"TemplateId\"].map(\n            template_occurences\n        )\n\n        # Create columns for each variable (parsed or regex-derived) and extract them from each log message.\n        variable_columns = list(self.rex.keys())\n        variable_columns.extend(\n            [\n                str(i) + \"_\" + str(j)\n                for i in self.parsed_templates[\"TemplateId\"].values\n                for j in self.parsed_templates.loc[\n                    self.parsed_templates[\"TemplateId\"] == i, \"VariableIndices\"\n                ].values[0]\n            ]\n        )\n        par_df = pd.DataFrame(\n            columns=variable_columns, index=range(self.logdf.shape[0])\n        )\n        self.logdf = pd.concat((self.logdf, par_df), axis=1)\n        self._extract_variables()\n\n        # Create a dataframe of the parsed variables.\n        parsed_variables = pd.DataFrame()\n        parsed_variables[\"Name\"] = variable_columns\n        parsed_variables[\"Occurrences\"] = parsed_variables[\"Name\"].map(\n            lambda x: self.logdf[x].notna().sum()\n        )\n        parsed_variables[\"Preceding 3 tokens\"] = parsed_variables[\"Name\"].map(\n            lambda x: Drain._preceding_3(self.parsed_templates, x)\n        )\n        parsed_variables[\"Examples\"] = parsed_variables[\"Name\"].map(\n            lambda x: self.logdf[x].loc[self.logdf[x].notna()].unique()[:5].tolist()\n        )\n        parsed_variables[\"From regex\"] = parsed_variables[\"Name\"].map(\n            lambda x: True if x in self.rex.keys() else False\n        )\n\n        # Drop unnecessary columns from the parsed log.\n        to_drop = [\"MsgId\", \"Message\", \"Tokenized\", \"Replaced by regex\"]\n        to_drop.extend(\n            parsed_variables[parsed_variables[\"Occurrences\"] == 0][\"Name\"].tolist()\n        )\n        parsed_log = self.logdf.drop(columns=to_drop)\n        parsed_variables = (\n            parsed_variables[~parsed_variables.isin(to_drop)[\"Name\"]]\n            .reset_index()\n            .drop(columns=\"index\")\n        )\n\n        return parsed_log, self.parsed_templates, parsed_variables\n\n    def _extract_variables(self) -&gt; None:\n        \"\"\"\n        Extract the variables from the log messages.\n        \"\"\"\n\n        for row in tqdm(\n            self.parsed_templates.itertuples(),\n            desc=\"Extracting variables from each log message...\",\n            total=len(self.parsed_templates),\n        ):\n            template_id = row.TemplateId\n            variable_indices = row.VariableIndices\n\n            mask = self.logdf[\"TemplateId\"] == template_id\n            for i in variable_indices:\n                col_name = f\"{template_id}_{str(i)}\"\n                self.logdf.loc[mask, col_name] = self.logdf.loc[mask, \"Tokenized\"].str[\n                    i\n                ]\n\n            for i, col_name in enumerate(self.rex.keys()):\n                self.logdf.loc[mask, col_name] = self.logdf.loc[\n                    mask, \"Replaced by regex\"\n                ].str[i]\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain.__init__","title":"<code>__init__(indir='.', depth=4, st=0.4, max_children=100, rex={}, skip_writeout=False, message_prefix='.*')</code>","text":"<p>Initialize a Drain-based parser.</p> <p>Parameters:</p> Name Type Description Default <code>indir</code> <code>str</code> <p>the input directory stores the input log file name</p> <code>'.'</code> <code>depth</code> <code>int</code> <p>depth of all leaf nodes</p> <code>4</code> <code>st</code> <code>float</code> <p>similarity threshold</p> <code>0.4</code> <code>max_children</code> <code>int</code> <p>max number of children of an internal node</p> <code>100</code> <code>rex</code> <code>dict</code> <p>regular expressions used in preprocessing, provided as a dictionary from field name to field regex</p> <code>{}</code> <code>skip_writeout</code> <code>bool</code> <p>whether to skip writing out the parsed log file, templates and variables.</p> <code>False</code> <code>message_prefix</code> <code>str</code> <p>prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.</p> <code>'.*'</code> Source code in <code>src/logos/drain.py</code> <pre><code>def __init__(\n    self,\n    indir: str = \".\",\n    depth: int = 4,\n    st: float = 0.4,\n    max_children: int = 100,\n    rex: dict = {},\n    skip_writeout: bool = False,\n    message_prefix: str = r\".*\",\n):\n    \"\"\"\n    Initialize a Drain-based parser.\n\n    Parameters:\n        indir: the input directory stores the input log file name\n        depth: depth of all leaf nodes\n        st: similarity threshold\n        max_children: max number of children of an internal node\n        rex: regular expressions used in preprocessing, provided as a dictionary from field name to field regex\n        skip_writeout: whether to skip writing out the parsed log file, templates and variables.\n        message_prefix: prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.\n    \"\"\"\n    self.indir = indir\n    self.depth = depth - 2\n    self.st = st\n    self.max_children = max_children\n    self.rex = rex\n    self.skip_writeout = skip_writeout\n    self.message_prefix = message_prefix\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain.parse","title":"<code>parse(filename)</code>","text":"<p>Parse a log file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the log file to parse (without path).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A tuple of three dataframes, containing the parsed log file, the parsed log templates,</p> <code>DataFrame</code> <p>and the parsed variables respectively.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def parse(self, filename: str) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Parse a log file.\n\n    Parameters:\n        filename: The name of the log file to parse (without path).\n\n    Returns:\n        A tuple of three dataframes, containing the parsed log file, the parsed log templates,\n        and the parsed variables respectively.\n    \"\"\"\n\n    full_path = os.path.join(self.indir, filename)\n    print(f\"Parsing file: {full_path}\")\n    self.filename = filename\n    self.root = Node()\n    self.cluster_list = []\n    self.logdf = self._to_df(full_path)\n\n    tqdm.pandas(desc=\"Determining template for each line...\")\n    self.logdf.progress_apply(self._parse_message, axis=1)\n\n    return self._postprocess()\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._to_df","title":"<code>_to_df(log_file)</code>","text":"<p>Transform a log file into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>log_file</code> <code>str</code> <p>The path to the log file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the log file's lines, tokenized and with regexes replaced.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _to_df(self, log_file: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform a log file into a dataframe.\n\n    Parameters:\n        log_file: The path to the log file.\n\n    Returns:\n        A dataframe containing the log file's lines, tokenized and with regexes replaced.\n    \"\"\"\n\n    log_messages = []\n    linecount = 0\n\n    with open(log_file, \"r\") as f:\n        log_message = \"\"\n\n        for line in tqdm(f.readlines(), desc=\"Reading and tokenizing log lines...\"):\n            line = line.strip()\n\n            if re.match(self.message_prefix, line):\n                if log_message:\n                    try:\n                        log_messages.append(self._preprocess(log_message))\n                        linecount += 1\n                    except Exception as e:\n                        raise ValueError\n                log_message = line\n            else:\n                log_message += \" \" + line\n\n        if log_message:\n            try:\n                log_messages.append(self._preprocess(log_message))\n                linecount += 1\n            except Exception as e:\n                raise ValueError\n\n    logdf = pd.DataFrame(\n        log_messages, columns=[\"Message\", \"Tokenized\", \"Replaced by regex\"]\n    )\n    logdf[\"MsgId\"] = range(len(logdf))\n    return logdf\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._preprocess","title":"<code>_preprocess(msg)</code>","text":"<p>Preprocess a message of a log file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to preprocess.</p> required <p>Returns:</p> Type Description <code>tuple[str, list[str], list[str]]</code> <p>A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _preprocess(self, msg: str) -&gt; tuple[str, list[str], list[str]]:\n    \"\"\"\n    Preprocess a message of a log file.\n\n    Parameters:\n        msg: The message to preprocess.\n\n    Returns:\n        A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.\n    \"\"\"\n\n    msg = msg.strip()\n\n    regex_matches = []\n    for i, rex in enumerate(self.rex.values()):\n        matches = re.findall(rex, msg)  ##### ASSUMPTION: only 1 match of interest\n        regex_matches.append(matches[0] if matches else \"\")\n        msg = re.sub(rex, \"&lt;*\" + str(i) + \"&gt;\", msg, count=1)\n\n    pattern = r'([=,\\{\\}\\[\\]\\(\\);\"\\'])'  # Add spaces around punctuation\n    msg = re.sub(pattern, r\" \\1 \", msg)\n    pattern = r\"(?&lt;=\\D):|:(?=\\D)\"  # Colons not in timestamps\n    msg = re.sub(pattern, \" : \", msg)\n\n    return (msg, msg.strip().split(), regex_matches)\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._parse_message","title":"<code>_parse_message(msg)</code>","text":"<p>Parse a single log message and add it to the Drain parse tree in the appropriate cluster.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Series</code> <p>The log message to parse.</p> required Source code in <code>src/logos/drain.py</code> <pre><code>def _parse_message(self, msg: pd.Series) -&gt; None:\n    \"\"\"\n    Parse a single log message and add it to the Drain parse tree in the appropriate cluster.\n\n    Parameters:\n        msg: The log message to parse.\n    \"\"\"\n\n    line_id = msg[\"MsgId\"]\n    tokenized = msg[\"Tokenized\"]\n    cluster = self._tree_search(self.root, tokenized)\n\n    if cluster is None:\n        new_cluster = Cluster(template=tokenized, message_ids=[line_id])\n        self.cluster_list.append(new_cluster)\n        self._add_cluster_to_tree(self.root, new_cluster)\n    else:\n        new_template = self._get_updated_template(tokenized, cluster.template)\n        cluster.message_ids.append(line_id)\n        if \" \".join(new_template) != \" \".join(cluster.template):\n            cluster.template = new_template\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._tree_search","title":"<code>_tree_search(root, tokenized)</code>","text":"<p>Search the Drain parse tree for a cluster matching <code>tokenized</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Node</code> <p>The root of the Drain parse tree.</p> required <code>tokenized</code> <code>list[str]</code> <p>The tokenized log message to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Cluster]</code> <p>The cluster in the Drain parse tree that matches <code>tokenized</code>,</p> <code>Optional[Cluster]</code> <p>or None if no such cluster exists.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _tree_search(self, root: Node, tokenized: list[str]) -&gt; Optional[Cluster]:\n    \"\"\"\n    Search the Drain parse tree for a cluster matching `tokenized`.\n\n    Parameters:\n        root: The root of the Drain parse tree.\n        tokenized: The tokenized log message to search for.\n\n    Returns:\n        The cluster in the Drain parse tree that matches `tokenized`,\n        or None if no such cluster exists.\n    \"\"\"\n\n    num_toks = len(tokenized)\n    if num_toks not in root.children:\n        return None\n\n    node = root.children[num_toks]\n\n    depth = 1\n    for token in tokenized:\n        if depth &gt;= self.depth or depth &gt; num_toks:\n            break\n        if token in node.children:\n            node = node.children[token]\n        elif \"&lt;*&gt;\" in node.children:\n            node = node.children[\"&lt;*&gt;\"]\n        else:\n            return None\n        depth += 1\n\n    cluster_list = node.children\n    returned_cluster = self._find_cluster(cluster_list, tokenized)\n\n    return returned_cluster\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._add_cluster_to_tree","title":"<code>_add_cluster_to_tree(root, cluster)</code>","text":"<p>Add a cluster to the Drain parse tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Node</code> <p>The root of the Drain parse tree.</p> required <code>cluster</code> <code>Cluster</code> <p>The cluster to add.</p> required Source code in <code>src/logos/drain.py</code> <pre><code>def _add_cluster_to_tree(self, root: Node, cluster: Cluster) -&gt; None:\n    \"\"\"\n    Add a cluster to the Drain parse tree.\n\n    Parameters:\n        root: The root of the Drain parse tree.\n        cluster: The cluster to add.\n    \"\"\"\n\n    # Add a node to the first layer of the tree representing the length of the log message.\n    length = len(cluster.template)\n    first_layer_node = None\n    if length not in root.children:\n        first_layer_node = Node(depth=1, id=length)\n        root.children[length] = first_layer_node\n    else:\n        first_layer_node = root.children[length]\n\n    # Traverse the tree to add the new cluster.\n    node = first_layer_node\n    depth = 1\n    for token in cluster.template:\n        # If out of depth, add current log cluster to the leaf node\n        if depth &gt;= self.depth or depth &gt; length:\n            if len(node.children) == 0:\n                node.children = [cluster]\n            else:\n                node.children.append(cluster)\n            break\n\n        # If token not matched in this layer of existing tree.\n        if token not in node.children:\n            if not any(char.isdigit() for char in token):\n                if \"&lt;*&gt;\" in node.children:\n                    if len(node.children) &lt; self.max_children:\n                        new_node = Node(depth=depth + 1, id=token)\n                        node.children[token] = new_node\n                        node = new_node\n                    else:\n                        node = node.children[\"&lt;*&gt;\"]\n                else:\n                    if len(node.children) + 1 &lt; self.max_children:\n                        new_node = Node(depth=depth + 1, id=token)\n                        node.children[token] = new_node\n                        node = new_node\n                    elif len(node.children) + 1 == self.max_children:\n                        new_node = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                        node.children[\"&lt;*&gt;\"] = new_node\n                        node = new_node\n                    else:\n                        node = node.children[\"&lt;*&gt;\"]\n            else:\n                if \"&lt;*&gt;\" not in node.children:\n                    node.children[\"&lt;*&gt;\"] = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                node = node.children[\"&lt;*&gt;\"]\n\n        # If the token is matched\n        else:\n            node = node.children[token]\n\n        depth += 1\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._similarity","title":"<code>_similarity(seq1, seq2)</code>","text":"<p>Determine the fraction of tokens in <code>seq1</code> that are identical to the corresponding token in <code>seq2</code>. Also return the number of parameters in <code>seq1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>seq1</code> <code>list[str]</code> <p>The first sequence.</p> required <code>seq2</code> <code>list[str]</code> <p>The second sequence.</p> required <p>Returns:</p> Type Description <code>tuple[float, int]</code> <p>A tuple containing the fraction of identical tokens and the number of parameters in <code>seq1</code>.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _similarity(self, seq1: list[str], seq2: list[str]) -&gt; tuple[float, int]:\n    \"\"\"\n    Determine the fraction of tokens in `seq1` that are identical to the corresponding token in `seq2`.\n    Also return the number of parameters in `seq1`.\n\n    Parameters:\n        seq1: The first sequence.\n        seq2: The second sequence.\n\n    Returns:\n        A tuple containing the fraction of identical tokens and the number of parameters in `seq1`.\n    \"\"\"\n    assert len(seq1) == len(seq2)\n    matches = 0\n    num_params = 0\n\n    for token1, token2 in zip(seq1, seq2):\n        if token1 == \"&lt;*&gt;\":\n            num_params += 1\n        if token1 == token2:\n            matches += 1\n\n    similarity = float(matches) / len(seq1)\n\n    return similarity, num_params\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._find_cluster","title":"<code>_find_cluster(cluster_list, seq)</code>","text":"<p>Find the cluster in <code>cluster_list</code> that is most similar to <code>seq</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_list</code> <code>list[Cluster]</code> <p>The list of clusters to search.</p> required <code>seq</code> <code>list[str]</code> <p>The sequence of tokens to compare to.</p> required <p>Returns:</p> Type Description <code>Optional[Cluster]</code> <p>The cluster in <code>cluster_list</code> that is most similar to <code>seq</code>,</p> <code>Optional[Cluster]</code> <p>or None if no cluster is sufficiently similar.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _find_cluster(\n    self, cluster_list: list[Cluster], seq: list[str]\n) -&gt; Optional[Cluster]:\n    \"\"\"\n    Find the cluster in `cluster_list` that is most similar to `seq`.\n\n    Parameters:\n        cluster_list: The list of clusters to search.\n        seq: The sequence of tokens to compare to.\n\n    Returns:\n        The cluster in `cluster_list` that is most similar to `seq`,\n        or None if no cluster is sufficiently similar.\n    \"\"\"\n\n    max_similarity = -1\n    max_num_params = -1\n    max_cluster = None\n\n    for cluster in cluster_list:\n        similarity, num_params = self._similarity(cluster.template, seq)\n        if similarity &gt; max_similarity or (\n            similarity == max_similarity and num_params &gt; max_num_params\n        ):\n            max_similarity = similarity\n            max_num_params = num_params\n            max_cluster = cluster\n\n    if max_similarity &gt;= self.st:\n        return max_cluster\n    else:\n        return None\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._get_updated_template","title":"<code>_get_updated_template(template, msg)</code>","text":"<p>Get the updated template from matching <code>msg</code> to <code>template</code>.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>list[str]</code> <p>The template to match to.</p> required <code>msg</code> <code>list[str]</code> <p>The message to match.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The updated template.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _get_updated_template(self, template: list[str], msg: list[str]) -&gt; list[str]:\n    \"\"\"\n    Get the updated template from matching `msg` to `template`.\n\n    Parameters:\n        template: The template to match to.\n        msg: The message to match.\n\n    Returns:\n        The updated template.\n    \"\"\"\n\n    assert len(template) == len(msg)\n    updated_template = []\n\n    for i, word in enumerate(template):\n        if word == msg[i]:\n            updated_template.append(word)\n        else:\n            updated_template.append(\"&lt;*&gt;\")\n\n    return updated_template\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._preceding_3","title":"<code>_preceding_3(parsed_templates, x)</code>  <code>staticmethod</code>","text":"<p>Get the 3 tokens preceding the variable <code>x</code> in the template.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_templates</code> <code>DataFrame</code> <p>The dataframe containing information about the parsed templates.</p> required <code>x</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The 3 tokens preceding the variable <code>x</code> in the template.</p> Source code in <code>src/logos/drain.py</code> <pre><code>@staticmethod\ndef _preceding_3(parsed_templates: pd.DataFrame, x: str) -&gt; list[str]:\n    \"\"\"\n    Get the 3 tokens preceding the variable `x` in the template.\n\n    Parameters:\n        parsed_templates: The dataframe containing information about the parsed templates.\n        x: The name of the variable.\n\n    Returns:\n        The 3 tokens preceding the variable `x` in the template.\n    \"\"\"\n\n    splitx = x.split(\"_\")\n    if len(splitx) != 2:\n        return []\n    id = splitx[0]\n    position = int(splitx[1])\n    start_position = max(0, position - 3)\n    return (\n        parsed_templates[parsed_templates[\"TemplateId\"] == id][\"TemplateText\"]\n        .values[0]\n        .split()[start_position:position]\n    )\n</code></pre>"},{"location":"reference/logos/drain/#logos.drain.Drain._extract_variables","title":"<code>_extract_variables()</code>","text":"<p>Extract the variables from the log messages.</p> Source code in <code>src/logos/drain.py</code> <pre><code>def _extract_variables(self) -&gt; None:\n    \"\"\"\n    Extract the variables from the log messages.\n    \"\"\"\n\n    for row in tqdm(\n        self.parsed_templates.itertuples(),\n        desc=\"Extracting variables from each log message...\",\n        total=len(self.parsed_templates),\n    ):\n        template_id = row.TemplateId\n        variable_indices = row.VariableIndices\n\n        mask = self.logdf[\"TemplateId\"] == template_id\n        for i in variable_indices:\n            col_name = f\"{template_id}_{str(i)}\"\n            self.logdf.loc[mask, col_name] = self.logdf.loc[mask, \"Tokenized\"].str[\n                i\n            ]\n\n        for i, col_name in enumerate(self.rex.keys()):\n            self.logdf.loc[mask, col_name] = self.logdf.loc[\n                mask, \"Replaced by regex\"\n            ].str[i]\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/","title":"EdgeOccurrenceTree","text":""},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree","title":"<code>EdgeOccurrenceTree</code>","text":"<p>A tree of DAGs based on the ATE cluster they belong to.</p> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>class EdgeOccurrenceTree:\n    \"\"\"\n    A tree of DAGs based on the ATE cluster they belong to.\n    \"\"\"\n\n    def __init__(self, cluster_id: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialize a tree node with a specific cluster id.\n\n        Parameters:\n            cluster_id: The cluster id of the DAGs that belong to this node.\n        \"\"\"\n\n        self.cluster_id = cluster_id\n        self.left = None\n        self.right = None\n\n    @staticmethod\n    def build_tree(linked: np.ndarray, leaves: list[int]) -&gt; tuple[Self, int]:\n        \"\"\"\n        Build a tree from a linkage matrix.\n\n        Parameters:\n            linked: The linkage matrix.\n            leaves: The list of leaf nodes.\n\n        Returns:\n            A tuple containing the root of the tree, and the index of the next cluster to be merged.\n        \"\"\"\n\n        # Base case: if there is only one cluster, return it as a leaf.\n        if len(leaves) == 1:\n            return EdgeOccurrenceTree(cluster_id=leaves[0]), -1\n\n        # Otherwise, build the tree recursively.\n        root = EdgeOccurrenceTree()\n        curr = root\n        i = len(linked) - 1\n        while i &gt; -1:\n            # Linked contains 4 elements: cluster1, cluster2, distance, num_observations\n            # They represent the clusters that were merged, the distance between them, and\n            # the number of observations in the new cluster.\n            c1, c2, _, _ = linked[i]\n\n            if c1 not in leaves and c2 not in leaves:\n                curr.left, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n                curr.right, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n                break\n            if c1 in leaves:\n                curr.left = EdgeOccurrenceTree(leaves.index(c1))\n                curr.right = EdgeOccurrenceTree()\n                curr = curr.right\n            if c2 in leaves:\n                curr.right = EdgeOccurrenceTree(leaves.index(c2))\n                break\n            i -= 1\n        root = EdgeOccurrenceTree._cleanup_tree(root)\n        return root, i\n\n    @staticmethod\n    def _cleanup_tree(root: Optional[Self]) -&gt; Self:\n        \"\"\"\n        Clean up the tree by removing nodes that have only one child, and nodes that have no\n        children and are not leaves.\n\n        Parameters:\n            root: The root of the tree.\n\n        Returns:\n            The root of the cleaned up tree.\n        \"\"\"\n\n        if root is None:\n            return None\n\n        # Recursively clean up left and right subtrees\n        root.left = EdgeOccurrenceTree._cleanup_tree(root.left)\n        root.right = EdgeOccurrenceTree._cleanup_tree(root.right)\n\n        # If the current node has only one child, replace the node with its child\n        if root.left is None and root.right is not None:\n            return root.right\n        elif root.left is not None and root.right is None:\n            return root.left\n\n        # If the current node has no left and right child and is not a leaf, remove the node\n        if root.left is None and root.right is None and root.cluster_id is None:\n            return None\n\n        return root\n\n    def print_tree(self, depth: int = 0) -&gt; None:\n        \"\"\"\n        Print the tree in a readable format.\n\n        Parameters:\n            depth: The depth of the current node in the tree.\n        \"\"\"\n\n        prefix = \"\"\n        for _ in range(depth):\n            prefix += \"-\"\n        if self.cluster_id is not None:\n            print(prefix + str(self.cluster_id))\n        else:\n            print(prefix + \"node\")\n        if self.left:\n            self.left.print_tree(depth + 1)\n        if self.right:\n            self.right.print_tree(depth + 1)\n\n    def assign_dags_to_nodes(self, cluster_mapping: dict[nx.DiGraph, int]) -&gt; None:\n        \"\"\"\n        Assign each DAG to the node it belongs to, based on `cluster_mapping`.\n\n        Parameters:\n            cluster_mapping: A dictionary mapping DAGs to cluster id's.\n        \"\"\"\n        self.num_dags = 0\n\n        # If leaf, assign DAGs and set count.\n        if self.cluster_id is not None:\n            self.dags = [\n                key\n                for key in cluster_mapping.keys()\n                if cluster_mapping[key] == self.cluster_id\n            ]\n            self.num_dags = len(self.dags)\n\n        # Otherwise, recurse for children and retireve counts.\n        if self.left:\n            self.left.assign_dags_to_nodes(cluster_mapping)\n            self.num_dags += self.left.num_dags\n        if self.right:\n            self.right.assign_dags_to_nodes(cluster_mapping)\n            self.num_dags += self.right.num_dags\n\n    def count_edge_occurrences(\n        self, treatment: str, outcome: str, dag: nx.DiGraph\n    ) -&gt; None:\n        \"\"\"\n        Recursively count the number of times each edge occurs amongst the DAGs\n        assigned to all the children of this node, omitting the edge from treatment -&gt; outcome,\n        since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.\n\n        Parameters:\n            treatment: The treatment variable.\n            outcome: The outcome variable.\n            dag: The optional dag structure to ignore.\n        \"\"\"\n        self.edge_counts: Types.EdgeCountDict = defaultdict(int)\n\n        # If leaf, actually compute count.\n        if self.cluster_id is not None:\n            edges_to_ignore = [(treatment, outcome)]\n            if dag:\n                edges_to_ignore.extend(dag.edges)\n            for graph in self.dags:\n                for edge in graph.edges:\n                    if edge not in edges_to_ignore:\n                        self.edge_counts[edge] += 1\n\n        # Otherwise, derive counts from children.\n        if self.left:\n            self.left.count_edge_occurrences(treatment, outcome, dag)\n            for key in self.left.edge_counts.keys():\n                self.edge_counts[key] += self.left.edge_counts[key]\n        if self.right:\n            self.right.count_edge_occurrences(treatment, outcome, dag)\n            for key in self.right.edge_counts.keys():\n                self.edge_counts[key] += self.right.edge_counts[key]\n\n        # Compute statistics.\n        freq_counts = list(self.edge_counts.values())\n        if len(freq_counts) == 0:\n            self.mean = None\n            self.std_dev = None\n        else:\n            self.mean = np.mean(freq_counts)\n            self.std_dev = np.std(freq_counts)\n\n    def calculate_edge_expectancy(\n        self, totals: tuple[int, Types.EdgeCountDict] = None\n    ) -&gt; None:\n        \"\"\"\n        For each edge at each node, calculate what percent over or under\n        expectancy the edge is at in relationship to its parent.\n\n        Parameters:\n            totals: A tuple containing the total number of DAGs and the mapping from\n                edges to their counts for the parent of this node.\n        \"\"\"\n        # At root node, calculate expectancy\n        if totals is None:\n            totals = (self.num_dags, self.edge_counts)\n\n        # Otherwise, calculate expectancy based on parent.\n        total_dags, total_edges = totals\n        self.percent_expectancy = defaultdict(float)\n\n        for edge in self.edge_counts.keys():\n            expected = self.num_dags / total_dags * total_edges[edge]\n            self.percent_expectancy[edge] = (\n                self.edge_counts[edge] - expected\n            ) / expected\n\n        # Recurse for children.\n        if self.left:\n            self.left.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n        if self.right:\n            self.right.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n\n    def find_outliers_in_tree(self, threshold: float = 0) -&gt; None:\n        \"\"\"\n        Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an\n        edge that is below expectancy on one side of the tree, and above on the other side, and\n        optionally, over some threshold on both sides.\n\n        Parameters:\n            threshold: The threshold for an edge to be considered an outlier.\n        \"\"\"\n\n        # If able to compare, find outliers.\n        if self.left and self.right:\n            self.left.outliers = {}\n            self.right.outliers = {}\n            edges = set(self.left.edge_counts.keys()).union(\n                set(self.right.edge_counts.keys())\n            )\n            for edge in edges:\n                if (\n                    np.sign(self.left.percent_expectancy[edge])\n                    != np.sign(self.right.percent_expectancy[edge])\n                    and abs(self.left.percent_expectancy[edge]) &gt; threshold\n                    and abs(self.right.percent_expectancy[edge]) &gt; threshold\n                ):\n                    self.left.outliers[edge] = self.left.percent_expectancy[edge]\n                    self.right.outliers[edge] = self.right.percent_expectancy[edge]\n\n        # Recurse for children.\n        if self.left:\n            self.left.find_outliers_in_tree(threshold)\n        if self.right:\n            self.right.find_outliers_in_tree(threshold)\n\n    def find_outliers_per_cluster(\n        self,\n        dag: nx.DiGraph,\n    ) -&gt; tuple[Types.EdgeCountDict, dict[Types.Edge, float]]:\n        \"\"\"\n        Collect the edge counts and outliers found earlier into appropriate dictionaries\n        per cluster.\n\n        Parameters:\n            dag: The DAG to ignore when collecting outliers.\n\n        Returns:\n            A tuple containing the following: a dictionary mapping cluster id's to edge counts,\n            and a dictionary mapping cluster id's to outlier edges.\n        \"\"\"\n\n        cluster_edge_count = {}\n        cluster_outliers = {}\n\n        # If leaf, add to cluster counts.\n        if self.cluster_id is not None:\n            cluster_edge_count[self.cluster_id] = self.edge_counts\n            edges_to_ignore = dag.edges if dag is not None else []\n            cluster_outliers[self.cluster_id] = {\n                edge: self.outliers[edge]\n                for edge in self.outliers\n                if edge not in edges_to_ignore\n            }\n\n        # Otherwise, recurse for children.\n        if self.left:\n            lec, lo = self.left.find_outliers_per_cluster(dag)\n            cluster_edge_count.update(lec)\n            cluster_outliers.update(lo)\n        if self.right:\n            rec, ro = self.right.find_outliers_per_cluster(dag)\n            cluster_edge_count.update(rec)\n            cluster_outliers.update(ro)\n\n        return cluster_edge_count, cluster_outliers\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.__init__","title":"<code>__init__(cluster_id=None)</code>","text":"<p>Initialize a tree node with a specific cluster id.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>Optional[str]</code> <p>The cluster id of the DAGs that belong to this node.</p> <code>None</code> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def __init__(self, cluster_id: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialize a tree node with a specific cluster id.\n\n    Parameters:\n        cluster_id: The cluster id of the DAGs that belong to this node.\n    \"\"\"\n\n    self.cluster_id = cluster_id\n    self.left = None\n    self.right = None\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.build_tree","title":"<code>build_tree(linked, leaves)</code>  <code>staticmethod</code>","text":"<p>Build a tree from a linkage matrix.</p> <p>Parameters:</p> Name Type Description Default <code>linked</code> <code>ndarray</code> <p>The linkage matrix.</p> required <code>leaves</code> <code>list[int]</code> <p>The list of leaf nodes.</p> required <p>Returns:</p> Type Description <code>tuple[Self, int]</code> <p>A tuple containing the root of the tree, and the index of the next cluster to be merged.</p> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>@staticmethod\ndef build_tree(linked: np.ndarray, leaves: list[int]) -&gt; tuple[Self, int]:\n    \"\"\"\n    Build a tree from a linkage matrix.\n\n    Parameters:\n        linked: The linkage matrix.\n        leaves: The list of leaf nodes.\n\n    Returns:\n        A tuple containing the root of the tree, and the index of the next cluster to be merged.\n    \"\"\"\n\n    # Base case: if there is only one cluster, return it as a leaf.\n    if len(leaves) == 1:\n        return EdgeOccurrenceTree(cluster_id=leaves[0]), -1\n\n    # Otherwise, build the tree recursively.\n    root = EdgeOccurrenceTree()\n    curr = root\n    i = len(linked) - 1\n    while i &gt; -1:\n        # Linked contains 4 elements: cluster1, cluster2, distance, num_observations\n        # They represent the clusters that were merged, the distance between them, and\n        # the number of observations in the new cluster.\n        c1, c2, _, _ = linked[i]\n\n        if c1 not in leaves and c2 not in leaves:\n            curr.left, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n            curr.right, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n            break\n        if c1 in leaves:\n            curr.left = EdgeOccurrenceTree(leaves.index(c1))\n            curr.right = EdgeOccurrenceTree()\n            curr = curr.right\n        if c2 in leaves:\n            curr.right = EdgeOccurrenceTree(leaves.index(c2))\n            break\n        i -= 1\n    root = EdgeOccurrenceTree._cleanup_tree(root)\n    return root, i\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree._cleanup_tree","title":"<code>_cleanup_tree(root)</code>  <code>staticmethod</code>","text":"<p>Clean up the tree by removing nodes that have only one child, and nodes that have no children and are not leaves.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Optional[Self]</code> <p>The root of the tree.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The root of the cleaned up tree.</p> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>@staticmethod\ndef _cleanup_tree(root: Optional[Self]) -&gt; Self:\n    \"\"\"\n    Clean up the tree by removing nodes that have only one child, and nodes that have no\n    children and are not leaves.\n\n    Parameters:\n        root: The root of the tree.\n\n    Returns:\n        The root of the cleaned up tree.\n    \"\"\"\n\n    if root is None:\n        return None\n\n    # Recursively clean up left and right subtrees\n    root.left = EdgeOccurrenceTree._cleanup_tree(root.left)\n    root.right = EdgeOccurrenceTree._cleanup_tree(root.right)\n\n    # If the current node has only one child, replace the node with its child\n    if root.left is None and root.right is not None:\n        return root.right\n    elif root.left is not None and root.right is None:\n        return root.left\n\n    # If the current node has no left and right child and is not a leaf, remove the node\n    if root.left is None and root.right is None and root.cluster_id is None:\n        return None\n\n    return root\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.print_tree","title":"<code>print_tree(depth=0)</code>","text":"<p>Print the tree in a readable format.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth of the current node in the tree.</p> <code>0</code> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def print_tree(self, depth: int = 0) -&gt; None:\n    \"\"\"\n    Print the tree in a readable format.\n\n    Parameters:\n        depth: The depth of the current node in the tree.\n    \"\"\"\n\n    prefix = \"\"\n    for _ in range(depth):\n        prefix += \"-\"\n    if self.cluster_id is not None:\n        print(prefix + str(self.cluster_id))\n    else:\n        print(prefix + \"node\")\n    if self.left:\n        self.left.print_tree(depth + 1)\n    if self.right:\n        self.right.print_tree(depth + 1)\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.assign_dags_to_nodes","title":"<code>assign_dags_to_nodes(cluster_mapping)</code>","text":"<p>Assign each DAG to the node it belongs to, based on <code>cluster_mapping</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_mapping</code> <code>dict[DiGraph, int]</code> <p>A dictionary mapping DAGs to cluster id's.</p> required Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def assign_dags_to_nodes(self, cluster_mapping: dict[nx.DiGraph, int]) -&gt; None:\n    \"\"\"\n    Assign each DAG to the node it belongs to, based on `cluster_mapping`.\n\n    Parameters:\n        cluster_mapping: A dictionary mapping DAGs to cluster id's.\n    \"\"\"\n    self.num_dags = 0\n\n    # If leaf, assign DAGs and set count.\n    if self.cluster_id is not None:\n        self.dags = [\n            key\n            for key in cluster_mapping.keys()\n            if cluster_mapping[key] == self.cluster_id\n        ]\n        self.num_dags = len(self.dags)\n\n    # Otherwise, recurse for children and retireve counts.\n    if self.left:\n        self.left.assign_dags_to_nodes(cluster_mapping)\n        self.num_dags += self.left.num_dags\n    if self.right:\n        self.right.assign_dags_to_nodes(cluster_mapping)\n        self.num_dags += self.right.num_dags\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.count_edge_occurrences","title":"<code>count_edge_occurrences(treatment, outcome, dag)</code>","text":"<p>Recursively count the number of times each edge occurs amongst the DAGs assigned to all the children of this node, omitting the edge from treatment -&gt; outcome, since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The outcome variable.</p> required <code>dag</code> <code>DiGraph</code> <p>The optional dag structure to ignore.</p> required Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def count_edge_occurrences(\n    self, treatment: str, outcome: str, dag: nx.DiGraph\n) -&gt; None:\n    \"\"\"\n    Recursively count the number of times each edge occurs amongst the DAGs\n    assigned to all the children of this node, omitting the edge from treatment -&gt; outcome,\n    since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.\n\n    Parameters:\n        treatment: The treatment variable.\n        outcome: The outcome variable.\n        dag: The optional dag structure to ignore.\n    \"\"\"\n    self.edge_counts: Types.EdgeCountDict = defaultdict(int)\n\n    # If leaf, actually compute count.\n    if self.cluster_id is not None:\n        edges_to_ignore = [(treatment, outcome)]\n        if dag:\n            edges_to_ignore.extend(dag.edges)\n        for graph in self.dags:\n            for edge in graph.edges:\n                if edge not in edges_to_ignore:\n                    self.edge_counts[edge] += 1\n\n    # Otherwise, derive counts from children.\n    if self.left:\n        self.left.count_edge_occurrences(treatment, outcome, dag)\n        for key in self.left.edge_counts.keys():\n            self.edge_counts[key] += self.left.edge_counts[key]\n    if self.right:\n        self.right.count_edge_occurrences(treatment, outcome, dag)\n        for key in self.right.edge_counts.keys():\n            self.edge_counts[key] += self.right.edge_counts[key]\n\n    # Compute statistics.\n    freq_counts = list(self.edge_counts.values())\n    if len(freq_counts) == 0:\n        self.mean = None\n        self.std_dev = None\n    else:\n        self.mean = np.mean(freq_counts)\n        self.std_dev = np.std(freq_counts)\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.calculate_edge_expectancy","title":"<code>calculate_edge_expectancy(totals=None)</code>","text":"<p>For each edge at each node, calculate what percent over or under expectancy the edge is at in relationship to its parent.</p> <p>Parameters:</p> Name Type Description Default <code>totals</code> <code>tuple[int, EdgeCountDict]</code> <p>A tuple containing the total number of DAGs and the mapping from edges to their counts for the parent of this node.</p> <code>None</code> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def calculate_edge_expectancy(\n    self, totals: tuple[int, Types.EdgeCountDict] = None\n) -&gt; None:\n    \"\"\"\n    For each edge at each node, calculate what percent over or under\n    expectancy the edge is at in relationship to its parent.\n\n    Parameters:\n        totals: A tuple containing the total number of DAGs and the mapping from\n            edges to their counts for the parent of this node.\n    \"\"\"\n    # At root node, calculate expectancy\n    if totals is None:\n        totals = (self.num_dags, self.edge_counts)\n\n    # Otherwise, calculate expectancy based on parent.\n    total_dags, total_edges = totals\n    self.percent_expectancy = defaultdict(float)\n\n    for edge in self.edge_counts.keys():\n        expected = self.num_dags / total_dags * total_edges[edge]\n        self.percent_expectancy[edge] = (\n            self.edge_counts[edge] - expected\n        ) / expected\n\n    # Recurse for children.\n    if self.left:\n        self.left.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n    if self.right:\n        self.right.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.find_outliers_in_tree","title":"<code>find_outliers_in_tree(threshold=0)</code>","text":"<p>Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an edge that is below expectancy on one side of the tree, and above on the other side, and optionally, over some threshold on both sides.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for an edge to be considered an outlier.</p> <code>0</code> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def find_outliers_in_tree(self, threshold: float = 0) -&gt; None:\n    \"\"\"\n    Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an\n    edge that is below expectancy on one side of the tree, and above on the other side, and\n    optionally, over some threshold on both sides.\n\n    Parameters:\n        threshold: The threshold for an edge to be considered an outlier.\n    \"\"\"\n\n    # If able to compare, find outliers.\n    if self.left and self.right:\n        self.left.outliers = {}\n        self.right.outliers = {}\n        edges = set(self.left.edge_counts.keys()).union(\n            set(self.right.edge_counts.keys())\n        )\n        for edge in edges:\n            if (\n                np.sign(self.left.percent_expectancy[edge])\n                != np.sign(self.right.percent_expectancy[edge])\n                and abs(self.left.percent_expectancy[edge]) &gt; threshold\n                and abs(self.right.percent_expectancy[edge]) &gt; threshold\n            ):\n                self.left.outliers[edge] = self.left.percent_expectancy[edge]\n                self.right.outliers[edge] = self.right.percent_expectancy[edge]\n\n    # Recurse for children.\n    if self.left:\n        self.left.find_outliers_in_tree(threshold)\n    if self.right:\n        self.right.find_outliers_in_tree(threshold)\n</code></pre>"},{"location":"reference/logos/edge_occurrence_tree/#logos.edge_occurrence_tree.EdgeOccurrenceTree.find_outliers_per_cluster","title":"<code>find_outliers_per_cluster(dag)</code>","text":"<p>Collect the edge counts and outliers found earlier into appropriate dictionaries per cluster.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DiGraph</code> <p>The DAG to ignore when collecting outliers.</p> required <p>Returns:</p> Type Description <code>EdgeCountDict</code> <p>A tuple containing the following: a dictionary mapping cluster id's to edge counts,</p> <code>dict[Edge, float]</code> <p>and a dictionary mapping cluster id's to outlier edges.</p> Source code in <code>src/logos/edge_occurrence_tree.py</code> <pre><code>def find_outliers_per_cluster(\n    self,\n    dag: nx.DiGraph,\n) -&gt; tuple[Types.EdgeCountDict, dict[Types.Edge, float]]:\n    \"\"\"\n    Collect the edge counts and outliers found earlier into appropriate dictionaries\n    per cluster.\n\n    Parameters:\n        dag: The DAG to ignore when collecting outliers.\n\n    Returns:\n        A tuple containing the following: a dictionary mapping cluster id's to edge counts,\n        and a dictionary mapping cluster id's to outlier edges.\n    \"\"\"\n\n    cluster_edge_count = {}\n    cluster_outliers = {}\n\n    # If leaf, add to cluster counts.\n    if self.cluster_id is not None:\n        cluster_edge_count[self.cluster_id] = self.edge_counts\n        edges_to_ignore = dag.edges if dag is not None else []\n        cluster_outliers[self.cluster_id] = {\n            edge: self.outliers[edge]\n            for edge in self.outliers\n            if edge not in edges_to_ignore\n        }\n\n    # Otherwise, recurse for children.\n    if self.left:\n        lec, lo = self.left.find_outliers_per_cluster(dag)\n        cluster_edge_count.update(lec)\n        cluster_outliers.update(lo)\n    if self.right:\n        rec, ro = self.right.find_outliers_per_cluster(dag)\n        cluster_edge_count.update(rec)\n        cluster_outliers.update(ro)\n\n    return cluster_edge_count, cluster_outliers\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/","title":"EdgeStateMatrix","text":""},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix","title":"<code>EdgeStateMatrix</code>","text":"<p>A class for managing an edge state matrix.</p> <p>An edge state matrix is square, with the entry (i,j) representing the state of the directed edge between nodes i and j. The state of an edge is one of:      0: The existence of the state is undecided.     -1: The edge does not exist.      1: The edge exists.</p> <p>Self-edges are not allowed. The presence of an edge implies the absence of its inverse.</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>class EdgeStateMatrix:\n    \"\"\"\n    A class for managing an edge state matrix.\n\n    An edge state matrix is square, with the entry (i,j) representing the state\n    of the directed edge between nodes i and j. The state of an edge is one of:\n         0: The existence of the state is undecided.\n        -1: The edge does not exist.\n         1: The edge exists.\n\n    Self-edges are not allowed. The presence of an edge implies the absence of\n    its inverse.\n    \"\"\"\n\n    def __init__(self, variables: list[str]) -&gt; None:\n        \"\"\"\n        Initialize the edge state matrix to the right dimensions and mark self-edges\n        as rejected and all other edges as undecided.\n\n        Parameters:\n            variables: The variables to initialize the edge state matrix based on. This\n                list must include variable NAMES, not tags.\n        \"\"\"\n\n        n = len(variables)\n        self._variables = variables\n        self._m = np.zeros((n, n))\n        for i in range(n):\n            self._m[i, i] = -1\n\n    @property\n    def m(self) -&gt; np.ndarray:\n        \"\"\"\n        Returns the edge state matrix.\n        \"\"\"\n        return self._m\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"\n        Returns the number of nodes.\n        \"\"\"\n        return self._m.shape[0]\n\n    def clear_and_set_from_graph(self, graph: nx.DiGraph) -&gt; None:\n        \"\"\"\n        Clear the edge state matrix and then set it based on the provided graph.\n        In particular, mark all edges in the graph as accepted and all others as rejected.\n\n        Parameters:\n            graph: The graph to use to set the edge states.\n        \"\"\"\n\n        self._m = np.zeros((self.n, self.n))\n        for edge in graph.edges:\n            print(\"Marking edge as accepted: \", edge)\n            self._m[self.idx(edge[0]), self.idx(edge[1])] = 1\n\n        self._m[self._m == 0] = -1\n\n    def clear_and_set_from_matrix(self, m: np.ndarray) -&gt; None:\n        \"\"\"\n        Clear the edge state matrix and then set it based on the provided matrix.\n\n        Parameters:\n            m: The matrix to use to set the edge states.\n        \"\"\"\n\n        self._m = m\n\n    def idx(self, var: str) -&gt; int:\n        \"\"\"\n        Retrieve the index of a variable in the edge state matrix.\n\n        Parameters:\n            var: The name or tag of the variable.\n\n        Returns:\n            The index of the variable in the edge state matrix.\n        \"\"\"\n        return self._variables.index(var)\n\n    def get_edge_state(self, src: str, dst: str) -&gt; str:\n        \"\"\"\n        Get the state of a specific edge.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n\n        Returns:\n            The state of the edge (Accepted, Rejected, or Undecided).\n        \"\"\"\n        src_idx = self.idx(src)\n        dst_idx = self.idx(dst)\n        return self.edge_state_to_str(self._m[src_idx][dst_idx])\n\n    def edge_state_to_str(self, state: int) -&gt; str:\n        \"\"\"\n        Translate between edge value and its interpretation.\n\n        Parameters:\n            state: The state of the edge represented as an integer.\n\n        Returns:\n            The state of the edge (Accepted, Rejected, or Undecided).\n        \"\"\"\n        if state == 0:\n            return \"Undecided\"\n        elif state == -1:\n            return \"Rejected\"\n        elif state == 1:\n            return \"Accepted\"\n        else:\n            raise ValueError(f\"Invalid edge state {state}\")\n\n    def mark_edge(self, src: str, dst: str, state: str) -&gt; list[str]:\n        \"\"\"\n        Mark an edge as being in a specified state.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            state: The state to mark the edge with (Accepted, Rejected, or Undecided).\n\n        Returns:\n            A list of variables that were removed from the partial causal graph as a result\n            of this edge being marked as Accepted.\n\n        Throws:\n            ValueError: If `state` is not one of \"Accepted\", \"Rejected\", or \"Undecided\".\n        \"\"\"\n\n        src_idx = self.idx(src)\n        dst_idx = self.idx(dst)\n\n        if state == \"Accepted\":\n            self._m[src_idx][dst_idx] = 1\n            self._m[dst_idx][src_idx] = -1\n            return self._reject_other_variants(src, dst)\n        elif state == \"Rejected\":\n            self._m[src_idx][dst_idx] = -1\n            return []\n        elif state == \"Undecided\":\n            self._m[src_idx][dst_idx] = 0\n            return []\n        else:\n            raise ValueError(f\"Invalid edge state {state}\")\n\n    def _reject_other_variants(self, src: str, dst: str) -&gt; list[str]:\n        \"\"\"\n        Mark any edges that touch a variable different from `src` and `dst`, but sharing\n        the same base variable as `src` or `dst`, as rejected. Also remove any such variables\n        from the partial causal graph.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n\n        Returns:\n            A list of variables that were removed from the partial causal graph as a result\n            of this edge being marked as Accepted.\n        \"\"\"\n\n        src_base = PreparedVariableName(src).base_var()\n        dst_base = PreparedVariableName(dst).base_var()\n\n        l = []\n        for var in self._variables:\n            var_base = PreparedVariableName(var).base_var()\n            if (var_base == src_base and var != src) or (\n                var_base == dst_base and var != dst\n            ):\n                self._m[self.idx(var), :] = -1\n                self._m[:, self.idx(var)] = -1\n                l.append(var)\n\n        return l \n\n    @staticmethod\n    def enumerate_with_max_edges(n: int, max_edges: int) -&gt; list[np.ndarray]:\n        \"\"\"\n        Enumerate all edge state matrices of dimension `n` with at most `max_edges` accepted edges.\n\n        Parameters:\n            n: The dimension of the edge state matrices.\n            max_edges: The maximum number of edges to allow in the edge state matrices.\n\n        Returns:\n            A list of edge state matrices.\n        \"\"\"\n        valid_matrices = {0: [np.full(shape=(n, n), fill_value=-1)]}\n\n        # Enumerate all valid matrices with k edges\n        for k in range(1, max_edges + 1):\n            valid_matrices[k] = []\n\n            # For each valid matrix with k-1 edges...\n            for m in valid_matrices[k - 1]:\n                # ...add a new edge in every possible way\n                for i in range(n):\n                    for j in range(i + 1, n):\n                        if m[i, j] &lt; 0 and m[j, i] &lt; 0:\n                            forward = m.copy()\n                            forward[i, j] = 1\n                            valid_matrices[k].append(forward)\n                            backward = m.copy()\n                            backward[j, i] = 1\n                            valid_matrices[k].append(backward)\n\n        # Flatten the collection of matrices into a single list\n        returned_matrices = []\n        for k in range(1, max_edges + 1):\n            returned_matrices.extend(valid_matrices[k])\n\n        return returned_matrices\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.m","title":"<code>m: np.ndarray</code>  <code>property</code>","text":"<p>Returns the edge state matrix.</p>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>Returns the number of nodes.</p>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.__init__","title":"<code>__init__(variables)</code>","text":"<p>Initialize the edge state matrix to the right dimensions and mark self-edges as rejected and all other edges as undecided.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>The variables to initialize the edge state matrix based on. This list must include variable NAMES, not tags.</p> required Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def __init__(self, variables: list[str]) -&gt; None:\n    \"\"\"\n    Initialize the edge state matrix to the right dimensions and mark self-edges\n    as rejected and all other edges as undecided.\n\n    Parameters:\n        variables: The variables to initialize the edge state matrix based on. This\n            list must include variable NAMES, not tags.\n    \"\"\"\n\n    n = len(variables)\n    self._variables = variables\n    self._m = np.zeros((n, n))\n    for i in range(n):\n        self._m[i, i] = -1\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.clear_and_set_from_graph","title":"<code>clear_and_set_from_graph(graph)</code>","text":"<p>Clear the edge state matrix and then set it based on the provided graph. In particular, mark all edges in the graph as accepted and all others as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to use to set the edge states.</p> required Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def clear_and_set_from_graph(self, graph: nx.DiGraph) -&gt; None:\n    \"\"\"\n    Clear the edge state matrix and then set it based on the provided graph.\n    In particular, mark all edges in the graph as accepted and all others as rejected.\n\n    Parameters:\n        graph: The graph to use to set the edge states.\n    \"\"\"\n\n    self._m = np.zeros((self.n, self.n))\n    for edge in graph.edges:\n        print(\"Marking edge as accepted: \", edge)\n        self._m[self.idx(edge[0]), self.idx(edge[1])] = 1\n\n    self._m[self._m == 0] = -1\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.clear_and_set_from_matrix","title":"<code>clear_and_set_from_matrix(m)</code>","text":"<p>Clear the edge state matrix and then set it based on the provided matrix.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray</code> <p>The matrix to use to set the edge states.</p> required Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def clear_and_set_from_matrix(self, m: np.ndarray) -&gt; None:\n    \"\"\"\n    Clear the edge state matrix and then set it based on the provided matrix.\n\n    Parameters:\n        m: The matrix to use to set the edge states.\n    \"\"\"\n\n    self._m = m\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.idx","title":"<code>idx(var)</code>","text":"<p>Retrieve the index of a variable in the edge state matrix.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the variable in the edge state matrix.</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def idx(self, var: str) -&gt; int:\n    \"\"\"\n    Retrieve the index of a variable in the edge state matrix.\n\n    Parameters:\n        var: The name or tag of the variable.\n\n    Returns:\n        The index of the variable in the edge state matrix.\n    \"\"\"\n    return self._variables.index(var)\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.get_edge_state","title":"<code>get_edge_state(src, dst)</code>","text":"<p>Get the state of a specific edge.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The state of the edge (Accepted, Rejected, or Undecided).</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def get_edge_state(self, src: str, dst: str) -&gt; str:\n    \"\"\"\n    Get the state of a specific edge.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n\n    Returns:\n        The state of the edge (Accepted, Rejected, or Undecided).\n    \"\"\"\n    src_idx = self.idx(src)\n    dst_idx = self.idx(dst)\n    return self.edge_state_to_str(self._m[src_idx][dst_idx])\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.edge_state_to_str","title":"<code>edge_state_to_str(state)</code>","text":"<p>Translate between edge value and its interpretation.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>int</code> <p>The state of the edge represented as an integer.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The state of the edge (Accepted, Rejected, or Undecided).</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def edge_state_to_str(self, state: int) -&gt; str:\n    \"\"\"\n    Translate between edge value and its interpretation.\n\n    Parameters:\n        state: The state of the edge represented as an integer.\n\n    Returns:\n        The state of the edge (Accepted, Rejected, or Undecided).\n    \"\"\"\n    if state == 0:\n        return \"Undecided\"\n    elif state == -1:\n        return \"Rejected\"\n    elif state == 1:\n        return \"Accepted\"\n    else:\n        raise ValueError(f\"Invalid edge state {state}\")\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.mark_edge","title":"<code>mark_edge(src, dst, state)</code>","text":"<p>Mark an edge as being in a specified state.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>state</code> <code>str</code> <p>The state to mark the edge with (Accepted, Rejected, or Undecided).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables that were removed from the partial causal graph as a result</p> <code>list[str]</code> <p>of this edge being marked as Accepted.</p> Throws <p>ValueError: If <code>state</code> is not one of \"Accepted\", \"Rejected\", or \"Undecided\".</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def mark_edge(self, src: str, dst: str, state: str) -&gt; list[str]:\n    \"\"\"\n    Mark an edge as being in a specified state.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        state: The state to mark the edge with (Accepted, Rejected, or Undecided).\n\n    Returns:\n        A list of variables that were removed from the partial causal graph as a result\n        of this edge being marked as Accepted.\n\n    Throws:\n        ValueError: If `state` is not one of \"Accepted\", \"Rejected\", or \"Undecided\".\n    \"\"\"\n\n    src_idx = self.idx(src)\n    dst_idx = self.idx(dst)\n\n    if state == \"Accepted\":\n        self._m[src_idx][dst_idx] = 1\n        self._m[dst_idx][src_idx] = -1\n        return self._reject_other_variants(src, dst)\n    elif state == \"Rejected\":\n        self._m[src_idx][dst_idx] = -1\n        return []\n    elif state == \"Undecided\":\n        self._m[src_idx][dst_idx] = 0\n        return []\n    else:\n        raise ValueError(f\"Invalid edge state {state}\")\n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix._reject_other_variants","title":"<code>_reject_other_variants(src, dst)</code>","text":"<p>Mark any edges that touch a variable different from <code>src</code> and <code>dst</code>, but sharing the same base variable as <code>src</code> or <code>dst</code>, as rejected. Also remove any such variables from the partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables that were removed from the partial causal graph as a result</p> <code>list[str]</code> <p>of this edge being marked as Accepted.</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>def _reject_other_variants(self, src: str, dst: str) -&gt; list[str]:\n    \"\"\"\n    Mark any edges that touch a variable different from `src` and `dst`, but sharing\n    the same base variable as `src` or `dst`, as rejected. Also remove any such variables\n    from the partial causal graph.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n\n    Returns:\n        A list of variables that were removed from the partial causal graph as a result\n        of this edge being marked as Accepted.\n    \"\"\"\n\n    src_base = PreparedVariableName(src).base_var()\n    dst_base = PreparedVariableName(dst).base_var()\n\n    l = []\n    for var in self._variables:\n        var_base = PreparedVariableName(var).base_var()\n        if (var_base == src_base and var != src) or (\n            var_base == dst_base and var != dst\n        ):\n            self._m[self.idx(var), :] = -1\n            self._m[:, self.idx(var)] = -1\n            l.append(var)\n\n    return l \n</code></pre>"},{"location":"reference/logos/edge_state_matrix/#logos.edge_state_matrix.EdgeStateMatrix.enumerate_with_max_edges","title":"<code>enumerate_with_max_edges(n, max_edges)</code>  <code>staticmethod</code>","text":"<p>Enumerate all edge state matrices of dimension <code>n</code> with at most <code>max_edges</code> accepted edges.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The dimension of the edge state matrices.</p> required <code>max_edges</code> <code>int</code> <p>The maximum number of edges to allow in the edge state matrices.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>A list of edge state matrices.</p> Source code in <code>src/logos/edge_state_matrix.py</code> <pre><code>@staticmethod\ndef enumerate_with_max_edges(n: int, max_edges: int) -&gt; list[np.ndarray]:\n    \"\"\"\n    Enumerate all edge state matrices of dimension `n` with at most `max_edges` accepted edges.\n\n    Parameters:\n        n: The dimension of the edge state matrices.\n        max_edges: The maximum number of edges to allow in the edge state matrices.\n\n    Returns:\n        A list of edge state matrices.\n    \"\"\"\n    valid_matrices = {0: [np.full(shape=(n, n), fill_value=-1)]}\n\n    # Enumerate all valid matrices with k edges\n    for k in range(1, max_edges + 1):\n        valid_matrices[k] = []\n\n        # For each valid matrix with k-1 edges...\n        for m in valid_matrices[k - 1]:\n            # ...add a new edge in every possible way\n            for i in range(n):\n                for j in range(i + 1, n):\n                    if m[i, j] &lt; 0 and m[j, i] &lt; 0:\n                        forward = m.copy()\n                        forward[i, j] = 1\n                        valid_matrices[k].append(forward)\n                        backward = m.copy()\n                        backward[j, i] = 1\n                        valid_matrices[k].append(backward)\n\n    # Flatten the collection of matrices into a single list\n    returned_matrices = []\n    for k in range(1, max_edges + 1):\n        returned_matrices.extend(valid_matrices[k])\n\n    return returned_matrices\n</code></pre>"},{"location":"reference/logos/graph_renderer/","title":"GraphRenderer","text":""},{"location":"reference/logos/graph_renderer/#logos.graph_renderer.GraphRenderer","title":"<code>GraphRenderer</code>","text":"<p>Render a digraph with appropriate margins and node tags.</p> Source code in <code>src/logos/graph_renderer.py</code> <pre><code>class GraphRenderer:\n    \"\"\"\n    Render a digraph with appropriate margins and node tags.\n    \"\"\"\n\n    @staticmethod\n    def draw_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Draw a graph with appropriate margins and node tags.\n\n        Parameters:\n            graph: The graph to be drawn.\n            var_info: A dataframe containing the tags of the variables in the\n                graph.\n\n        Returns:\n            A base64-encoded string representation of the graph.\n        \"\"\"\n        if graph.number_of_nodes() == 0:\n            return \"\"\n\n        pos = nx.spring_layout(graph)\n        nx.draw(\n            graph,\n            pos,\n            edgelist=graph.edges(),\n            with_labels=False,\n            width=2.0,\n            node_color=\"#d3d3d3\",\n            edge_color=[graph[u][v].get(\"color\", \"#7f9aba\") for u, v in graph.edges()],\n        )\n        node_labels = {\n            n: (\n                n\n                if len(var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values) == 0\n                else var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]\n            )\n            for n in list(graph.nodes)\n        }\n        text = nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n        for _, t in text.items():\n            t.set_rotation(30)\n\n        # Fix margins\n        x_values, y_values = zip(*pos.values())\n        x_max, x_min = max(x_values), min(x_values)\n        y_max, y_min = max(y_values), min(y_values)\n        if x_max != x_min:\n            x_margin = (x_max - x_min) * 0.3\n            plt.xlim(x_min - x_margin, x_max + x_margin)\n        if y_max != y_min:\n            y_margin = (y_max - y_min) * 0.3\n            plt.ylim(y_min - y_margin, y_max + y_margin)\n\n        buffer = BytesIO()\n        plt.savefig(buffer, format=\"png\")\n        plt.clf()\n        img_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        plt.close()\n\n        return img_str\n\n    @staticmethod\n    def save_graph(graph: nx.DiGraph, var_info: pd.DataFrame, filename: str) -&gt; None:\n        \"\"\"\n        Save the graph to a file as a png image.\n\n        Parameters:\n            graph: The graph to be saved.\n            var_info: A dataframe containing the tags of the variables in the\n                graph.\n            filename: The name of the file to which the graph should be saved.\n        \"\"\"\n        img_str = GraphRenderer.draw_graph(graph, var_info)\n        with open(filename, \"wb\") as f:\n            f.write(base64.b64decode(img_str))\n\n    @staticmethod\n    def graph_string_to_html(graph: str) -&gt; HTML:\n        \"\"\"\n        Convert the string representation of the rgaph to an HTML object\n\n        Parameters:\n            graph: The graph to be displayed.\n        \"\"\"\n        return HTML('&lt;img src=\"data:image/png;base64,{}\" style=\"max-width: 100%; height: auto;\"&gt;'.format(graph))\n\n    @staticmethod\n    def display_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Display the graph.\n\n        Parameters:\n            graph: The graph to be displayed.\n            var_info: A dataframe containing the tags of the variables in the\n                graph.\n        \"\"\"\n        display(\n            GraphRenderer.graph_string_to_html(\n                GraphRenderer.draw_graph(graph, var_info)\n            )\n        )\n</code></pre>"},{"location":"reference/logos/graph_renderer/#logos.graph_renderer.GraphRenderer.draw_graph","title":"<code>draw_graph(graph, var_info)</code>  <code>staticmethod</code>","text":"<p>Draw a graph with appropriate margins and node tags.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be drawn.</p> required <code>var_info</code> <code>DataFrame</code> <p>A dataframe containing the tags of the variables in the graph.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A base64-encoded string representation of the graph.</p> Source code in <code>src/logos/graph_renderer.py</code> <pre><code>@staticmethod\ndef draw_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Draw a graph with appropriate margins and node tags.\n\n    Parameters:\n        graph: The graph to be drawn.\n        var_info: A dataframe containing the tags of the variables in the\n            graph.\n\n    Returns:\n        A base64-encoded string representation of the graph.\n    \"\"\"\n    if graph.number_of_nodes() == 0:\n        return \"\"\n\n    pos = nx.spring_layout(graph)\n    nx.draw(\n        graph,\n        pos,\n        edgelist=graph.edges(),\n        with_labels=False,\n        width=2.0,\n        node_color=\"#d3d3d3\",\n        edge_color=[graph[u][v].get(\"color\", \"#7f9aba\") for u, v in graph.edges()],\n    )\n    node_labels = {\n        n: (\n            n\n            if len(var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values) == 0\n            else var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]\n        )\n        for n in list(graph.nodes)\n    }\n    text = nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n    for _, t in text.items():\n        t.set_rotation(30)\n\n    # Fix margins\n    x_values, y_values = zip(*pos.values())\n    x_max, x_min = max(x_values), min(x_values)\n    y_max, y_min = max(y_values), min(y_values)\n    if x_max != x_min:\n        x_margin = (x_max - x_min) * 0.3\n        plt.xlim(x_min - x_margin, x_max + x_margin)\n    if y_max != y_min:\n        y_margin = (y_max - y_min) * 0.3\n        plt.ylim(y_min - y_margin, y_max + y_margin)\n\n    buffer = BytesIO()\n    plt.savefig(buffer, format=\"png\")\n    plt.clf()\n    img_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n    plt.close()\n\n    return img_str\n</code></pre>"},{"location":"reference/logos/graph_renderer/#logos.graph_renderer.GraphRenderer.save_graph","title":"<code>save_graph(graph, var_info, filename)</code>  <code>staticmethod</code>","text":"<p>Save the graph to a file as a png image.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be saved.</p> required <code>var_info</code> <code>DataFrame</code> <p>A dataframe containing the tags of the variables in the graph.</p> required <code>filename</code> <code>str</code> <p>The name of the file to which the graph should be saved.</p> required Source code in <code>src/logos/graph_renderer.py</code> <pre><code>@staticmethod\ndef save_graph(graph: nx.DiGraph, var_info: pd.DataFrame, filename: str) -&gt; None:\n    \"\"\"\n    Save the graph to a file as a png image.\n\n    Parameters:\n        graph: The graph to be saved.\n        var_info: A dataframe containing the tags of the variables in the\n            graph.\n        filename: The name of the file to which the graph should be saved.\n    \"\"\"\n    img_str = GraphRenderer.draw_graph(graph, var_info)\n    with open(filename, \"wb\") as f:\n        f.write(base64.b64decode(img_str))\n</code></pre>"},{"location":"reference/logos/graph_renderer/#logos.graph_renderer.GraphRenderer.graph_string_to_html","title":"<code>graph_string_to_html(graph)</code>  <code>staticmethod</code>","text":"<p>Convert the string representation of the rgaph to an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>str</code> <p>The graph to be displayed.</p> required Source code in <code>src/logos/graph_renderer.py</code> <pre><code>@staticmethod\ndef graph_string_to_html(graph: str) -&gt; HTML:\n    \"\"\"\n    Convert the string representation of the rgaph to an HTML object\n\n    Parameters:\n        graph: The graph to be displayed.\n    \"\"\"\n    return HTML('&lt;img src=\"data:image/png;base64,{}\" style=\"max-width: 100%; height: auto;\"&gt;'.format(graph))\n</code></pre>"},{"location":"reference/logos/graph_renderer/#logos.graph_renderer.GraphRenderer.display_graph","title":"<code>display_graph(graph, var_info)</code>  <code>staticmethod</code>","text":"<p>Display the graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be displayed.</p> required <code>var_info</code> <code>DataFrame</code> <p>A dataframe containing the tags of the variables in the graph.</p> required Source code in <code>src/logos/graph_renderer.py</code> <pre><code>@staticmethod\ndef display_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Display the graph.\n\n    Parameters:\n        graph: The graph to be displayed.\n        var_info: A dataframe containing the tags of the variables in the\n            graph.\n    \"\"\"\n    display(\n        GraphRenderer.graph_string_to_html(\n            GraphRenderer.draw_graph(graph, var_info)\n        )\n    )\n</code></pre>"},{"location":"reference/logos/interactive_causal_graph_refiner/","title":"InteractiveCausalGraphRefiner","text":""},{"location":"reference/logos/interactive_causal_graph_refiner/#logos.interactive_causal_graph_refiner.InteractiveCausalGraphRefiner","title":"<code>InteractiveCausalGraphRefiner</code>","text":"Source code in <code>src/logos/interactive_causal_graph_refiner.py</code> <pre><code>class InteractiveCausalGraphRefiner:\n\n    @staticmethod\n    def get_suggestion(\n        data: pd.DataFrame,\n        method: InteractiveCausalGraphRefinerMethod,\n        eccs: Optional[ECCS] = None,\n        treatment_name: Optional[str] = None,\n        outcome_name: Optional[str] = None,\n        graph: Optional[nx.DiGraph] = None,\n        model: Optional[str] = None,\n        gpt_log_path: Optional[str] = None,\n        data_tags: Optional[pd.DataFrame] = None,\n    ) -&gt; Edge:\n        \"\"\"\n        Get the next edge for which the user should porduce a judgment, in the\n        process of refining a causal graph.\n\n        Parameters:\n            data: The dataframe containing the data.\n            method: The method to use for suggesting the next edge.\n            eccs: The ECCS object to use for suggesting the next edge. Only applies\n                if `method` is `InteractiveCausalGraphRefinerMethod.LOGOS`.\n            treatment_name: The name of the treatment variable. Only applies if\n                `method` is `InteractiveCausalGraphRefinerMethod.LOGOS` or\n                `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n            outcome_name: The name of the outcome variable. Only applies if\n                `method` is `InteractiveCausalGraphRefinerMethod.LOGOS` or\n                `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n            graph: The graph to use for suggesting the next edge. Only applies if\n                `method` is `InteractiveCausalGraphRefinerMethod.REGRESSION` or\n                `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n            model: The model to use for suggesting the next edge. Only applies if\n                `method` is not `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n            gpt_log_path: The path to the GPT log file. Only applies if `method` is\n                `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n            data_tags: The dataframe containing the data tags. Only applies if `method`\n                is `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n\n        Returns:\n            The next edge for which the user should produce a judgment.\n        \"\"\"\n        if method == InteractiveCausalGraphRefinerMethod.LOGOS:\n            return InteractiveCausalGraphRefiner._get_suggestion_logos(\n                eccs, treatment_name, outcome_name\n            )\n        elif method == InteractiveCausalGraphRefinerMethod.REGRESSION:\n            return InteractiveCausalGraphRefiner._get_suggestion_regression(data, graph)\n        elif method == InteractiveCausalGraphRefinerMethod.LANGMODEL:\n            return InteractiveCausalGraphRefiner._get_suggestion_langmodel(\n                data,\n                data_tags,\n                treatment_name,\n                outcome_name,\n                graph,\n                model,\n                gpt_log_path,\n            )\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n    @staticmethod\n    def _get_suggestion_logos(\n        eccs: ECCS, treatment_name: str, outcome_name: str\n    ) -&gt; Edge:\n        \"\"\"\n        Implement `get_suggestion()` for the `LOGOS` method.\n\n        Parameters:\n            eccs: The ECCS object to use for suggesting the next edge.\n            treatment_name: The name of the treatment variable.\n            outcome_name: The name of the outcome variable.\n\n        Returns:\n            The next edge for which the user should produce a judgment.\n        \"\"\"\n        eccs.set_treatment(treatment_name)\n        eccs.set_outcome(outcome_name)\n        edge_edits, _, _ = eccs.suggest_best_single_adjustment_set_change(\n            max_results=1, use_optimized=True\n        )\n        return edge_edits[0].edge if (edge_edits and len(edge_edits) &gt; 0) else None\n\n    most_recent_graph = None\n    cache = []\n\n    @classmethod\n    def _get_suggestion_regression(cls, data: pd.DataFrame, graph: nx.DiGraph) -&gt; Edge:\n        \"\"\"\n        Implement `get_suggestion()` for the `REGRESSION` method.\n\n        Parameters:\n            data: The dataframe containing the data.\n            graph: The graph to use for suggesting the next edge.\n        \"\"\"\n        if graph != cls.most_recent_graph:\n            cls.most_recent_graph = graph\n            cls.cache = []\n        if len(cls.cache) &gt; 0:\n            return cls.cache.pop(0)\n\n        l = []\n\n        data, _ = Regression.get_normalized_copy(data)\n\n        for v in graph.nodes:\n            for w in set(data.columns) - set(graph.neighbors(v)) - set([v]):\n                d = Regression.ols(w, data[w], data[v])\n                abs_slope = abs(d[\"Slope\"])\n                l.append((Edge((w, v)), abs_slope))\n\n        l.sort(key=lambda x: x[1], reverse=True)\n        cls.cache = [row[0] for row in l[1:]]\n\n        return l[0][0]\n\n    @classmethod\n    def _get_suggestion_langmodel(\n        cls,\n        data: pd.DataFrame,\n        data_tags: pd.DataFrame,\n        treatment_name: str,\n        outcome_name: str,\n        graph: nx.DiGraph,\n        model: str = \"gpt-4o-mini-2024-07-18\",\n        gpt_log_path: Optional[str] = None,\n    ) -&gt; Edge:\n        \"\"\"\n        Implement `get_suggestion()` for the `LANGMODEL` method.\n\n        Parameters:\n            data: The dataframe containing the data.\n            treatment_name: The name of the treatment variable.\n            outcome_name: The name of the outcome variable.\n            graph: The graph to use for suggesting the next edge.\n            model: The model to use for suggesting the next edge.\n            gpt_log_path: The path to the GPT log file.\n            data_tags: The dataframe containing the data tags.\n        \"\"\"\n        if graph != cls.most_recent_graph:\n            cls.most_recent_graph = graph\n            cls.cache = []\n        if len(cls.cache) &gt; 0:\n            return cls.cache.pop(0)\n\n        client = OpenAI()\n\n        treatment_tag = TagUtils.tag_of(data_tags, treatment_name, \"prepared\")\n        outcome_tag = TagUtils.tag_of(data_tags, outcome_name, \"prepared\")\n\n        num_samples_per_var = 3\n\n        if gpt_log_path == None:\n            gpt_log_path = (\n                f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n            )\n        with open(gpt_log_path, \"w+\") as f:\n\n            # Define the messages to send to the model\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant for causal reasoning.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Below is a list of variable names and some example distinct values for each. \"\"\"\n                    f\"\"\"The lists are not sorted in compatible ways, so that elements in the same position may not correspond to the same entity. \"\"\"\n                    f\"\"\"{', '.join([f'{TagUtils.tag_of(data_tags, v, \"prepared\")}: [{\", \".join(str(x) for x in data[v].unique().tolist()[:num_samples_per_var])}]' for v in data.columns])}\"\"\"\n                    \"\"\"\\n\\n\"\"\"\n                    \"\"\"I have constructed a partial causal graph over these variables. Here is the list of directed edges: \"\"\"\n                    f\"\"\"[{', '.join([f'({TagUtils.tag_of(data_tags, u, \"prepared\")}, {TagUtils.tag_of(data_tags, v, \"prepared\")})' for u, v in graph.edges])}]\"\"\"\n                    \"\"\"\\n\\n\"\"\"\n                    f\"\"\"I plan to use this causal graph to calculate the ATE of {treatment_tag} on {outcome_tag}. \"\"\"\n                    \"\"\"However, I'm not sure of its correctness nor completeness. \"\"\"\n                    \"\"\"I want you to rank pairs of variables from this collection of variables, based on how important it is for me to either add or remove an edge between them in the graph\"\"\"\n                    \"\"\" for the accuracy of my ATE calculation. \"\"\"\n                    \"\"\"I understand that you may think this is speculative, but I want you to do your best to come up with such a ranked list ALWAYS. \"\"\"\n                    \"\"\"I will interpret any results you give me knowing that you may not be sure about them. \"\"\"\n                    \"\"\"Only return the ranked answers, one per line, preceded by a number and a period. Separate each variable in a pair with a comma. \"\"\"\n                    \"\"\"Do not return any other text before or after the list.\"\"\",\n                },\n            ]\n\n            reply = (\n                client.chat.completions.create(model=model, messages=messages)\n                .choices[0]\n                .message.content\n            )\n\n            # Log the messages and the reply\n            f.write(f\"{datetime.now()}\\n\")\n            f.write(\"Messages:\\n\")\n            for message in messages:\n                f.write(f\"{message['role']}: {message['content']}\\n\")\n            f.write(\"----------------\\n\")\n            f.write(f\"Reply: {reply}\\n\\n\")\n            f.write(\"================\\n\")\n            f.flush()\n            f.close()\n\n        # Combat hallucinations\n        reply_rows = reply.split(\"\\n\")\n        reply_rows = [\n            row for row in reply_rows if row.strip() != \"\" and row[0].isdigit()\n        ]\n        possibly_ranked_edges = [\n            [v.strip() for v in \".\".join(row.split(\".\")[1:]).strip().split(\",\")]\n            for row in reply_rows\n        ]\n        print(possibly_ranked_edges)\n        ranked_edges = []\n        tags = data_tags[\"Tag\"].values\n        for edge in possibly_ranked_edges:\n            if len(edge) != 2:\n                continue\n\n            left = None\n            right = None\n\n            if edge[0] in tags:\n                left = edge[0]\n            elif f\"{edge[0]} mean\" in tags:\n                left = f\"{edge[0]} mean\"\n\n            if edge[1] in tags:\n                right = edge[1]\n            elif f\"{edge[1]} mean\" in tags:\n                right = f\"{edge[1]} mean\"\n\n            if left is not None and right is not None:\n                ranked_edges.append(Edge((left, right)))\n\n        cls.cache = ranked_edges[1:]\n        return ranked_edges[0]\n</code></pre>"},{"location":"reference/logos/interactive_causal_graph_refiner/#logos.interactive_causal_graph_refiner.InteractiveCausalGraphRefiner.get_suggestion","title":"<code>get_suggestion(data, method, eccs=None, treatment_name=None, outcome_name=None, graph=None, model=None, gpt_log_path=None, data_tags=None)</code>  <code>staticmethod</code>","text":"<p>Get the next edge for which the user should porduce a judgment, in the process of refining a causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>method</code> <code>InteractiveCausalGraphRefinerMethod</code> <p>The method to use for suggesting the next edge.</p> required <code>eccs</code> <code>Optional[ECCS]</code> <p>The ECCS object to use for suggesting the next edge. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.LOGOS</code>.</p> <code>None</code> <code>treatment_name</code> <code>Optional[str]</code> <p>The name of the treatment variable. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.LOGOS</code> or <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <code>outcome_name</code> <code>Optional[str]</code> <p>The name of the outcome variable. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.LOGOS</code> or <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <code>graph</code> <code>Optional[DiGraph]</code> <p>The graph to use for suggesting the next edge. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.REGRESSION</code> or <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for suggesting the next edge. Only applies if <code>method</code> is not <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the GPT log file. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <code>data_tags</code> <code>Optional[DataFrame]</code> <p>The dataframe containing the data tags. Only applies if <code>method</code> is <code>InteractiveCausalGraphRefinerMethod.LANGMODEL</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Edge</code> <p>The next edge for which the user should produce a judgment.</p> Source code in <code>src/logos/interactive_causal_graph_refiner.py</code> <pre><code>@staticmethod\ndef get_suggestion(\n    data: pd.DataFrame,\n    method: InteractiveCausalGraphRefinerMethod,\n    eccs: Optional[ECCS] = None,\n    treatment_name: Optional[str] = None,\n    outcome_name: Optional[str] = None,\n    graph: Optional[nx.DiGraph] = None,\n    model: Optional[str] = None,\n    gpt_log_path: Optional[str] = None,\n    data_tags: Optional[pd.DataFrame] = None,\n) -&gt; Edge:\n    \"\"\"\n    Get the next edge for which the user should porduce a judgment, in the\n    process of refining a causal graph.\n\n    Parameters:\n        data: The dataframe containing the data.\n        method: The method to use for suggesting the next edge.\n        eccs: The ECCS object to use for suggesting the next edge. Only applies\n            if `method` is `InteractiveCausalGraphRefinerMethod.LOGOS`.\n        treatment_name: The name of the treatment variable. Only applies if\n            `method` is `InteractiveCausalGraphRefinerMethod.LOGOS` or\n            `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n        outcome_name: The name of the outcome variable. Only applies if\n            `method` is `InteractiveCausalGraphRefinerMethod.LOGOS` or\n            `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n        graph: The graph to use for suggesting the next edge. Only applies if\n            `method` is `InteractiveCausalGraphRefinerMethod.REGRESSION` or\n            `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n        model: The model to use for suggesting the next edge. Only applies if\n            `method` is not `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n        gpt_log_path: The path to the GPT log file. Only applies if `method` is\n            `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n        data_tags: The dataframe containing the data tags. Only applies if `method`\n            is `InteractiveCausalGraphRefinerMethod.LANGMODEL`.\n\n    Returns:\n        The next edge for which the user should produce a judgment.\n    \"\"\"\n    if method == InteractiveCausalGraphRefinerMethod.LOGOS:\n        return InteractiveCausalGraphRefiner._get_suggestion_logos(\n            eccs, treatment_name, outcome_name\n        )\n    elif method == InteractiveCausalGraphRefinerMethod.REGRESSION:\n        return InteractiveCausalGraphRefiner._get_suggestion_regression(data, graph)\n    elif method == InteractiveCausalGraphRefinerMethod.LANGMODEL:\n        return InteractiveCausalGraphRefiner._get_suggestion_langmodel(\n            data,\n            data_tags,\n            treatment_name,\n            outcome_name,\n            graph,\n            model,\n            gpt_log_path,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n</code></pre>"},{"location":"reference/logos/interactive_causal_graph_refiner/#logos.interactive_causal_graph_refiner.InteractiveCausalGraphRefiner._get_suggestion_logos","title":"<code>_get_suggestion_logos(eccs, treatment_name, outcome_name)</code>  <code>staticmethod</code>","text":"<p>Implement <code>get_suggestion()</code> for the <code>LOGOS</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>eccs</code> <code>ECCS</code> <p>The ECCS object to use for suggesting the next edge.</p> required <code>treatment_name</code> <code>str</code> <p>The name of the treatment variable.</p> required <code>outcome_name</code> <code>str</code> <p>The name of the outcome variable.</p> required <p>Returns:</p> Type Description <code>Edge</code> <p>The next edge for which the user should produce a judgment.</p> Source code in <code>src/logos/interactive_causal_graph_refiner.py</code> <pre><code>@staticmethod\ndef _get_suggestion_logos(\n    eccs: ECCS, treatment_name: str, outcome_name: str\n) -&gt; Edge:\n    \"\"\"\n    Implement `get_suggestion()` for the `LOGOS` method.\n\n    Parameters:\n        eccs: The ECCS object to use for suggesting the next edge.\n        treatment_name: The name of the treatment variable.\n        outcome_name: The name of the outcome variable.\n\n    Returns:\n        The next edge for which the user should produce a judgment.\n    \"\"\"\n    eccs.set_treatment(treatment_name)\n    eccs.set_outcome(outcome_name)\n    edge_edits, _, _ = eccs.suggest_best_single_adjustment_set_change(\n        max_results=1, use_optimized=True\n    )\n    return edge_edits[0].edge if (edge_edits and len(edge_edits) &gt; 0) else None\n</code></pre>"},{"location":"reference/logos/interactive_causal_graph_refiner/#logos.interactive_causal_graph_refiner.InteractiveCausalGraphRefiner._get_suggestion_regression","title":"<code>_get_suggestion_regression(data, graph)</code>  <code>classmethod</code>","text":"<p>Implement <code>get_suggestion()</code> for the <code>REGRESSION</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>graph</code> <code>DiGraph</code> <p>The graph to use for suggesting the next edge.</p> required Source code in <code>src/logos/interactive_causal_graph_refiner.py</code> <pre><code>@classmethod\ndef _get_suggestion_regression(cls, data: pd.DataFrame, graph: nx.DiGraph) -&gt; Edge:\n    \"\"\"\n    Implement `get_suggestion()` for the `REGRESSION` method.\n\n    Parameters:\n        data: The dataframe containing the data.\n        graph: The graph to use for suggesting the next edge.\n    \"\"\"\n    if graph != cls.most_recent_graph:\n        cls.most_recent_graph = graph\n        cls.cache = []\n    if len(cls.cache) &gt; 0:\n        return cls.cache.pop(0)\n\n    l = []\n\n    data, _ = Regression.get_normalized_copy(data)\n\n    for v in graph.nodes:\n        for w in set(data.columns) - set(graph.neighbors(v)) - set([v]):\n            d = Regression.ols(w, data[w], data[v])\n            abs_slope = abs(d[\"Slope\"])\n            l.append((Edge((w, v)), abs_slope))\n\n    l.sort(key=lambda x: x[1], reverse=True)\n    cls.cache = [row[0] for row in l[1:]]\n\n    return l[0][0]\n</code></pre>"},{"location":"reference/logos/interactive_causal_graph_refiner/#logos.interactive_causal_graph_refiner.InteractiveCausalGraphRefiner._get_suggestion_langmodel","title":"<code>_get_suggestion_langmodel(data, data_tags, treatment_name, outcome_name, graph, model='gpt-4o-mini-2024-07-18', gpt_log_path=None)</code>  <code>classmethod</code>","text":"<p>Implement <code>get_suggestion()</code> for the <code>LANGMODEL</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>treatment_name</code> <code>str</code> <p>The name of the treatment variable.</p> required <code>outcome_name</code> <code>str</code> <p>The name of the outcome variable.</p> required <code>graph</code> <code>DiGraph</code> <p>The graph to use for suggesting the next edge.</p> required <code>model</code> <code>str</code> <p>The model to use for suggesting the next edge.</p> <code>'gpt-4o-mini-2024-07-18'</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the GPT log file.</p> <code>None</code> <code>data_tags</code> <code>DataFrame</code> <p>The dataframe containing the data tags.</p> required Source code in <code>src/logos/interactive_causal_graph_refiner.py</code> <pre><code>@classmethod\ndef _get_suggestion_langmodel(\n    cls,\n    data: pd.DataFrame,\n    data_tags: pd.DataFrame,\n    treatment_name: str,\n    outcome_name: str,\n    graph: nx.DiGraph,\n    model: str = \"gpt-4o-mini-2024-07-18\",\n    gpt_log_path: Optional[str] = None,\n) -&gt; Edge:\n    \"\"\"\n    Implement `get_suggestion()` for the `LANGMODEL` method.\n\n    Parameters:\n        data: The dataframe containing the data.\n        treatment_name: The name of the treatment variable.\n        outcome_name: The name of the outcome variable.\n        graph: The graph to use for suggesting the next edge.\n        model: The model to use for suggesting the next edge.\n        gpt_log_path: The path to the GPT log file.\n        data_tags: The dataframe containing the data tags.\n    \"\"\"\n    if graph != cls.most_recent_graph:\n        cls.most_recent_graph = graph\n        cls.cache = []\n    if len(cls.cache) &gt; 0:\n        return cls.cache.pop(0)\n\n    client = OpenAI()\n\n    treatment_tag = TagUtils.tag_of(data_tags, treatment_name, \"prepared\")\n    outcome_tag = TagUtils.tag_of(data_tags, outcome_name, \"prepared\")\n\n    num_samples_per_var = 3\n\n    if gpt_log_path == None:\n        gpt_log_path = (\n            f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n        )\n    with open(gpt_log_path, \"w+\") as f:\n\n        # Define the messages to send to the model\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant for causal reasoning.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Below is a list of variable names and some example distinct values for each. \"\"\"\n                f\"\"\"The lists are not sorted in compatible ways, so that elements in the same position may not correspond to the same entity. \"\"\"\n                f\"\"\"{', '.join([f'{TagUtils.tag_of(data_tags, v, \"prepared\")}: [{\", \".join(str(x) for x in data[v].unique().tolist()[:num_samples_per_var])}]' for v in data.columns])}\"\"\"\n                \"\"\"\\n\\n\"\"\"\n                \"\"\"I have constructed a partial causal graph over these variables. Here is the list of directed edges: \"\"\"\n                f\"\"\"[{', '.join([f'({TagUtils.tag_of(data_tags, u, \"prepared\")}, {TagUtils.tag_of(data_tags, v, \"prepared\")})' for u, v in graph.edges])}]\"\"\"\n                \"\"\"\\n\\n\"\"\"\n                f\"\"\"I plan to use this causal graph to calculate the ATE of {treatment_tag} on {outcome_tag}. \"\"\"\n                \"\"\"However, I'm not sure of its correctness nor completeness. \"\"\"\n                \"\"\"I want you to rank pairs of variables from this collection of variables, based on how important it is for me to either add or remove an edge between them in the graph\"\"\"\n                \"\"\" for the accuracy of my ATE calculation. \"\"\"\n                \"\"\"I understand that you may think this is speculative, but I want you to do your best to come up with such a ranked list ALWAYS. \"\"\"\n                \"\"\"I will interpret any results you give me knowing that you may not be sure about them. \"\"\"\n                \"\"\"Only return the ranked answers, one per line, preceded by a number and a period. Separate each variable in a pair with a comma. \"\"\"\n                \"\"\"Do not return any other text before or after the list.\"\"\",\n            },\n        ]\n\n        reply = (\n            client.chat.completions.create(model=model, messages=messages)\n            .choices[0]\n            .message.content\n        )\n\n        # Log the messages and the reply\n        f.write(f\"{datetime.now()}\\n\")\n        f.write(\"Messages:\\n\")\n        for message in messages:\n            f.write(f\"{message['role']}: {message['content']}\\n\")\n        f.write(\"----------------\\n\")\n        f.write(f\"Reply: {reply}\\n\\n\")\n        f.write(\"================\\n\")\n        f.flush()\n        f.close()\n\n    # Combat hallucinations\n    reply_rows = reply.split(\"\\n\")\n    reply_rows = [\n        row for row in reply_rows if row.strip() != \"\" and row[0].isdigit()\n    ]\n    possibly_ranked_edges = [\n        [v.strip() for v in \".\".join(row.split(\".\")[1:]).strip().split(\",\")]\n        for row in reply_rows\n    ]\n    print(possibly_ranked_edges)\n    ranked_edges = []\n    tags = data_tags[\"Tag\"].values\n    for edge in possibly_ranked_edges:\n        if len(edge) != 2:\n            continue\n\n        left = None\n        right = None\n\n        if edge[0] in tags:\n            left = edge[0]\n        elif f\"{edge[0]} mean\" in tags:\n            left = f\"{edge[0]} mean\"\n\n        if edge[1] in tags:\n            right = edge[1]\n        elif f\"{edge[1]} mean\" in tags:\n            right = f\"{edge[1]} mean\"\n\n        if left is not None and right is not None:\n            ranked_edges.append(Edge((left, right)))\n\n    cls.cache = ranked_edges[1:]\n    return ranked_edges[0]\n</code></pre>"},{"location":"reference/logos/logos/","title":"LOGos","text":""},{"location":"reference/logos/logos/#logos.logos.LOGos","title":"<code>LOGos</code>","text":"<p>LOGos provides a high-level interface for causal analysis of event logs.</p> Source code in <code>src/logos/logos.py</code> <pre><code>class LOGos:\n    \"\"\"\n    LOGos provides a high-level interface for causal analysis of event logs.\n    \"\"\"\n\n    def _set_vars_to_defaults(self) -&gt; None:\n        \"\"\"\n        Set some of the variables to their default values.\n        \"\"\"\n        # The parsed log as a dataframe, and metadata about the parsed variables.\n        self._parsed_log: pd.DataFrame = pd.DataFrame()\n        self._parsed_variables: pd.DataFrame = pd.DataFrame()\n        self._parsed_templates: pd.DataFrame = pd.DataFrame()\n\n        # The variable used to define causal units and the number of causal units.\n        self._causal_unit_var: Optional[str] = None\n        self._num_causal_units: Optional[int] = None\n\n        # The prepared log as a dataframe, and metadata about the prepared variables.\n        self._prepared_log: pd.DataFrame = pd.DataFrame()\n        self._prepared_variables: pd.DataFrame = pd.DataFrame()\n\n        # The available aggregation and imputation functions.\n        agg_module = importlib.import_module(\"src.logos.aggimp.agg_funcs\")\n        self._agg_funcs: dict[str, Callable] = {\n            n: f for n, f in inspect.getmembers(agg_module, inspect.isfunction)\n        }\n\n        imp_module = importlib.import_module(\"src.logos.aggimp.imp_funcs\")\n        self._imp_funcs: dict[str, Callable] = {\n            n: f for n, f in inspect.getmembers(imp_module, inspect.isfunction)\n        }\n\n        # The graph of causal relationships.\n        self._graph: nx.DiGraph = nx.DiGraph()\n\n        # The exploration progress matrix, indicating which edges have been explored.\n        self._edge_states: Optional[EdgeStateMatrix] = None\n\n        # The most recent next exploration suggestion.\n        self._next_exploration: Optional[str] = None\n\n        # An ECCS object for refinement.\n        self._eccs: Optional[ECCS] = None\n\n    @property\n    def parsed_log(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed log as a dataframe.\n        \"\"\"\n        return self._parsed_log\n\n    @property\n    def parsed_variables(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed variables as a dataframe.\n        \"\"\"\n        return self._parsed_variables\n\n    @property\n    def parsed_templates(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed templates as a dataframe.\n        \"\"\"\n        return self._parsed_templates\n\n    @property\n    def prepared_log(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the prepared log as a dataframe.\n        \"\"\"\n        return self._prepared_log\n\n    @property\n    def prepared_variables(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the prepared variables as a dataframe.\n        \"\"\"\n        return self._prepared_variables\n\n    @property\n    def prepared_variable_names(self) -&gt; list[str]:\n        \"\"\"\n        Get the names of the prepared variables.\n        \"\"\"\n        return self._prepared_variables[\"Name\"].values.tolist()\n\n    @property\n    def prepared_variable_tags(self) -&gt; list[str]:\n        \"\"\"\n        Get the tags of the prepared variables.\n        \"\"\"\n        return self._prepared_variables[\"Tag\"].values.tolist()\n\n    def prepared_variable_names_with_base_x_and_no_pre_post_agg(\n        self, x: Union[str, PreparedVariableName]\n    ) -&gt; list[str]:\n        \"\"\"\n        Get all prepared variables with the given base variable and no pre-\n        or post-aggregate values.\n\n        Parameters:\n            x: The base variable to check.\n\n        Returns:\n            A list of variables with the given base variable and no pre-\n            or post-aggregate values.\n        \"\"\"\n        return [\n            var\n            for var in self.prepared_variable_names\n            if PreparedVariableName(var).has_base_var(x)\n            and PreparedVariableName(var).no_pre_post_aggs()\n        ]\n\n    @property\n    def num_prepared_variables(self) -&gt; int:\n        \"\"\"\n        Get the number of prepared variables.\n        \"\"\"\n        return len(self.prepared_variables)\n\n    def __init__(\n        self, filename: str, workdir: str, skip_writeout: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Initialize a LOGos instance, giving it the full path to the log file that will be analyzed.\n\n        Parameters:\n            filename: The full path to the log file that will be analyzed.\n            workdir: The directory where the parsed and prepared dataframes will be stored.\n            skip_writeout: Whether to skip writing out the parsed and prepared dataframes.\n        \"\"\"\n\n        self._set_vars_to_defaults()\n        self._filename = filename\n        print(f\"Initialized LOGos with log file {filename}\")\n\n        # Set and create working directory\n        self._workdir = workdir\n        if not os.path.exists(self._workdir):\n            os.makedirs(self._workdir, exist_ok=True)\n        print(f\"Work directory set to {self._workdir}\")\n\n        self._skip_writeout = skip_writeout\n\n    def _get_filename(self, var_name: str) -&gt; str:\n        \"\"\"\n        Create the file name string for dumping/loading pkl files.\n\n        Parameters:\n            var_name: The name of the variable to be dumped/loaded.\n\n        Returns:\n            The file name string.\n        \"\"\"\n        return os.path.join(\n            self._workdir,\n            os.path.basename(self._filename)\n            + f\"{var_name}_{self._causal_unit_var}_{self._num_causal_units}.pkl\",\n        )\n\n    def _find_type(self, row: pd.Series) -&gt; str:\n        \"\"\"\n        Identify the type of a parsed variable.\n\n        Parameters:\n            row: A row of the parsed variables dataframe.\n\n        Returns:\n            The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".\n        \"\"\"\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"error\", category=UserWarning)\n\n            try:\n                y = pd.to_numeric(row[\"Examples\"], errors=\"raise\")\n                return \"num\"\n            except Exception as e:\n                try:\n                    y = pd.to_timedelta(row[\"Examples\"], errors=\"raise\")\n                    return \"time\"\n                except Exception as e:\n                    try:\n                        y = pd.to_datetime(row[\"Examples\"], errors=\"raise\")\n                        return \"date\"\n                    except Exception as e:\n                        return \"str\"\n\n    def _find_uninteresting(self, row: pd.Series) -&gt; bool:\n        \"\"\"\n        Identify whether a parsed variable is likely to be uninteresting.\n\n        Parameters:\n            row: A row of the parsed variables dataframe.\n\n        Returns:\n            True if the variable is likely to be uninteresting, False otherwise.\n        \"\"\"\n        return (\n            row[\"Type\"] != \"num\"\n            and (self._parsed_log[row[\"Name\"]].nunique() &gt;= 0.15 * row[\"Occurrences\"])\n        ) or (self._parsed_log[row[\"Name\"]].nunique() == 1)\n\n    \"\"\"\n    A default dictionary of regular expressions to be used for parsing the log.\n    \"\"\"\n    DEFAULT_REGEX_DICT = {\n        \"Timestamp\": r\"\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}Z\",\n    }\n\n    def parse(\n        self,\n        regex_dict: dict[str, str] = DEFAULT_REGEX_DICT,\n        sim_thresh: float = 0.65,\n        depth: int = 5,\n        force: bool = False,\n        message_prefix: str = r\".*\",\n        enable_gpt_tagging: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Parse the log file into a dataframe.\n\n        Parameters:\n            regex_dict: (for Drain) A dictionary of regular expressions to be used for parsing.\n            sim_thresh: (for Drain) The similarity threshold to be used for parsing.\n            depth: (for Drain) The parse tree depth to be used for parsing.\n            force: Whether to force re-parsing of the log file.\n            message_prefix: A prefix used to identify the beginning of each log message.\n                Can be used to collapse multiple lines into a single message. Each line that doesn't start with this\n                prefix will be concatenated to the previous log message.\n            enable_gpt_tagging: A boolean indicating whether GPT tagging should be enabled.\n\n        Returns:\n            The time elapsed for parsing, as a string.\n        \"\"\"\n        start_time = datetime.now()\n        parser = Drain(\n            indir=os.path.dirname(self._filename),\n            depth=depth,\n            st=sim_thresh,\n            rex=regex_dict,\n            skip_writeout=self._skip_writeout,\n            message_prefix=message_prefix,\n        )\n\n        # Check if the parsed files already exist.\n        files_exist = not force\n        parsed_df_names = [\n            nameof(self._parsed_log),\n            nameof(self._parsed_templates),\n            nameof(self._parsed_variables),\n        ]\n        for var_name in parsed_df_names:\n            if not os.path.isfile(self._get_filename(var_name)):\n                files_exist = False\n                break\n\n        if files_exist:\n            self._parsed_log = Pickler.load(self._get_filename(parsed_df_names[0]))\n            self._parsed_templates = Pickler.load(\n                self._get_filename(parsed_df_names[1])\n            )\n            self._parsed_variables = Pickler.load(\n                self._get_filename(parsed_df_names[2])\n            )\n        else:\n            (\n                self._parsed_log,\n                self._parsed_templates,\n                self._parsed_variables,\n            ) = parser.parse(self._filename.split(\"/\")[-1])\n            tqdm.pandas(desc=\"Determining variable types...\")\n            self._parsed_variables[\"Type\"] = self._parsed_variables.progress_apply(\n                self._find_type, axis=1\n            )\n\n            # Cast and convert date columns\n            is_date = self._parsed_variables[\"Type\"] == \"date\"\n            date_cols = self._parsed_variables.loc[is_date, \"Name\"]\n            tqdm.pandas(desc=\"Casting date variables...\")\n            self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_apply(\n                pd.to_datetime, errors=\"coerce\"\n            )\n            tqdm.pandas(desc=\"Casting date variables round 2...\")\n            self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_applymap(\n                lambda x: x.timestamp() if not pd.isnull(x) else None\n            )\n            self._parsed_variables.loc[is_date, \"Type\"] = \"num\"\n\n            # Cast and convert time columns\n            is_time = self._parsed_variables[\"Type\"] == \"time\"\n            time_cols = self._parsed_variables.loc[is_time, \"Name\"]\n            tqdm.pandas(desc=\"Casting time variables...\")\n            self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_apply(\n                pd.to_timedelta, errors=\"coerce\"\n            )\n            tqdm.pandas(desc=\"Casting time variables round 2...\")\n            self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_applymap(\n                lambda x: x.total_seconds() if not pd.isnull(x) else None\n            )\n            self._parsed_variables.loc[is_time, \"Type\"] = \"num\"\n\n            # Cast numeric columns\n            is_num = self._parsed_variables[\"Type\"] == \"num\"\n            numeric_cols = self._parsed_variables.loc[is_num, \"Name\"]\n            tqdm.pandas(desc=\"Casting numerical variables...\")\n            self._parsed_log[numeric_cols] = self._parsed_log[\n                numeric_cols\n            ].progress_apply(pd.to_numeric, errors=\"coerce\")\n\n            # Tag variables.\n            tqdm.pandas(desc=\"Tagging variables...\")\n            if enable_gpt_tagging:\n                tag, tag_origin = zip(\n                    *self._parsed_variables.progress_apply(\n                        lambda x: TagUtils.waterfall_tag(self.parsed_templates, x),\n                        axis=1,\n                    )\n                )\n            else:\n                tag, tag_origin = zip(\n                    *self._parsed_variables.progress_apply(\n                        lambda x: TagUtils.preceding_tokens_tag(x),\n                        axis=1,\n                    )\n                )\n            self._parsed_variables[\"Tag\"] = tag\n            self._parsed_variables[\"TagOrigin\"] = tag_origin\n            TagUtils.deduplicate_tags(self._parsed_variables)\n\n            # Detect identifiers.\n            tqdm.pandas(desc=\"Detecting identifiers...\")\n            self._parsed_variables[\"IsUninteresting\"] = (\n                self._parsed_variables.progress_apply(self._find_uninteresting, axis=1)\n            )\n\n            # Reorder columns.\n            self._parsed_variables = self._parsed_variables[\n                [\n                    \"Name\",\n                    \"Tag\",\n                    \"TagOrigin\",\n                    \"Type\",\n                    \"IsUninteresting\",\n                    \"Occurrences\",\n                    \"Preceding 3 tokens\",\n                    \"Examples\",\n                    \"From regex\",\n                ]\n            ]\n\n        # Write out files if appropriate.\n        if not self._skip_writeout and not files_exist:\n            Pickler.dump(self._parsed_log, self._get_filename(parsed_df_names[0]))\n            Pickler.dump(self._parsed_templates, self._get_filename(parsed_df_names[1]))\n            Pickler.dump(self._parsed_variables, self._get_filename(parsed_df_names[2]))\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(f\"Parsing complete in {elapsed} seconds!\")\n        return elapsed\n\n    def include_in_template(\n        self,\n        var: str,\n        enable_gpt_tagging: bool = False,\n        skip_writeout: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"\n        Treat a certain parsed variable as part of its template and regenerate parsed dataframes.\n\n        Parameters:\n            var: The name or tag of the variable to be included in its template.\n            enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n            skip_writeout: Whether to skip writing out the regenerated parsed dataframes. Defaults\n                to the value of self._skip_writeout.\n        \"\"\"\n        name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n\n        old_template_id = ParsedVariableName(name).template_id()\n        idx = ParsedVariableName(name).index()\n        value_counts = self._parsed_log[name].value_counts().to_dict()\n\n        ### Modify _parsed_templates\n        old_template_row = (\n            self._parsed_templates.loc[\n                self._parsed_templates[\"TemplateId\"] == old_template_id\n            ]\n            .iloc[0]\n            .copy()\n        )\n        toks = old_template_row[\"TemplateText\"].split(\" \")\n        new_template_ids = {}\n        new_variable_indices = old_template_row[\"VariableIndices\"]\n        new_variable_indices.remove(idx)\n\n        for value, occurences in value_counts.items():\n            new_template_row = old_template_row.copy()\n            toks[idx] = value\n\n            new_template_row[\"TemplateText\"] = \" \".join(toks)\n            new_template_row[\"TemplateId\"] = hashlib.md5(\n                new_template_row[\"TemplateText\"].encode(\"utf-8\")\n            ).hexdigest()[0:8]\n            new_template_row[\"Occurrences\"] = occurences\n            new_template_row[\"VariableIndices\"] = new_variable_indices\n            new_template_row[\"RegexIndices\"] = old_template_row[\"RegexIndices\"]\n\n            self._parsed_templates.loc[len(self._parsed_templates)] = new_template_row\n            new_template_ids[value] = new_template_row[\"TemplateId\"]\n\n        self._parsed_templates = self._parsed_templates[\n            self._parsed_templates[\"TemplateId\"] != old_template_id\n        ].reset_index(drop=True)\n\n        ### Modify _parsed_log\n\n        # Update the template ids of all rows that belonged to the old template\n        self._parsed_log[\"TemplateId\"] = self._parsed_log.apply(\n            lambda x: (\n                new_template_ids[x[name]]\n                if (x[\"TemplateId\"] == old_template_id)\n                else x[\"TemplateId\"]\n            ),\n            axis=1,\n        )\n\n        # Create new variables for each new template id and assign the value of the old variables to them\n        new_variables = []\n        for new_template_id in new_template_ids.values():\n            for other_idx in new_variable_indices:\n                new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n                new_variables.append(new_var_name)\n                self._parsed_log[new_var_name] = self._parsed_log.apply(\n                    lambda x: (\n                        x[f\"{old_template_id}_{other_idx}\"]\n                        if (x[\"TemplateId\"] == new_template_id)\n                        else None\n                    ),\n                    axis=1,\n                )\n\n        # Drop variable columns associated with old template id\n        variables_to_drop = [\n            v for v in self._parsed_log.columns if v.startswith(old_template_id)\n        ]\n        self._parsed_log.drop(columns=variables_to_drop, inplace=True)\n\n        ### Modify _parsed_variables\n\n        # Add variable rows for each new variable\n        for value, occurrences in value_counts.items():\n            for other_idx in new_variable_indices:\n                new_template_id = new_template_ids[value]\n                new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n\n                x = {}\n                x[\"Name\"] = new_var_name\n                x[\"Occurrences\"] = occurrences\n                x[\"Preceding 3 tokens\"] = (\n                    self._parsed_templates[\n                        self._parsed_templates[\"TemplateId\"] == new_template_id\n                    ][\"TemplateText\"]\n                    .values[0]\n                    .split()[max(0, other_idx - 3) : other_idx]\n                )\n                x[\"Examples\"] = (\n                    self._parsed_log[new_var_name]\n                    .loc[self._parsed_log[new_var_name].notna()]\n                    .unique()[:5]\n                    .tolist()\n                )\n                x[\"From regex\"] = False\n                if enable_gpt_tagging:\n                    x[\"Tag\"], x[\"TagOrigin\"] = TagUtils.waterfall_tag(\n                        self.parsed_templates, pd.Series(x)\n                    )\n                else:\n                    x[\"Tag\"], x[\"TagOrigin\"] = TagUtils.preceding_tokens_tag(\n                        pd.Series(x)\n                    )\n                x[\"Type\"] = self._find_type(pd.Series(x))\n                x[\"IsUninteresting\"] = self._find_uninteresting(pd.Series(x))\n\n                self._parsed_variables.loc[len(self._parsed_variables)] = x\n\n        # Drop variable rows associated with old template id\n        self._parsed_variables = self._parsed_variables[\n            ~self._parsed_variables[\"Name\"].isin(variables_to_drop)\n        ].reset_index(drop=True)\n\n        # Deduplicate tags again\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n        # Write out files if appropriate.\n        if skip_writeout is None:\n            skip_writeout = self._skip_writeout\n        if not skip_writeout:\n            print(\"about to write stuff out\")\n            Pickler.dump(self._parsed_log, self._get_filename(nameof(self._parsed_log)))\n            Pickler.dump(\n                self._parsed_templates,\n                self._get_filename(nameof(self._parsed_templates)),\n            )\n            Pickler.dump(\n                self._parsed_variables,\n                self._get_filename(nameof(self._parsed_variables)),\n            )\n\n    def tag_parsed_variable(self, name: str, tag: str) -&gt; None:\n        \"\"\"\n        Tag a parsed variable.\n\n        Parameters:\n            name: The name of the variable to be tagged.\n            tag: The tag to be assigned to the variable.\n        \"\"\"\n        TagUtils.set_tag(self._parsed_variables, name, tag, \"parsed\")\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n    def get_tag_of_parsed(self, name: str) -&gt; str:\n        \"\"\"\n        Get the tag of a parsed variable.\n\n        Parameters:\n            name: The name of the variable.\n\n        Returns:\n            The tag of the variable.\n        \"\"\"\n        return TagUtils.get_tag(self._parsed_variables, name, \"parsed\")\n\n    def tag_prepared_variable(self, name: str, tag: str) -&gt; None:\n        \"\"\"\n        Tag a prepared variable.\n\n        Parameters:\n            name: The name of the variable to be tagged.\n            tag: The tag to be assigned to the variable.\n        \"\"\"\n        TagUtils.set_tag(self._prepared_variables, name, tag, \"prepared\")\n        TagUtils.deduplicate_tags(self._prepared_variables)\n\n    def get_tag_of_prepared(self, name: str) -&gt; str:\n        \"\"\"\n        Get the tag of a prepared variable.\n\n        Parameters:\n            name: The name of the variable.\n\n        Returns:\n            The tag of the variable.\n        \"\"\"\n        return TagUtils.get_tag(self._prepared_variables, name, \"prepared\")\n\n    def get_causal_unit_info(self) -&gt; Tuple[str, int]:\n        \"\"\"\n        Get the variable used to define causal units and the number of\n        causal units.\n\n        Returns:\n            The name of the variable used to define causal units\n            and the number of causal units.\n        \"\"\"\n        return self._causal_unit_var, self._num_causal_units\n\n    def suggest_causal_unit_defs(\n        self,\n        min_causal_units: int = 4,\n        num_suggestions: int = 10,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Suggest at most `num_suggestions` causal unit definitions based on IUS maximization,\n        while returning at least `min_causal_units` causal units.\n\n        Parameters:\n            min_causal_units: The minimum number of causal units that a suggested\n                definition should create.\n            num_suggestions: The maximum number of causal unit definitions to suggest.\n\n        Returns:\n            A DataFrame with one row for each suggested causal unit definition, or `None`\n                if no suggestions were made.\n        \"\"\"\n\n        return CausalUnitSuggester.suggest_causal_unit_defs(\n            self._parsed_log[self._parsed_variables[\"Name\"].values],\n            self._parsed_variables,\n            min_causal_units=min_causal_units,\n            num_suggestions=num_suggestions,\n        )\n\n    def set_causal_unit(\n        self,\n        var: str,\n        num_units: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set the variable used to define causal units and optionally the number of\n        causal units. The latter will be ignored if the variable is categorical, but it\n        must be specified if the variable is numerical.\n\n        Parameters:\n            var: The name or tag of the variable to be used as the causal unit.\n            num_units: The number of causal units to be created.\n\n        Raises:\n            ValueError: If the variable is numerical and `num_units` is not specified.\n        \"\"\"\n        var_name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n        var_type = self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == var_name, \"Type\"\n        ].values[0]\n\n        if var_type == \"num\" and num_units is None:\n            raise ValueError(\n                \"The number of causal units must be specified if the causal unit is numerical.\"\n            )\n\n        self._causal_unit_var = var_name\n        self._num_causal_units = num_units\n\n        print(\n            f\"Causal unit set to {var_name} (tag: {self.get_tag_of_parsed(var_name)}) \"\n            + (\n                \"\"\n                if not self._num_causal_units\n                else f\" with {self._num_causal_units} causal units.\"\n            )\n        )\n\n    def prepare(\n        self,\n        custom_agg: dict[str, list[str]] = {},\n        custom_imp: dict[str, list[str]] = {},\n        count_occurences: bool = False,\n        ignore_uninteresting: bool = True,\n        force: bool = False,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n        drop_bad_aggs: bool = True,\n        reject_prunable_edges: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.\n\n        Parameters:\n            custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n            custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n            count_occurences: Whether to include extra variables counting the occurence of each template.\n            ignore_uninteresting: Whether to ignore uninteresting variables.\n            force: Whether to force re-preparation of the log.\n            lasso_alpha: The alpha parameter to be used for LASSO regression.\n            lasso_max_iter: The maximum number of iterations to be used for LASSO regression.\n            drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n                variables based on the same base variable but using a different aggregation function.\n            reject_prunable_edges: Whether to reject edges that are prunable based on LASSO applciation.\n\n        Returns:\n            The time elapsed for preparation, as a string.\n        \"\"\"\n\n        start_time = datetime.now()\n        # Ensure causal unit is set. TODO: make IUS maximizer the default\n        if self._causal_unit_var is None:\n            print(\"Causal unit not defined. Aborting.\")\n            return None\n\n        # Check if the prepared files already exist.\n        files_exist = not force\n        prepared_df_names = [\n            nameof(self._prepared_log),\n            nameof(self._prepared_variables),\n        ]\n        for var_name in prepared_df_names:\n            if not os.path.isfile(self._get_filename(var_name)):\n                files_exist = False\n                break\n\n        if files_exist:\n            self._prepared_log = Pickler.load(self._get_filename(prepared_df_names[0]))\n            self._prepared_variables = Pickler.load(\n                self._get_filename(prepared_df_names[1])\n            )\n        else:\n            self._prepare_anew(\n                custom_agg,\n                custom_imp,\n                count_occurences=count_occurences,\n                ignore_uninteresting=ignore_uninteresting,\n                drop_bad_aggs=drop_bad_aggs,\n            )\n\n        self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n        if reject_prunable_edges:\n            print(f\"Pruning edges...\")\n            self.reject_all_prunable_edges(\n                lasso_alpha=lasso_alpha, lasso_max_iter=lasso_max_iter\n            )\n\n        self._eccs = ECCS(self._prepared_log, nx.DiGraph())\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(\n            f\"\"\"Preparation complete in {elapsed} seconds! \"\"\"\n            f\"\"\"{np.count_nonzero(self._edge_states.m == -1)} of the {self.num_prepared_variables ** 2} possible edges were auto-rejected.\"\"\"\n        )\n\n        return elapsed\n\n    def _prepare_anew(\n        self,\n        custom_agg: dict[str, list[str]] = {},\n        custom_imp: dict[str, list[str]] = {},\n        count_occurences: bool = False,\n        ignore_uninteresting: bool = True,\n        drop_bad_aggs: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Prepare the log anew.\n\n        Parameters:\n            custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n            custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n            count_occurences: Whether to include extra variables counting the occurence of each template.\n            ignore_uninteresting: Whether to ignore uninteresting variables.\n            drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n                variables based on the same base variable but using a different aggregation function.\n        \"\"\"\n\n        print(f\"Determining the causal unit assignment...\")\n        causal_unit_assignment = CausalUnitSuggester._discretize(\n            self._parsed_log[self._causal_unit_var],\n            self._parsed_variables[\n                self._parsed_variables[\"Name\"] == self._causal_unit_var\n            ][\"Type\"].values[0],\n            self._num_causal_units,\n        )\n\n        # Convert keys in custom_agg and custom_imp to the names of the variables, if they are tags.\n        custom_agg = {\n            TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n            for k, v in custom_agg.items()\n        }\n        custom_imp = {\n            TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n            for k, v in custom_imp.items()\n        }\n\n        # Start with the parsed log, optionally with extra variables counting the occurence of each template.\n        if count_occurences:\n            print(f\"Adding template occurrence count variables...\")\n            self._prepared_log = pd.concat(\n                [\n                    self._parsed_log,\n                    pd.get_dummies(\n                        self._parsed_log[\"TemplateId\"],\n                        prefix=\"TemplateId\",\n                        prefix_sep=\"=\",\n                    ),\n                ],\n                axis=1,\n            )\n        else:\n            self._prepared_log = self._parsed_log.copy(deep=True)\n\n        # No longer need the column storing the actual template IDs\n        self._prepared_log.drop(columns=\"TemplateId\", inplace=True)\n\n        # Build dictionary of aggregation functions\n        agg_dict: dict[str, str] = {\n            variable.Name: (\n                custom_agg[variable.Name]\n                if variable.Name in custom_agg\n                else AggregateSelector.DEFAULT_AGGREGATES[variable.Type]\n            )\n            for variable in self._parsed_variables.itertuples()\n        }\n\n        # Add aggregations for template counts\n        for col in self._prepared_log.columns:\n            if PreparedVariableName(col).base_var() == \"TemplateId\":\n                agg_dict[col] = [\"sum\"]\n\n        # Drop uninteresting columns if requested, except if they are the causal unit.\n        ui_cols = self._parsed_variables.loc[\n            self._parsed_variables[\"IsUninteresting\"], \"Name\"\n        ].values\n        ui_cols = [x for x in ui_cols if x != self._causal_unit_var]\n        if ignore_uninteresting:\n            self._prepared_log.drop(\n                columns=ui_cols,\n                inplace=True,\n            )\n            for col in ui_cols:\n                agg_dict.pop(col, None)\n            print(\n                f\"Dropped {len(ui_cols)} uninteresting columns, out of an original total of {len(self.parsed_variables)}.\"\n            )\n\n        # Ensure the causal unit variable only has one aggregation function\n        agg_dict[self._causal_unit_var] = agg_dict[self._causal_unit_var][:1]\n\n        # Perform the aggregation\n        print(\"Calculating aggregates for each causal unit...\")\n        agg_func_dict: dict[str, list[Callable]] = {\n            name: [self._agg_funcs[f] for f in funcs]\n            for name, funcs in agg_dict.items()\n        }\n        self._prepared_log = self._prepared_log.groupby(\n            causal_unit_assignment\n        ).aggregate(agg_func_dict)\n        self._prepared_log.columns = [\n            \"+\".join(col) for col in self._prepared_log.columns.values\n        ]\n        self._parsed_variables[\"Aggregates\"] = self._parsed_variables[\"Name\"].map(\n            lambda x: agg_dict.get(x, [])\n        )\n        self._prepared_log.set_index(\n            f\"{self._causal_unit_var}+{self._parsed_variables[self._parsed_variables['Name'] == self._causal_unit_var]['Aggregates'].values[0][0]}\",\n            inplace=True,\n        )\n        self._prepared_log.sort_index(inplace=True)\n        self._prepared_log.index = self._prepared_log.index.astype(str)\n\n        # Perform the imputation\n        for col in tqdm(self._prepared_log.columns, desc=\"Imputing missing values...\"):\n            if self._prepared_log[col].isnull().values.any():\n                base_var = PreparedVariableName(col).base_var()\n                func_name: str = (\n                    custom_imp[base_var] if base_var in custom_imp else \"no_imp\"\n                )\n                self._prepared_log[col] = (self._imp_funcs[func_name])(\n                    self._prepared_log[col]\n                )\n        self._prepared_log.dropna(inplace=True)\n\n        # Drop variables that do not add information compared to other variables based on the same base variable\n        # but using a different aggregation function.\n        if drop_bad_aggs:\n            print(f\"Dropping aggregates that do not add information...\")\n            cols_to_drop = AggregateSelector.find_uninformative_aggregates(\n                self._prepared_log, self._parsed_variables, self._causal_unit_var\n            )\n            self._prepared_log.drop(columns=cols_to_drop, inplace=True)\n\n        # Identify the categorical variables and one-hot encode them\n        categorical_vars = self._prepared_log.select_dtypes(\n            include=\"object\"\n        ).columns.tolist()\n        for col in tqdm(\n            categorical_vars, desc=\"One-hot encoding categorical variables...\"\n        ):\n            self._prepared_log = pd.concat(\n                [\n                    self._prepared_log,\n                    pd.get_dummies(\n                        self._prepared_log[col], prefix=col, prefix_sep=\"=\", dtype=int\n                    ),\n                ],\n                axis=1,\n            )\n            self._prepared_log.drop(col, axis=1, inplace=True)\n        # Deal with https://github.com/pydot/pydot/issues/258\n        self._prepared_log.columns = [\n            x.replace(\":\", \";\") for x in self._prepared_log.columns\n        ]\n\n        # Generate dataframe of prepared variables for later tagging etc.\n        self._generate_prepared_variables_df()\n\n        # Convert any date columns to Unix timestamps in milliseconds\n        date_cols = self._prepared_variables.loc[\n            self._prepared_variables[\"Type\"] == \"date\", \"Name\"\n        ].values\n        self._prepared_log[date_cols] = self._prepared_log[date_cols].map(\n            lambda x: x.timestamp() * 1000.0\n        )\n\n        # Convert any time columns to milliseconds\n        time_cols = self._prepared_variables.loc[\n            self._prepared_variables[\"Type\"] == \"time\", \"Name\"\n        ].values\n        self._prepared_log[time_cols] = self._prepared_log[time_cols].map(\n            lambda x: x.total_seconds() * 1000.0\n        )\n\n        # Write out prepared log and variables\n        if not self._skip_writeout:\n            Pickler.dump(\n                self._prepared_log, self._get_filename(nameof(self._prepared_log))\n            )\n            Pickler.dump(\n                self._prepared_variables,\n                self._get_filename(nameof(self._prepared_variables)),\n            )\n\n        print(\n            f\"\"\"Successfully prepared the log with causal unit {self._causal_unit_var} \"\"\"\n            f\"\"\"(tag: {self.get_tag_of_parsed(self._causal_unit_var)})\"\"\"\n            + (\n                \"\"\n                if not self._num_causal_units\n                else f\" with {self._num_causal_units} causal units.\"\n            )\n        )\n\n        return\n\n    def _generate_prepared_variables_df(self) -&gt; None:\n        \"\"\"\n        Generate dataframe of prepared variables for later tagging etc.\n        \"\"\"\n\n        self._prepared_variables = pd.DataFrame()\n        self._prepared_variables[\"Name\"] = self._prepared_log.columns\n\n        # Bring in varable name components leveraging PreparedVariableName\n        self._prepared_variables[\"Base\"] = self._prepared_variables[\"Name\"].apply(\n            lambda x: PreparedVariableName(x).base_var()\n        )\n        self._prepared_variables[\"Pre-agg Value\"] = self._prepared_variables[\n            \"Name\"\n        ].apply(lambda x: PreparedVariableName(x).pre_agg_value())\n        self._prepared_variables[\"Agg\"] = self._prepared_variables[\"Name\"].apply(\n            lambda x: PreparedVariableName(x).aggregate()\n        )\n        self._prepared_variables[\"Post-agg Value\"] = self._prepared_variables[\n            \"Name\"\n        ].apply(lambda x: PreparedVariableName(x).post_agg_value())\n\n        # Bring in other info from self._parsed_variables\n        self._prepared_variables[\"Tag\"] = self._prepared_variables.apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x[\"Base\"],\n                    \"Tag\",\n                ].values[0]\n                if x[\"Base\"] != \"TemplateId\"\n                else \"TemplateId\"\n            )\n            + (f\" {x['Pre-agg Value']}\" if x[\"Pre-agg Value\"] != \"\" else \"\")\n            + (f\" {x['Agg']}\" if x[\"Agg\"] != \"\" else \"\")\n            + (f\" {x['Post-agg Value']}\" if x[\"Post-agg Value\"] != \"\" else \"\"),\n            axis=1,\n        )\n        self._prepared_variables[\"Base Variable Occurences\"] = self._prepared_variables[\n            \"Base\"\n        ].apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x, \"Occurrences\"\n                ].values[0]\n                if x != \"TemplateId\"\n                else \"\"\n            )\n        )\n        self._prepared_variables[\"Type\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x, \"Type\"\n                ].values[0]\n                if x != \"TemplateId\"\n                else \"\"\n            )\n        )\n        self._prepared_variables[\"Examples\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x, \"Examples\"\n                ].values[0]\n                if x != \"TemplateId\"\n                else \"\"\n            )\n        )\n        self._prepared_variables[\"From regex\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x, \"From regex\"\n                ].values[0]\n                if x != \"TemplateId\"\n                else \"\"\n            )\n        )\n\n        # Bring in template text, only for appropriate base variables.\n        self._prepared_variables[\"TemplateText\"] = self._prepared_variables.apply(\n            lambda x: (\n                self._parsed_templates.loc[\n                    self._parsed_templates[\"TemplateId\"]\n                    == PreparedVariableName(x[\"Name\"]).template_id(),\n                    \"TemplateText\",\n                ].values[0]\n                if x[\"From regex\"] == False\n                else \"\"\n            ),\n            axis=1,\n        )\n\n    def inspect(\n        self,\n        var: str,\n        ref_var: Optional[str] = None,\n        row_limit: Optional[int] = 10,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Print information about a specific prepared variable.\n\n        Parameters:\n            var: The name or tag of the variable.\n            ref_var: The name or tag of a reference variable.\n            row_limit: The number of rows of the prepared log to print out,\n                to illustrate example values of this variable.\n\n        Returns:\n            A tuple containing:\n                (1) Information about the base variable of `var`, if `var` is not related to the\n                    occurrence count of a template.\n                (2) Information about the template of `var`, if `var` was not created from a regex.\n                (3) A sample of the prepared log, with `row_limit` rows.\n        \"\"\"\n\n        # Retrieve the name of this variable, if a tag was passed in.\n        name = TagUtils.name_of(self._prepared_variables, var, \"prepared\")\n\n        print(f\"Information about prepared variable {name}:\\n\")\n        base_var = PreparedVariableName(name).base_var()\n        from_regex = False\n\n        base_var_info_df = pd.DataFrame()\n        if base_var != \"TemplateId\":\n            print(f\"--&gt; Variable Information about {base_var}:\")\n            base_var_info_df = self._parsed_variables[\n                self._parsed_variables[\"Name\"] == base_var\n            ]\n            from_regex = base_var_info_df[\"From regex\"].values[0]\n            display(base_var_info_df)\n\n        template_info_df = pd.DataFrame()\n        if not from_regex:\n            template_id = PreparedVariableName(name).template_id()\n            print(f\"--&gt; Template Information about {template_id}:\")\n            template_info_df = self._parsed_templates[\n                self._parsed_templates[\"TemplateId\"] == template_id\n            ]\n            display(template_info_df)\n\n        print(\"--&gt; Causal Unit Partial Information:\")\n        if row_limit == None:\n            row_limit = len(self._prepared_log)\n        col_list = [name]\n        col_list.extend([ref_var] if ref_var is not None else [])\n        prepared_log_info_df = self._prepared_log[col_list].head(row_limit)\n        col_names = [f\"{name} (candidate)\"]\n        col_names.extend([f\"{ref_var} (outcome)\"] if ref_var is not None else [])\n        prepared_log_info_df.columns = col_names\n        display(prepared_log_info_df)\n\n        return base_var_info_df, template_info_df, prepared_log_info_df\n\n    def clear_graph(self, clear_edge_states: bool = True) -&gt; None:\n        \"\"\"\n        Clear the graph and possibly edge states.\n\n        Parameters:\n            clear_edge_states: Whether to also clear the edge states.\n        \"\"\"\n        self._graph = nx.DiGraph()\n        if clear_edge_states:\n            self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n        if self._eccs:\n            self._eccs.clear_graph(clear_edge_states)\n\n    def display_graph(self) -&gt; None:\n        \"\"\"\n        Display the current graph.\n        \"\"\"\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    def save_graph(self, filename: str) -&gt; None:\n        \"\"\"\n        Save the current graph to a file.\n\n        Parameters:\n            filename: The name of the file to save to.\n        \"\"\"\n        GraphRenderer.save_graph(self._graph, self._prepared_variables, filename)\n\n    def accept(\n        self,\n        src: str,\n        dst: str,\n        also_fix: bool,\n        interactive: bool = True,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark a causal graph edge as accepted.\n\n        This will also reject the edge from `dst` to `src` and remove any other variables with the\n        same base variable as either `src` or `dst` from consideration for the partial causal graph.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            also_fix: Whether to also fix the edge, for ECCS.\n            interactive: Whether to display the graph interactively after accepting the edge.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge addition,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n\n        src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        to_drop = self._edge_states.mark_edge(src_name, dst_name, \"Accepted\")\n        for node in to_drop:\n            if node in self._graph.nodes:\n                self._graph.remove_node(node)\n\n        self._graph.add_node(src_name)\n        self._graph.add_node(dst_name)\n        self._graph.add_edge(src_name, dst_name)\n        if (dst_name, src_name) in self._graph.edges:\n            self._graph.remove_edge(dst_name, src_name)\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n        if self._eccs:\n            self._eccs.remove_edge(dst_name, src_name)\n            self._eccs.add_edge(src_name, dst_name)\n            if also_fix:\n                self._eccs.fix_edge(src_name, dst_name)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            (\n                GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n                if not interactive\n                else \"\"\n            ),\n        )\n\n    def reject(\n        self,\n        src: str,\n        dst: str,\n        also_ban: bool,\n        interactive: bool = True,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark a causal graph edge as rejected.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            also_ban: Whether to also ban the edge, for ECCS.\n            interactive: Whether to display the graph interactively after rejecting the edge.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejection,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n\n        src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        self._edge_states.mark_edge(src_name, dst_name, \"Rejected\")\n        if self._eccs and also_ban:\n            self._eccs.ban_edge(src_name, dst_name)\n\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            (\n                GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n                if not interactive\n                else \"\"\n            ),\n        )\n\n    def reject_undecided_incoming(\n        self, dst: str, also_ban: bool, interactive: bool = True\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark all undecided incoming edges to a variable as rejected.\n\n        Parameters:\n            dst: The name or tag of the destination variable.\n            also_ban: Whether to also ban the edges, for ECCS.\n            interactive: Whether to display the graph interactively after rejecting the edges.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejections,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        for v in self.prepared_variable_names:\n            if self._edge_states.get_edge_state(v, dst_name) == \"Undecided\":\n                self._edge_states.mark_edge(v, dst_name, \"Rejected\")\n                if self._eccs and also_ban:\n                    self._eccs.ban_edge(v, dst_name)\n\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            (\n                GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n                if not interactive\n                else \"\"\n            ),\n        )\n\n    def reject_undecided_outgoing(\n        self, src: str, also_ban: bool, interactive: bool = True\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark all undecided outgoing edges from a variable as rejected.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            also_ban: Whether to also ban the edges, for ECCS.\n            interactive: Whether to display the graph interactively after rejecting the edges.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejections,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n        src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n        for v in self.prepared_variable_names:\n            if self._edge_states.get_edge_state(src_name, v) == \"Undecided\":\n                self._edge_states.mark_edge(src_name, v, \"Rejected\")\n                if self._eccs and also_ban:\n                    self._eccs.ban_edge(src_name, v)\n\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            (\n                GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n                if not interactive\n                else \"\"\n            ),\n        )\n\n    def reject_all_prunable_edges(\n        self,\n        also_ban: bool,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        For every prepared variable, reject all incoming edges that start at a variable\n        that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.\n\n        Parameters:\n            also_ban: Whether to also ban the edges, for ECCS.\n            lasso_alpha: The alpha parameter to be used for Lasso regression.\n            lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejections,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n        num_processors = multiprocessing.cpu_count()\n        with multiprocessing.Pool(processes=num_processors) as pool:\n            all_candidates = pool.starmap(\n                Pruner.prune_with_lasso,\n                tqdm(\n                    [\n                        (self._prepared_log, [target], lasso_alpha, lasso_max_iter)\n                        for target in self.prepared_variable_names\n                    ],\n                    total=self.num_prepared_variables,\n                    desc=\"Finding pruned variables...\",\n                ),\n            )\n\n        Printer.printv(all_candidates)\n\n        for candidates, target in zip(all_candidates, self.prepared_variable_names):\n            non_candidates = (\n                set(self._prepared_log.columns) - set(candidates) - set([target])\n            )\n            for nc in non_candidates:\n                self._edge_states.mark_edge(nc, target, \"Rejected\")\n                if self._eccs and also_ban:\n                    self._eccs.ban_edge(nc, target)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables),\n        )\n\n    @property\n    def exploration_score(self) -&gt; float:\n        \"\"\"\n        Calculate the exploration score of the current partial causal graph,\n        based on the edge state matrix.\n\n        Returns:\n            The exploration score of the current partial causal graph.\n        \"\"\"\n        # Number of edges incident to a node in the current partial graph\n        M = self._graph.number_of_nodes()\n        N = self.num_prepared_variables\n        incident = M * (2 * N - M - 1)\n        if incident == 0:\n            return 0\n\n        # Number of edges among the incident that have been considered\n        graph_var_indices = [self._edge_states.idx(x) for x in list(self._graph.nodes)]\n        other_indices = list(np.setdiff1d(np.arange(N), graph_var_indices))\n        considered = np.sum(\n            self._edge_states.m[graph_var_indices][:, graph_var_indices] != 0\n        )\n        considered -= M  # subtract self-edges\n        considered += np.sum(\n            self._edge_states.m[graph_var_indices][:, other_indices] != 0\n        )\n        considered += np.sum(\n            self._edge_states.m[other_indices][:, graph_var_indices] != 0\n        )\n\n        Printer.printv(f\"Considered: {considered}\")\n        Printer.printv(f\"Incident: {incident}\")\n\n        return considered / incident\n\n    def rank_candidate_causes(\n        self,\n        target: Optional[str] = None,\n        ignore: Optional[List[str]] = None,\n        method: CandidateCauseRankerMethod = CandidateCauseRankerMethod.LOGOS,\n        prune_candidates: bool = True,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n        model: str = \"gpt-4o-mini-2024-07-18\",\n        gpt_log_path: Optional[str] = None,\n    ) -&gt; Tuple[pd.DataFrame, str]:\n        \"\"\"\n        Present the user with ranked candidate causes for `target`. If no `target`\n        is specified, the most recent suggestion of `suggest_next_exploration()` is used, if any.\n        If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n        Parameters:\n            target: The name or tag of the target variable.\n            ignore: A list of variables to ignore.\n            method: The method to use for ranking candidate causes.\n            prune_candidates: Whether to prune the candidate causes using Lasso regression. Only\n                applies if `method` is `CandidateCauseRankerMethod.LOGOS`.\n            lasso_alpha: The alpha parameter to be used for Lasso regression. Only applies if\n                `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n            lasso_max_iter: The maximum number of iterations to be used for Lasso regression. Only\n                applies if `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n            model: The model to use for the langmodel method. Only applies if the method is\n                `CandidateCauseRankerMethod.LANGMODEL`.\n            gpt_log_path: The path to the log file to use for the langmodel method. Only applies if\n                the method is `CandidateCauseRankerMethod.LANGMODEL`.\n        Returns:\n            A tuple containing:\n            (1) A dataframe containing the candidate causes for `target` and\n            (2) The time elapsed for exploration, as a string.\n        \"\"\"\n\n        start_time = datetime.now()\n\n        # Handle the case where the user has not specified a target.\n        if target is None and self._next_exploration is None:\n            print(\"No target specified.\")\n            return pd.DataFrame(columns=CandidateCauseRanker.COLUMN_ORDER), \"\"\n        elif target is None:\n            target = self._next_exploration\n\n        # If the user provided the target as a tag, retrieve its name\n        target = TagUtils.name_of(self._prepared_variables, target, \"prepared\")\n\n        # Use the specified method to rank candidate causes\n        result_df, pruned = CandidateCauseRanker.rank(\n            self.prepared_log,\n            self.prepared_variables,\n            target,\n            ignore,\n            method,\n            prune_candidates,\n            lasso_alpha,\n            lasso_max_iter,\n            model,\n            (\n                gpt_log_path\n                if (gpt_log_path is not None)\n                else os.path.join(\n                    self._workdir,\n                    f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\",\n                )\n            ),\n        )\n\n        # Mark the edges rejected by the pruning step, if any.\n        for var in pruned:\n            self._edge_states.mark_edge(var, target, \"Rejected\")\n\n        # Add fields to the returned dataframe\n        result_df[\"Candidate-&gt;Target Edge Status\"] = result_df[\"Candidate\"].apply(\n            lambda x: self._edge_states.get_edge_state(x, target)\n        )\n        result_df[\"Target-&gt;Candidate Edge Status\"] = result_df[\"Candidate\"].apply(\n            lambda x: self._edge_states.get_edge_state(target, x)\n        )\n\n        ret_val = result_df[CandidateCauseRanker.COLUMN_ORDER]\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(f\"Candidate cause exploration complete in {elapsed} seconds!\")\n\n        return ret_val, elapsed\n\n    def get_causal_graph_refinement_suggestion(\n        self,\n        method: InteractiveCausalGraphRefinerMethod = InteractiveCausalGraphRefinerMethod.LOGOS,\n        treatment: Optional[str] = None,\n        outcome: Optional[str] = None,\n        model: str = \"gpt-4o-mini-2024-07-18\",\n        gpt_log_path: Optional[str] = None,\n    ) -&gt; Tuple[Edge, str]:\n        \"\"\"\n        Present the user with an edge, the presence and direction of which they should assess.\n\n        Parameters:\n            method: The method to use for producing a causal graph refinement suggestion.\n            treatment: The name or tag of the treatment variable. Only applies if the method is\n                `InteractiveCausalGraphRefinerMethod.LOGOS`.\n            outcome: The name or tag of the outcome variable. Only applies if the method is\n                `InteractiveCausalGraphRefinerMethod.LOGOS`.\n            model: The model to use for the langmodel method. Only applies if the method is\n                `CandidateCauseRankerMethod.LANGMODEL`.\n            gpt_log_path: The path to the log file to use for the langmodel method. Only applies if\n                the method is `CandidateCauseRankerMethod.LANGMODEL`.\n        Returns:\n            A tuple containing:\n            (1) The edge to assess, as an Edge object, and\n            (2) The time elapsed for generating the suggestion, as a string.\n        \"\"\"\n\n        start_time = datetime.now()\n\n        treatment_name = TagUtils.name_of(\n            self._prepared_variables, treatment, \"prepared\"\n        )\n        outcome_name = TagUtils.name_of(self._prepared_variables, outcome, \"prepared\")\n\n        edge = InteractiveCausalGraphRefiner.get_suggestion(\n            self.prepared_log,\n            method,\n            self._eccs,\n            treatment_name,\n            outcome_name,\n            self._graph,\n            model,\n            (\n                gpt_log_path\n                if (gpt_log_path is not None)\n                else os.path.join(\n                    self._workdir,\n                    f\"refiner-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\",\n                )\n            ),\n            self.prepared_variables\n        )\n\n        edge_tags = None\n        if edge:\n            edge_tags = tuple(\n                TagUtils.tag_of(self._prepared_variables, x, \"prepared\") for x in edge\n            )\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(f\"Candidate cause exploration complete in {elapsed} seconds!\")\n\n        return edge_tags, elapsed\n\n    def suggest_next_exploration(self) -&gt; Optional[str]:\n        \"\"\"\n        Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph\n        that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable\n        with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.\n\n        If all edges are decided, return None.\n\n        Returns:\n            The name of the variable to explore next.\n        \"\"\"\n\n        # Try to find a suggestion from the partial causal graph.\n        node_names = list(self._graph.nodes)\n        graph_var_indices = [self._edge_states.idx(x) for x in node_names]\n        graph_var_incoming_edge_states = self._edge_states.m[:, graph_var_indices]\n        undecided_edges_per_col = (\n            np.sum(graph_var_incoming_edge_states == 0, axis=0)\n            if len(graph_var_incoming_edge_states) &gt; 0\n            else []\n        )\n        max_undecided = (\n            np.max(undecided_edges_per_col) if len(undecided_edges_per_col) &gt; 0 else 0\n        )\n\n        if max_undecided &gt; 0:\n            max_undecided_idx = np.argmax(undecided_edges_per_col)\n            self._next_exploration = node_names[max_undecided_idx]\n            return self._next_exploration\n\n        # If no suggestion was found, try to find a suggestion from the entire collection of prepared variables.\n        undecided_edges_per_col = np.sum(self._edge_states.m == 0, axis=0)\n        max_undecided = np.max(undecided_edges_per_col)\n\n        if max_undecided &gt; 0:\n            max_undecided_idx = np.argmax(undecided_edges_per_col)\n            self._next_exploration = self._prepared_variables.loc[\n                max_undecided_idx, \"Name\"\n            ]\n            return self._next_exploration\n\n        # If no suggestion was found, return None.\n        self._next_exploration = None\n        return None\n\n    def discover_graph(\n        self,\n        method: str = \"hill_climb\",\n        max_cond_vars: int = 3,\n        model: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Discover a causal graph based on the prepared table automatically.\n\n        Parameters:\n            method: The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".\n            max_cond_vars: The maximum number of conditioning variables to be used for PC.\n            model: The model to be used for GPT-based graph discovery.\n\n        \"\"\"\n\n        if method == \"PC\":\n            self._graph = CausalDiscoverer.pc(\n                self._prepared_log, max_cond_vars=max_cond_vars\n            )\n        elif method == \"hill_climb\":\n            self._graph = CausalDiscoverer.hill_climb(self._prepared_log)\n        elif method == \"exhaustive\":\n            self._graph = CausalDiscoverer.exhaustive(self._prepared_log)\n        elif method == \"GPT\":\n            self._graph = CausalDiscoverer.gpt(self._prepared_log, model=model)\n        else:\n            raise ValueError(f\"Invalid graph discovery method {method}\")\n\n        self._edge_states.clear_and_set_from_graph(self._graph)\n\n    def get_adjusted_ate(\n        self,\n        treatment: str,\n        outcome: str,\n        confounder: Optional[str] = None,\n    ) -&gt; float:\n        \"\"\"\n        Calculate the adjusted ATE of `treatment` on `outcome`, given the current partial causal graph.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n                causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n\n        Returns:\n            The adjusted ATE of `treatment` on `outcome`.\n        \"\"\"\n        return ATECalculator.get_ate_and_confidence(\n            self.prepared_log,\n            self.prepared_variables,\n            treatment,\n            outcome,\n            confounder,\n            graph=self._graph,\n            calculate_p_value=False,\n            calculate_std_error=False,\n        )[\"ATE\"]\n\n    def get_unadjusted_ate(\n        self,\n        treatment: str,\n        outcome: str,\n    ) -&gt; float:\n        \"\"\"\n        Calculate the unadjusted ATE of `treatment` on `outcome`, ignoring the current partial causal graph\n        in favor of a two-node graph with just `treatment` and `outcome`.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n\n        Returns:\n            The unadjusted ATE of `treatment` on `outcome`.\n        \"\"\"\n        return ATECalculator.get_ate_and_confidence(\n            self.prepared_log,\n            self.prepared_variables,\n            treatment,\n            outcome,\n            calculate_p_value=False,\n            calculate_std_error=False,\n        )[\"ATE\"]\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.parsed_log","title":"<code>parsed_log: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed log as a dataframe.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.parsed_variables","title":"<code>parsed_variables: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed variables as a dataframe.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.parsed_templates","title":"<code>parsed_templates: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed templates as a dataframe.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepared_log","title":"<code>prepared_log: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the prepared log as a dataframe.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepared_variables","title":"<code>prepared_variables: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the prepared variables as a dataframe.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepared_variable_names","title":"<code>prepared_variable_names: list[str]</code>  <code>property</code>","text":"<p>Get the names of the prepared variables.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepared_variable_tags","title":"<code>prepared_variable_tags: list[str]</code>  <code>property</code>","text":"<p>Get the tags of the prepared variables.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.num_prepared_variables","title":"<code>num_prepared_variables: int</code>  <code>property</code>","text":"<p>Get the number of prepared variables.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos.exploration_score","title":"<code>exploration_score: float</code>  <code>property</code>","text":"<p>Calculate the exploration score of the current partial causal graph, based on the edge state matrix.</p> <p>Returns:</p> Type Description <code>float</code> <p>The exploration score of the current partial causal graph.</p>"},{"location":"reference/logos/logos/#logos.logos.LOGos._set_vars_to_defaults","title":"<code>_set_vars_to_defaults()</code>","text":"<p>Set some of the variables to their default values.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def _set_vars_to_defaults(self) -&gt; None:\n    \"\"\"\n    Set some of the variables to their default values.\n    \"\"\"\n    # The parsed log as a dataframe, and metadata about the parsed variables.\n    self._parsed_log: pd.DataFrame = pd.DataFrame()\n    self._parsed_variables: pd.DataFrame = pd.DataFrame()\n    self._parsed_templates: pd.DataFrame = pd.DataFrame()\n\n    # The variable used to define causal units and the number of causal units.\n    self._causal_unit_var: Optional[str] = None\n    self._num_causal_units: Optional[int] = None\n\n    # The prepared log as a dataframe, and metadata about the prepared variables.\n    self._prepared_log: pd.DataFrame = pd.DataFrame()\n    self._prepared_variables: pd.DataFrame = pd.DataFrame()\n\n    # The available aggregation and imputation functions.\n    agg_module = importlib.import_module(\"src.logos.aggimp.agg_funcs\")\n    self._agg_funcs: dict[str, Callable] = {\n        n: f for n, f in inspect.getmembers(agg_module, inspect.isfunction)\n    }\n\n    imp_module = importlib.import_module(\"src.logos.aggimp.imp_funcs\")\n    self._imp_funcs: dict[str, Callable] = {\n        n: f for n, f in inspect.getmembers(imp_module, inspect.isfunction)\n    }\n\n    # The graph of causal relationships.\n    self._graph: nx.DiGraph = nx.DiGraph()\n\n    # The exploration progress matrix, indicating which edges have been explored.\n    self._edge_states: Optional[EdgeStateMatrix] = None\n\n    # The most recent next exploration suggestion.\n    self._next_exploration: Optional[str] = None\n\n    # An ECCS object for refinement.\n    self._eccs: Optional[ECCS] = None\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepared_variable_names_with_base_x_and_no_pre_post_agg","title":"<code>prepared_variable_names_with_base_x_and_no_pre_post_agg(x)</code>","text":"<p>Get all prepared variables with the given base variable and no pre- or post-aggregate values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[str, PreparedVariableName]</code> <p>The base variable to check.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables with the given base variable and no pre-</p> <code>list[str]</code> <p>or post-aggregate values.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def prepared_variable_names_with_base_x_and_no_pre_post_agg(\n    self, x: Union[str, PreparedVariableName]\n) -&gt; list[str]:\n    \"\"\"\n    Get all prepared variables with the given base variable and no pre-\n    or post-aggregate values.\n\n    Parameters:\n        x: The base variable to check.\n\n    Returns:\n        A list of variables with the given base variable and no pre-\n        or post-aggregate values.\n    \"\"\"\n    return [\n        var\n        for var in self.prepared_variable_names\n        if PreparedVariableName(var).has_base_var(x)\n        and PreparedVariableName(var).no_pre_post_aggs()\n    ]\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.__init__","title":"<code>__init__(filename, workdir, skip_writeout=False)</code>","text":"<p>Initialize a LOGos instance, giving it the full path to the log file that will be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The full path to the log file that will be analyzed.</p> required <code>workdir</code> <code>str</code> <p>The directory where the parsed and prepared dataframes will be stored.</p> required <code>skip_writeout</code> <code>bool</code> <p>Whether to skip writing out the parsed and prepared dataframes.</p> <code>False</code> Source code in <code>src/logos/logos.py</code> <pre><code>def __init__(\n    self, filename: str, workdir: str, skip_writeout: bool = False\n) -&gt; None:\n    \"\"\"\n    Initialize a LOGos instance, giving it the full path to the log file that will be analyzed.\n\n    Parameters:\n        filename: The full path to the log file that will be analyzed.\n        workdir: The directory where the parsed and prepared dataframes will be stored.\n        skip_writeout: Whether to skip writing out the parsed and prepared dataframes.\n    \"\"\"\n\n    self._set_vars_to_defaults()\n    self._filename = filename\n    print(f\"Initialized LOGos with log file {filename}\")\n\n    # Set and create working directory\n    self._workdir = workdir\n    if not os.path.exists(self._workdir):\n        os.makedirs(self._workdir, exist_ok=True)\n    print(f\"Work directory set to {self._workdir}\")\n\n    self._skip_writeout = skip_writeout\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos._get_filename","title":"<code>_get_filename(var_name)</code>","text":"<p>Create the file name string for dumping/loading pkl files.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>The name of the variable to be dumped/loaded.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The file name string.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def _get_filename(self, var_name: str) -&gt; str:\n    \"\"\"\n    Create the file name string for dumping/loading pkl files.\n\n    Parameters:\n        var_name: The name of the variable to be dumped/loaded.\n\n    Returns:\n        The file name string.\n    \"\"\"\n    return os.path.join(\n        self._workdir,\n        os.path.basename(self._filename)\n        + f\"{var_name}_{self._causal_unit_var}_{self._num_causal_units}.pkl\",\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos._find_type","title":"<code>_find_type(row)</code>","text":"<p>Identify the type of a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row of the parsed variables dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".</p> Source code in <code>src/logos/logos.py</code> <pre><code>def _find_type(self, row: pd.Series) -&gt; str:\n    \"\"\"\n    Identify the type of a parsed variable.\n\n    Parameters:\n        row: A row of the parsed variables dataframe.\n\n    Returns:\n        The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".\n    \"\"\"\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"error\", category=UserWarning)\n\n        try:\n            y = pd.to_numeric(row[\"Examples\"], errors=\"raise\")\n            return \"num\"\n        except Exception as e:\n            try:\n                y = pd.to_timedelta(row[\"Examples\"], errors=\"raise\")\n                return \"time\"\n            except Exception as e:\n                try:\n                    y = pd.to_datetime(row[\"Examples\"], errors=\"raise\")\n                    return \"date\"\n                except Exception as e:\n                    return \"str\"\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos._find_uninteresting","title":"<code>_find_uninteresting(row)</code>","text":"<p>Identify whether a parsed variable is likely to be uninteresting.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row of the parsed variables dataframe.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the variable is likely to be uninteresting, False otherwise.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def _find_uninteresting(self, row: pd.Series) -&gt; bool:\n    \"\"\"\n    Identify whether a parsed variable is likely to be uninteresting.\n\n    Parameters:\n        row: A row of the parsed variables dataframe.\n\n    Returns:\n        True if the variable is likely to be uninteresting, False otherwise.\n    \"\"\"\n    return (\n        row[\"Type\"] != \"num\"\n        and (self._parsed_log[row[\"Name\"]].nunique() &gt;= 0.15 * row[\"Occurrences\"])\n    ) or (self._parsed_log[row[\"Name\"]].nunique() == 1)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.parse","title":"<code>parse(regex_dict=DEFAULT_REGEX_DICT, sim_thresh=0.65, depth=5, force=False, message_prefix='.*', enable_gpt_tagging=False)</code>","text":"<p>Parse the log file into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>regex_dict</code> <code>dict[str, str]</code> <p>(for Drain) A dictionary of regular expressions to be used for parsing.</p> <code>DEFAULT_REGEX_DICT</code> <code>sim_thresh</code> <code>float</code> <p>(for Drain) The similarity threshold to be used for parsing.</p> <code>0.65</code> <code>depth</code> <code>int</code> <p>(for Drain) The parse tree depth to be used for parsing.</p> <code>5</code> <code>force</code> <code>bool</code> <p>Whether to force re-parsing of the log file.</p> <code>False</code> <code>message_prefix</code> <code>str</code> <p>A prefix used to identify the beginning of each log message. Can be used to collapse multiple lines into a single message. Each line that doesn't start with this prefix will be concatenated to the previous log message.</p> <code>'.*'</code> <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT tagging should be enabled.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The time elapsed for parsing, as a string.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def parse(\n    self,\n    regex_dict: dict[str, str] = DEFAULT_REGEX_DICT,\n    sim_thresh: float = 0.65,\n    depth: int = 5,\n    force: bool = False,\n    message_prefix: str = r\".*\",\n    enable_gpt_tagging: bool = False,\n) -&gt; str:\n    \"\"\"\n    Parse the log file into a dataframe.\n\n    Parameters:\n        regex_dict: (for Drain) A dictionary of regular expressions to be used for parsing.\n        sim_thresh: (for Drain) The similarity threshold to be used for parsing.\n        depth: (for Drain) The parse tree depth to be used for parsing.\n        force: Whether to force re-parsing of the log file.\n        message_prefix: A prefix used to identify the beginning of each log message.\n            Can be used to collapse multiple lines into a single message. Each line that doesn't start with this\n            prefix will be concatenated to the previous log message.\n        enable_gpt_tagging: A boolean indicating whether GPT tagging should be enabled.\n\n    Returns:\n        The time elapsed for parsing, as a string.\n    \"\"\"\n    start_time = datetime.now()\n    parser = Drain(\n        indir=os.path.dirname(self._filename),\n        depth=depth,\n        st=sim_thresh,\n        rex=regex_dict,\n        skip_writeout=self._skip_writeout,\n        message_prefix=message_prefix,\n    )\n\n    # Check if the parsed files already exist.\n    files_exist = not force\n    parsed_df_names = [\n        nameof(self._parsed_log),\n        nameof(self._parsed_templates),\n        nameof(self._parsed_variables),\n    ]\n    for var_name in parsed_df_names:\n        if not os.path.isfile(self._get_filename(var_name)):\n            files_exist = False\n            break\n\n    if files_exist:\n        self._parsed_log = Pickler.load(self._get_filename(parsed_df_names[0]))\n        self._parsed_templates = Pickler.load(\n            self._get_filename(parsed_df_names[1])\n        )\n        self._parsed_variables = Pickler.load(\n            self._get_filename(parsed_df_names[2])\n        )\n    else:\n        (\n            self._parsed_log,\n            self._parsed_templates,\n            self._parsed_variables,\n        ) = parser.parse(self._filename.split(\"/\")[-1])\n        tqdm.pandas(desc=\"Determining variable types...\")\n        self._parsed_variables[\"Type\"] = self._parsed_variables.progress_apply(\n            self._find_type, axis=1\n        )\n\n        # Cast and convert date columns\n        is_date = self._parsed_variables[\"Type\"] == \"date\"\n        date_cols = self._parsed_variables.loc[is_date, \"Name\"]\n        tqdm.pandas(desc=\"Casting date variables...\")\n        self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_apply(\n            pd.to_datetime, errors=\"coerce\"\n        )\n        tqdm.pandas(desc=\"Casting date variables round 2...\")\n        self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_applymap(\n            lambda x: x.timestamp() if not pd.isnull(x) else None\n        )\n        self._parsed_variables.loc[is_date, \"Type\"] = \"num\"\n\n        # Cast and convert time columns\n        is_time = self._parsed_variables[\"Type\"] == \"time\"\n        time_cols = self._parsed_variables.loc[is_time, \"Name\"]\n        tqdm.pandas(desc=\"Casting time variables...\")\n        self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_apply(\n            pd.to_timedelta, errors=\"coerce\"\n        )\n        tqdm.pandas(desc=\"Casting time variables round 2...\")\n        self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_applymap(\n            lambda x: x.total_seconds() if not pd.isnull(x) else None\n        )\n        self._parsed_variables.loc[is_time, \"Type\"] = \"num\"\n\n        # Cast numeric columns\n        is_num = self._parsed_variables[\"Type\"] == \"num\"\n        numeric_cols = self._parsed_variables.loc[is_num, \"Name\"]\n        tqdm.pandas(desc=\"Casting numerical variables...\")\n        self._parsed_log[numeric_cols] = self._parsed_log[\n            numeric_cols\n        ].progress_apply(pd.to_numeric, errors=\"coerce\")\n\n        # Tag variables.\n        tqdm.pandas(desc=\"Tagging variables...\")\n        if enable_gpt_tagging:\n            tag, tag_origin = zip(\n                *self._parsed_variables.progress_apply(\n                    lambda x: TagUtils.waterfall_tag(self.parsed_templates, x),\n                    axis=1,\n                )\n            )\n        else:\n            tag, tag_origin = zip(\n                *self._parsed_variables.progress_apply(\n                    lambda x: TagUtils.preceding_tokens_tag(x),\n                    axis=1,\n                )\n            )\n        self._parsed_variables[\"Tag\"] = tag\n        self._parsed_variables[\"TagOrigin\"] = tag_origin\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n        # Detect identifiers.\n        tqdm.pandas(desc=\"Detecting identifiers...\")\n        self._parsed_variables[\"IsUninteresting\"] = (\n            self._parsed_variables.progress_apply(self._find_uninteresting, axis=1)\n        )\n\n        # Reorder columns.\n        self._parsed_variables = self._parsed_variables[\n            [\n                \"Name\",\n                \"Tag\",\n                \"TagOrigin\",\n                \"Type\",\n                \"IsUninteresting\",\n                \"Occurrences\",\n                \"Preceding 3 tokens\",\n                \"Examples\",\n                \"From regex\",\n            ]\n        ]\n\n    # Write out files if appropriate.\n    if not self._skip_writeout and not files_exist:\n        Pickler.dump(self._parsed_log, self._get_filename(parsed_df_names[0]))\n        Pickler.dump(self._parsed_templates, self._get_filename(parsed_df_names[1]))\n        Pickler.dump(self._parsed_variables, self._get_filename(parsed_df_names[2]))\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(f\"Parsing complete in {elapsed} seconds!\")\n    return elapsed\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.include_in_template","title":"<code>include_in_template(var, enable_gpt_tagging=False, skip_writeout=None)</code>","text":"<p>Treat a certain parsed variable as part of its template and regenerate parsed dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable to be included in its template.</p> required <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT-3.5 tagging should be enabled.</p> <code>False</code> <code>skip_writeout</code> <code>Optional[bool]</code> <p>Whether to skip writing out the regenerated parsed dataframes. Defaults to the value of self._skip_writeout.</p> <code>None</code> Source code in <code>src/logos/logos.py</code> <pre><code>def include_in_template(\n    self,\n    var: str,\n    enable_gpt_tagging: bool = False,\n    skip_writeout: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"\n    Treat a certain parsed variable as part of its template and regenerate parsed dataframes.\n\n    Parameters:\n        var: The name or tag of the variable to be included in its template.\n        enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n        skip_writeout: Whether to skip writing out the regenerated parsed dataframes. Defaults\n            to the value of self._skip_writeout.\n    \"\"\"\n    name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n\n    old_template_id = ParsedVariableName(name).template_id()\n    idx = ParsedVariableName(name).index()\n    value_counts = self._parsed_log[name].value_counts().to_dict()\n\n    ### Modify _parsed_templates\n    old_template_row = (\n        self._parsed_templates.loc[\n            self._parsed_templates[\"TemplateId\"] == old_template_id\n        ]\n        .iloc[0]\n        .copy()\n    )\n    toks = old_template_row[\"TemplateText\"].split(\" \")\n    new_template_ids = {}\n    new_variable_indices = old_template_row[\"VariableIndices\"]\n    new_variable_indices.remove(idx)\n\n    for value, occurences in value_counts.items():\n        new_template_row = old_template_row.copy()\n        toks[idx] = value\n\n        new_template_row[\"TemplateText\"] = \" \".join(toks)\n        new_template_row[\"TemplateId\"] = hashlib.md5(\n            new_template_row[\"TemplateText\"].encode(\"utf-8\")\n        ).hexdigest()[0:8]\n        new_template_row[\"Occurrences\"] = occurences\n        new_template_row[\"VariableIndices\"] = new_variable_indices\n        new_template_row[\"RegexIndices\"] = old_template_row[\"RegexIndices\"]\n\n        self._parsed_templates.loc[len(self._parsed_templates)] = new_template_row\n        new_template_ids[value] = new_template_row[\"TemplateId\"]\n\n    self._parsed_templates = self._parsed_templates[\n        self._parsed_templates[\"TemplateId\"] != old_template_id\n    ].reset_index(drop=True)\n\n    ### Modify _parsed_log\n\n    # Update the template ids of all rows that belonged to the old template\n    self._parsed_log[\"TemplateId\"] = self._parsed_log.apply(\n        lambda x: (\n            new_template_ids[x[name]]\n            if (x[\"TemplateId\"] == old_template_id)\n            else x[\"TemplateId\"]\n        ),\n        axis=1,\n    )\n\n    # Create new variables for each new template id and assign the value of the old variables to them\n    new_variables = []\n    for new_template_id in new_template_ids.values():\n        for other_idx in new_variable_indices:\n            new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n            new_variables.append(new_var_name)\n            self._parsed_log[new_var_name] = self._parsed_log.apply(\n                lambda x: (\n                    x[f\"{old_template_id}_{other_idx}\"]\n                    if (x[\"TemplateId\"] == new_template_id)\n                    else None\n                ),\n                axis=1,\n            )\n\n    # Drop variable columns associated with old template id\n    variables_to_drop = [\n        v for v in self._parsed_log.columns if v.startswith(old_template_id)\n    ]\n    self._parsed_log.drop(columns=variables_to_drop, inplace=True)\n\n    ### Modify _parsed_variables\n\n    # Add variable rows for each new variable\n    for value, occurrences in value_counts.items():\n        for other_idx in new_variable_indices:\n            new_template_id = new_template_ids[value]\n            new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n\n            x = {}\n            x[\"Name\"] = new_var_name\n            x[\"Occurrences\"] = occurrences\n            x[\"Preceding 3 tokens\"] = (\n                self._parsed_templates[\n                    self._parsed_templates[\"TemplateId\"] == new_template_id\n                ][\"TemplateText\"]\n                .values[0]\n                .split()[max(0, other_idx - 3) : other_idx]\n            )\n            x[\"Examples\"] = (\n                self._parsed_log[new_var_name]\n                .loc[self._parsed_log[new_var_name].notna()]\n                .unique()[:5]\n                .tolist()\n            )\n            x[\"From regex\"] = False\n            if enable_gpt_tagging:\n                x[\"Tag\"], x[\"TagOrigin\"] = TagUtils.waterfall_tag(\n                    self.parsed_templates, pd.Series(x)\n                )\n            else:\n                x[\"Tag\"], x[\"TagOrigin\"] = TagUtils.preceding_tokens_tag(\n                    pd.Series(x)\n                )\n            x[\"Type\"] = self._find_type(pd.Series(x))\n            x[\"IsUninteresting\"] = self._find_uninteresting(pd.Series(x))\n\n            self._parsed_variables.loc[len(self._parsed_variables)] = x\n\n    # Drop variable rows associated with old template id\n    self._parsed_variables = self._parsed_variables[\n        ~self._parsed_variables[\"Name\"].isin(variables_to_drop)\n    ].reset_index(drop=True)\n\n    # Deduplicate tags again\n    TagUtils.deduplicate_tags(self._parsed_variables)\n\n    # Write out files if appropriate.\n    if skip_writeout is None:\n        skip_writeout = self._skip_writeout\n    if not skip_writeout:\n        print(\"about to write stuff out\")\n        Pickler.dump(self._parsed_log, self._get_filename(nameof(self._parsed_log)))\n        Pickler.dump(\n            self._parsed_templates,\n            self._get_filename(nameof(self._parsed_templates)),\n        )\n        Pickler.dump(\n            self._parsed_variables,\n            self._get_filename(nameof(self._parsed_variables)),\n        )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.tag_parsed_variable","title":"<code>tag_parsed_variable(name, tag)</code>","text":"<p>Tag a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to be tagged.</p> required <code>tag</code> <code>str</code> <p>The tag to be assigned to the variable.</p> required Source code in <code>src/logos/logos.py</code> <pre><code>def tag_parsed_variable(self, name: str, tag: str) -&gt; None:\n    \"\"\"\n    Tag a parsed variable.\n\n    Parameters:\n        name: The name of the variable to be tagged.\n        tag: The tag to be assigned to the variable.\n    \"\"\"\n    TagUtils.set_tag(self._parsed_variables, name, tag, \"parsed\")\n    TagUtils.deduplicate_tags(self._parsed_variables)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_tag_of_parsed","title":"<code>get_tag_of_parsed(name)</code>","text":"<p>Get the tag of a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The tag of the variable.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_tag_of_parsed(self, name: str) -&gt; str:\n    \"\"\"\n    Get the tag of a parsed variable.\n\n    Parameters:\n        name: The name of the variable.\n\n    Returns:\n        The tag of the variable.\n    \"\"\"\n    return TagUtils.get_tag(self._parsed_variables, name, \"parsed\")\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.tag_prepared_variable","title":"<code>tag_prepared_variable(name, tag)</code>","text":"<p>Tag a prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to be tagged.</p> required <code>tag</code> <code>str</code> <p>The tag to be assigned to the variable.</p> required Source code in <code>src/logos/logos.py</code> <pre><code>def tag_prepared_variable(self, name: str, tag: str) -&gt; None:\n    \"\"\"\n    Tag a prepared variable.\n\n    Parameters:\n        name: The name of the variable to be tagged.\n        tag: The tag to be assigned to the variable.\n    \"\"\"\n    TagUtils.set_tag(self._prepared_variables, name, tag, \"prepared\")\n    TagUtils.deduplicate_tags(self._prepared_variables)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_tag_of_prepared","title":"<code>get_tag_of_prepared(name)</code>","text":"<p>Get the tag of a prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The tag of the variable.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_tag_of_prepared(self, name: str) -&gt; str:\n    \"\"\"\n    Get the tag of a prepared variable.\n\n    Parameters:\n        name: The name of the variable.\n\n    Returns:\n        The tag of the variable.\n    \"\"\"\n    return TagUtils.get_tag(self._prepared_variables, name, \"prepared\")\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_causal_unit_info","title":"<code>get_causal_unit_info()</code>","text":"<p>Get the variable used to define causal units and the number of causal units.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the variable used to define causal units</p> <code>int</code> <p>and the number of causal units.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_causal_unit_info(self) -&gt; Tuple[str, int]:\n    \"\"\"\n    Get the variable used to define causal units and the number of\n    causal units.\n\n    Returns:\n        The name of the variable used to define causal units\n        and the number of causal units.\n    \"\"\"\n    return self._causal_unit_var, self._num_causal_units\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.suggest_causal_unit_defs","title":"<code>suggest_causal_unit_defs(min_causal_units=4, num_suggestions=10)</code>","text":"<p>Suggest at most <code>num_suggestions</code> causal unit definitions based on IUS maximization, while returning at least <code>min_causal_units</code> causal units.</p> <p>Parameters:</p> Name Type Description Default <code>min_causal_units</code> <code>int</code> <p>The minimum number of causal units that a suggested definition should create.</p> <code>4</code> <code>num_suggestions</code> <code>int</code> <p>The maximum number of causal unit definitions to suggest.</p> <code>10</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame with one row for each suggested causal unit definition, or <code>None</code> if no suggestions were made.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def suggest_causal_unit_defs(\n    self,\n    min_causal_units: int = 4,\n    num_suggestions: int = 10,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Suggest at most `num_suggestions` causal unit definitions based on IUS maximization,\n    while returning at least `min_causal_units` causal units.\n\n    Parameters:\n        min_causal_units: The minimum number of causal units that a suggested\n            definition should create.\n        num_suggestions: The maximum number of causal unit definitions to suggest.\n\n    Returns:\n        A DataFrame with one row for each suggested causal unit definition, or `None`\n            if no suggestions were made.\n    \"\"\"\n\n    return CausalUnitSuggester.suggest_causal_unit_defs(\n        self._parsed_log[self._parsed_variables[\"Name\"].values],\n        self._parsed_variables,\n        min_causal_units=min_causal_units,\n        num_suggestions=num_suggestions,\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.set_causal_unit","title":"<code>set_causal_unit(var, num_units=None)</code>","text":"<p>Set the variable used to define causal units and optionally the number of causal units. The latter will be ignored if the variable is categorical, but it must be specified if the variable is numerical.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable to be used as the causal unit.</p> required <code>num_units</code> <code>Optional[int]</code> <p>The number of causal units to be created.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the variable is numerical and <code>num_units</code> is not specified.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def set_causal_unit(\n    self,\n    var: str,\n    num_units: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Set the variable used to define causal units and optionally the number of\n    causal units. The latter will be ignored if the variable is categorical, but it\n    must be specified if the variable is numerical.\n\n    Parameters:\n        var: The name or tag of the variable to be used as the causal unit.\n        num_units: The number of causal units to be created.\n\n    Raises:\n        ValueError: If the variable is numerical and `num_units` is not specified.\n    \"\"\"\n    var_name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n    var_type = self._parsed_variables.loc[\n        self._parsed_variables[\"Name\"] == var_name, \"Type\"\n    ].values[0]\n\n    if var_type == \"num\" and num_units is None:\n        raise ValueError(\n            \"The number of causal units must be specified if the causal unit is numerical.\"\n        )\n\n    self._causal_unit_var = var_name\n    self._num_causal_units = num_units\n\n    print(\n        f\"Causal unit set to {var_name} (tag: {self.get_tag_of_parsed(var_name)}) \"\n        + (\n            \"\"\n            if not self._num_causal_units\n            else f\" with {self._num_causal_units} causal units.\"\n        )\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.prepare","title":"<code>prepare(custom_agg={}, custom_imp={}, count_occurences=False, ignore_uninteresting=True, force=False, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER, drop_bad_aggs=True, reject_prunable_edges=False)</code>","text":"<p>Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.</p> <p>Parameters:</p> Name Type Description Default <code>custom_agg</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom aggregation functions to be used for specific variables.</p> <code>{}</code> <code>custom_imp</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom imputation functions to be used for specific variables.</p> <code>{}</code> <code>count_occurences</code> <code>bool</code> <p>Whether to include extra variables counting the occurence of each template.</p> <code>False</code> <code>ignore_uninteresting</code> <code>bool</code> <p>Whether to ignore uninteresting variables.</p> <code>True</code> <code>force</code> <code>bool</code> <p>Whether to force re-preparation of the log.</p> <code>False</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for LASSO regression.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for LASSO regression.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>drop_bad_aggs</code> <code>bool</code> <p>Whether to drop prepared variables that do not add information compared to other variables based on the same base variable but using a different aggregation function.</p> <code>True</code> <code>reject_prunable_edges</code> <code>bool</code> <p>Whether to reject edges that are prunable based on LASSO applciation.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The time elapsed for preparation, as a string.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def prepare(\n    self,\n    custom_agg: dict[str, list[str]] = {},\n    custom_imp: dict[str, list[str]] = {},\n    count_occurences: bool = False,\n    ignore_uninteresting: bool = True,\n    force: bool = False,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    drop_bad_aggs: bool = True,\n    reject_prunable_edges: bool = False,\n) -&gt; str:\n    \"\"\"\n    Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.\n\n    Parameters:\n        custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n        custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n        count_occurences: Whether to include extra variables counting the occurence of each template.\n        ignore_uninteresting: Whether to ignore uninteresting variables.\n        force: Whether to force re-preparation of the log.\n        lasso_alpha: The alpha parameter to be used for LASSO regression.\n        lasso_max_iter: The maximum number of iterations to be used for LASSO regression.\n        drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n            variables based on the same base variable but using a different aggregation function.\n        reject_prunable_edges: Whether to reject edges that are prunable based on LASSO applciation.\n\n    Returns:\n        The time elapsed for preparation, as a string.\n    \"\"\"\n\n    start_time = datetime.now()\n    # Ensure causal unit is set. TODO: make IUS maximizer the default\n    if self._causal_unit_var is None:\n        print(\"Causal unit not defined. Aborting.\")\n        return None\n\n    # Check if the prepared files already exist.\n    files_exist = not force\n    prepared_df_names = [\n        nameof(self._prepared_log),\n        nameof(self._prepared_variables),\n    ]\n    for var_name in prepared_df_names:\n        if not os.path.isfile(self._get_filename(var_name)):\n            files_exist = False\n            break\n\n    if files_exist:\n        self._prepared_log = Pickler.load(self._get_filename(prepared_df_names[0]))\n        self._prepared_variables = Pickler.load(\n            self._get_filename(prepared_df_names[1])\n        )\n    else:\n        self._prepare_anew(\n            custom_agg,\n            custom_imp,\n            count_occurences=count_occurences,\n            ignore_uninteresting=ignore_uninteresting,\n            drop_bad_aggs=drop_bad_aggs,\n        )\n\n    self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n    if reject_prunable_edges:\n        print(f\"Pruning edges...\")\n        self.reject_all_prunable_edges(\n            lasso_alpha=lasso_alpha, lasso_max_iter=lasso_max_iter\n        )\n\n    self._eccs = ECCS(self._prepared_log, nx.DiGraph())\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(\n        f\"\"\"Preparation complete in {elapsed} seconds! \"\"\"\n        f\"\"\"{np.count_nonzero(self._edge_states.m == -1)} of the {self.num_prepared_variables ** 2} possible edges were auto-rejected.\"\"\"\n    )\n\n    return elapsed\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos._prepare_anew","title":"<code>_prepare_anew(custom_agg={}, custom_imp={}, count_occurences=False, ignore_uninteresting=True, drop_bad_aggs=True)</code>","text":"<p>Prepare the log anew.</p> <p>Parameters:</p> Name Type Description Default <code>custom_agg</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom aggregation functions to be used for specific variables.</p> <code>{}</code> <code>custom_imp</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom imputation functions to be used for specific variables.</p> <code>{}</code> <code>count_occurences</code> <code>bool</code> <p>Whether to include extra variables counting the occurence of each template.</p> <code>False</code> <code>ignore_uninteresting</code> <code>bool</code> <p>Whether to ignore uninteresting variables.</p> <code>True</code> <code>drop_bad_aggs</code> <code>bool</code> <p>Whether to drop prepared variables that do not add information compared to other variables based on the same base variable but using a different aggregation function.</p> <code>True</code> Source code in <code>src/logos/logos.py</code> <pre><code>def _prepare_anew(\n    self,\n    custom_agg: dict[str, list[str]] = {},\n    custom_imp: dict[str, list[str]] = {},\n    count_occurences: bool = False,\n    ignore_uninteresting: bool = True,\n    drop_bad_aggs: bool = True,\n) -&gt; None:\n    \"\"\"\n    Prepare the log anew.\n\n    Parameters:\n        custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n        custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n        count_occurences: Whether to include extra variables counting the occurence of each template.\n        ignore_uninteresting: Whether to ignore uninteresting variables.\n        drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n            variables based on the same base variable but using a different aggregation function.\n    \"\"\"\n\n    print(f\"Determining the causal unit assignment...\")\n    causal_unit_assignment = CausalUnitSuggester._discretize(\n        self._parsed_log[self._causal_unit_var],\n        self._parsed_variables[\n            self._parsed_variables[\"Name\"] == self._causal_unit_var\n        ][\"Type\"].values[0],\n        self._num_causal_units,\n    )\n\n    # Convert keys in custom_agg and custom_imp to the names of the variables, if they are tags.\n    custom_agg = {\n        TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n        for k, v in custom_agg.items()\n    }\n    custom_imp = {\n        TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n        for k, v in custom_imp.items()\n    }\n\n    # Start with the parsed log, optionally with extra variables counting the occurence of each template.\n    if count_occurences:\n        print(f\"Adding template occurrence count variables...\")\n        self._prepared_log = pd.concat(\n            [\n                self._parsed_log,\n                pd.get_dummies(\n                    self._parsed_log[\"TemplateId\"],\n                    prefix=\"TemplateId\",\n                    prefix_sep=\"=\",\n                ),\n            ],\n            axis=1,\n        )\n    else:\n        self._prepared_log = self._parsed_log.copy(deep=True)\n\n    # No longer need the column storing the actual template IDs\n    self._prepared_log.drop(columns=\"TemplateId\", inplace=True)\n\n    # Build dictionary of aggregation functions\n    agg_dict: dict[str, str] = {\n        variable.Name: (\n            custom_agg[variable.Name]\n            if variable.Name in custom_agg\n            else AggregateSelector.DEFAULT_AGGREGATES[variable.Type]\n        )\n        for variable in self._parsed_variables.itertuples()\n    }\n\n    # Add aggregations for template counts\n    for col in self._prepared_log.columns:\n        if PreparedVariableName(col).base_var() == \"TemplateId\":\n            agg_dict[col] = [\"sum\"]\n\n    # Drop uninteresting columns if requested, except if they are the causal unit.\n    ui_cols = self._parsed_variables.loc[\n        self._parsed_variables[\"IsUninteresting\"], \"Name\"\n    ].values\n    ui_cols = [x for x in ui_cols if x != self._causal_unit_var]\n    if ignore_uninteresting:\n        self._prepared_log.drop(\n            columns=ui_cols,\n            inplace=True,\n        )\n        for col in ui_cols:\n            agg_dict.pop(col, None)\n        print(\n            f\"Dropped {len(ui_cols)} uninteresting columns, out of an original total of {len(self.parsed_variables)}.\"\n        )\n\n    # Ensure the causal unit variable only has one aggregation function\n    agg_dict[self._causal_unit_var] = agg_dict[self._causal_unit_var][:1]\n\n    # Perform the aggregation\n    print(\"Calculating aggregates for each causal unit...\")\n    agg_func_dict: dict[str, list[Callable]] = {\n        name: [self._agg_funcs[f] for f in funcs]\n        for name, funcs in agg_dict.items()\n    }\n    self._prepared_log = self._prepared_log.groupby(\n        causal_unit_assignment\n    ).aggregate(agg_func_dict)\n    self._prepared_log.columns = [\n        \"+\".join(col) for col in self._prepared_log.columns.values\n    ]\n    self._parsed_variables[\"Aggregates\"] = self._parsed_variables[\"Name\"].map(\n        lambda x: agg_dict.get(x, [])\n    )\n    self._prepared_log.set_index(\n        f\"{self._causal_unit_var}+{self._parsed_variables[self._parsed_variables['Name'] == self._causal_unit_var]['Aggregates'].values[0][0]}\",\n        inplace=True,\n    )\n    self._prepared_log.sort_index(inplace=True)\n    self._prepared_log.index = self._prepared_log.index.astype(str)\n\n    # Perform the imputation\n    for col in tqdm(self._prepared_log.columns, desc=\"Imputing missing values...\"):\n        if self._prepared_log[col].isnull().values.any():\n            base_var = PreparedVariableName(col).base_var()\n            func_name: str = (\n                custom_imp[base_var] if base_var in custom_imp else \"no_imp\"\n            )\n            self._prepared_log[col] = (self._imp_funcs[func_name])(\n                self._prepared_log[col]\n            )\n    self._prepared_log.dropna(inplace=True)\n\n    # Drop variables that do not add information compared to other variables based on the same base variable\n    # but using a different aggregation function.\n    if drop_bad_aggs:\n        print(f\"Dropping aggregates that do not add information...\")\n        cols_to_drop = AggregateSelector.find_uninformative_aggregates(\n            self._prepared_log, self._parsed_variables, self._causal_unit_var\n        )\n        self._prepared_log.drop(columns=cols_to_drop, inplace=True)\n\n    # Identify the categorical variables and one-hot encode them\n    categorical_vars = self._prepared_log.select_dtypes(\n        include=\"object\"\n    ).columns.tolist()\n    for col in tqdm(\n        categorical_vars, desc=\"One-hot encoding categorical variables...\"\n    ):\n        self._prepared_log = pd.concat(\n            [\n                self._prepared_log,\n                pd.get_dummies(\n                    self._prepared_log[col], prefix=col, prefix_sep=\"=\", dtype=int\n                ),\n            ],\n            axis=1,\n        )\n        self._prepared_log.drop(col, axis=1, inplace=True)\n    # Deal with https://github.com/pydot/pydot/issues/258\n    self._prepared_log.columns = [\n        x.replace(\":\", \";\") for x in self._prepared_log.columns\n    ]\n\n    # Generate dataframe of prepared variables for later tagging etc.\n    self._generate_prepared_variables_df()\n\n    # Convert any date columns to Unix timestamps in milliseconds\n    date_cols = self._prepared_variables.loc[\n        self._prepared_variables[\"Type\"] == \"date\", \"Name\"\n    ].values\n    self._prepared_log[date_cols] = self._prepared_log[date_cols].map(\n        lambda x: x.timestamp() * 1000.0\n    )\n\n    # Convert any time columns to milliseconds\n    time_cols = self._prepared_variables.loc[\n        self._prepared_variables[\"Type\"] == \"time\", \"Name\"\n    ].values\n    self._prepared_log[time_cols] = self._prepared_log[time_cols].map(\n        lambda x: x.total_seconds() * 1000.0\n    )\n\n    # Write out prepared log and variables\n    if not self._skip_writeout:\n        Pickler.dump(\n            self._prepared_log, self._get_filename(nameof(self._prepared_log))\n        )\n        Pickler.dump(\n            self._prepared_variables,\n            self._get_filename(nameof(self._prepared_variables)),\n        )\n\n    print(\n        f\"\"\"Successfully prepared the log with causal unit {self._causal_unit_var} \"\"\"\n        f\"\"\"(tag: {self.get_tag_of_parsed(self._causal_unit_var)})\"\"\"\n        + (\n            \"\"\n            if not self._num_causal_units\n            else f\" with {self._num_causal_units} causal units.\"\n        )\n    )\n\n    return\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos._generate_prepared_variables_df","title":"<code>_generate_prepared_variables_df()</code>","text":"<p>Generate dataframe of prepared variables for later tagging etc.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def _generate_prepared_variables_df(self) -&gt; None:\n    \"\"\"\n    Generate dataframe of prepared variables for later tagging etc.\n    \"\"\"\n\n    self._prepared_variables = pd.DataFrame()\n    self._prepared_variables[\"Name\"] = self._prepared_log.columns\n\n    # Bring in varable name components leveraging PreparedVariableName\n    self._prepared_variables[\"Base\"] = self._prepared_variables[\"Name\"].apply(\n        lambda x: PreparedVariableName(x).base_var()\n    )\n    self._prepared_variables[\"Pre-agg Value\"] = self._prepared_variables[\n        \"Name\"\n    ].apply(lambda x: PreparedVariableName(x).pre_agg_value())\n    self._prepared_variables[\"Agg\"] = self._prepared_variables[\"Name\"].apply(\n        lambda x: PreparedVariableName(x).aggregate()\n    )\n    self._prepared_variables[\"Post-agg Value\"] = self._prepared_variables[\n        \"Name\"\n    ].apply(lambda x: PreparedVariableName(x).post_agg_value())\n\n    # Bring in other info from self._parsed_variables\n    self._prepared_variables[\"Tag\"] = self._prepared_variables.apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x[\"Base\"],\n                \"Tag\",\n            ].values[0]\n            if x[\"Base\"] != \"TemplateId\"\n            else \"TemplateId\"\n        )\n        + (f\" {x['Pre-agg Value']}\" if x[\"Pre-agg Value\"] != \"\" else \"\")\n        + (f\" {x['Agg']}\" if x[\"Agg\"] != \"\" else \"\")\n        + (f\" {x['Post-agg Value']}\" if x[\"Post-agg Value\"] != \"\" else \"\"),\n        axis=1,\n    )\n    self._prepared_variables[\"Base Variable Occurences\"] = self._prepared_variables[\n        \"Base\"\n    ].apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Occurrences\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n    )\n    self._prepared_variables[\"Type\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Type\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n    )\n    self._prepared_variables[\"Examples\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Examples\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n    )\n    self._prepared_variables[\"From regex\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"From regex\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n    )\n\n    # Bring in template text, only for appropriate base variables.\n    self._prepared_variables[\"TemplateText\"] = self._prepared_variables.apply(\n        lambda x: (\n            self._parsed_templates.loc[\n                self._parsed_templates[\"TemplateId\"]\n                == PreparedVariableName(x[\"Name\"]).template_id(),\n                \"TemplateText\",\n            ].values[0]\n            if x[\"From regex\"] == False\n            else \"\"\n        ),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.inspect","title":"<code>inspect(var, ref_var=None, row_limit=10)</code>","text":"<p>Print information about a specific prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable.</p> required <code>ref_var</code> <code>Optional[str]</code> <p>The name or tag of a reference variable.</p> <code>None</code> <code>row_limit</code> <code>Optional[int]</code> <p>The number of rows of the prepared log to print out, to illustrate example values of this variable.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing: (1) Information about the base variable of <code>var</code>, if <code>var</code> is not related to the     occurrence count of a template. (2) Information about the template of <code>var</code>, if <code>var</code> was not created from a regex. (3) A sample of the prepared log, with <code>row_limit</code> rows.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def inspect(\n    self,\n    var: str,\n    ref_var: Optional[str] = None,\n    row_limit: Optional[int] = 10,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Print information about a specific prepared variable.\n\n    Parameters:\n        var: The name or tag of the variable.\n        ref_var: The name or tag of a reference variable.\n        row_limit: The number of rows of the prepared log to print out,\n            to illustrate example values of this variable.\n\n    Returns:\n        A tuple containing:\n            (1) Information about the base variable of `var`, if `var` is not related to the\n                occurrence count of a template.\n            (2) Information about the template of `var`, if `var` was not created from a regex.\n            (3) A sample of the prepared log, with `row_limit` rows.\n    \"\"\"\n\n    # Retrieve the name of this variable, if a tag was passed in.\n    name = TagUtils.name_of(self._prepared_variables, var, \"prepared\")\n\n    print(f\"Information about prepared variable {name}:\\n\")\n    base_var = PreparedVariableName(name).base_var()\n    from_regex = False\n\n    base_var_info_df = pd.DataFrame()\n    if base_var != \"TemplateId\":\n        print(f\"--&gt; Variable Information about {base_var}:\")\n        base_var_info_df = self._parsed_variables[\n            self._parsed_variables[\"Name\"] == base_var\n        ]\n        from_regex = base_var_info_df[\"From regex\"].values[0]\n        display(base_var_info_df)\n\n    template_info_df = pd.DataFrame()\n    if not from_regex:\n        template_id = PreparedVariableName(name).template_id()\n        print(f\"--&gt; Template Information about {template_id}:\")\n        template_info_df = self._parsed_templates[\n            self._parsed_templates[\"TemplateId\"] == template_id\n        ]\n        display(template_info_df)\n\n    print(\"--&gt; Causal Unit Partial Information:\")\n    if row_limit == None:\n        row_limit = len(self._prepared_log)\n    col_list = [name]\n    col_list.extend([ref_var] if ref_var is not None else [])\n    prepared_log_info_df = self._prepared_log[col_list].head(row_limit)\n    col_names = [f\"{name} (candidate)\"]\n    col_names.extend([f\"{ref_var} (outcome)\"] if ref_var is not None else [])\n    prepared_log_info_df.columns = col_names\n    display(prepared_log_info_df)\n\n    return base_var_info_df, template_info_df, prepared_log_info_df\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.clear_graph","title":"<code>clear_graph(clear_edge_states=True)</code>","text":"<p>Clear the graph and possibly edge states.</p> <p>Parameters:</p> Name Type Description Default <code>clear_edge_states</code> <code>bool</code> <p>Whether to also clear the edge states.</p> <code>True</code> Source code in <code>src/logos/logos.py</code> <pre><code>def clear_graph(self, clear_edge_states: bool = True) -&gt; None:\n    \"\"\"\n    Clear the graph and possibly edge states.\n\n    Parameters:\n        clear_edge_states: Whether to also clear the edge states.\n    \"\"\"\n    self._graph = nx.DiGraph()\n    if clear_edge_states:\n        self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n    if self._eccs:\n        self._eccs.clear_graph(clear_edge_states)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.display_graph","title":"<code>display_graph()</code>","text":"<p>Display the current graph.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def display_graph(self) -&gt; None:\n    \"\"\"\n    Display the current graph.\n    \"\"\"\n    GraphRenderer.display_graph(self._graph, self._prepared_variables)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.save_graph","title":"<code>save_graph(filename)</code>","text":"<p>Save the current graph to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the file to save to.</p> required Source code in <code>src/logos/logos.py</code> <pre><code>def save_graph(self, filename: str) -&gt; None:\n    \"\"\"\n    Save the current graph to a file.\n\n    Parameters:\n        filename: The name of the file to save to.\n    \"\"\"\n    GraphRenderer.save_graph(self._graph, self._prepared_variables, filename)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.accept","title":"<code>accept(src, dst, also_fix, interactive=True)</code>","text":"<p>Mark a causal graph edge as accepted.</p> <p>This will also reject the edge from <code>dst</code> to <code>src</code> and remove any other variables with the same base variable as either <code>src</code> or <code>dst</code> from consideration for the partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>also_fix</code> <code>bool</code> <p>Whether to also fix the edge, for ECCS.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after accepting the edge.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge addition, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def accept(\n    self,\n    src: str,\n    dst: str,\n    also_fix: bool,\n    interactive: bool = True,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark a causal graph edge as accepted.\n\n    This will also reject the edge from `dst` to `src` and remove any other variables with the\n    same base variable as either `src` or `dst` from consideration for the partial causal graph.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        also_fix: Whether to also fix the edge, for ECCS.\n        interactive: Whether to display the graph interactively after accepting the edge.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge addition,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n\n    src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    to_drop = self._edge_states.mark_edge(src_name, dst_name, \"Accepted\")\n    for node in to_drop:\n        if node in self._graph.nodes:\n            self._graph.remove_node(node)\n\n    self._graph.add_node(src_name)\n    self._graph.add_node(dst_name)\n    self._graph.add_edge(src_name, dst_name)\n    if (dst_name, src_name) in self._graph.edges:\n        self._graph.remove_edge(dst_name, src_name)\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n    if self._eccs:\n        self._eccs.remove_edge(dst_name, src_name)\n        self._eccs.add_edge(src_name, dst_name)\n        if also_fix:\n            self._eccs.fix_edge(src_name, dst_name)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        (\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\"\n        ),\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.reject","title":"<code>reject(src, dst, also_ban, interactive=True)</code>","text":"<p>Mark a causal graph edge as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>also_ban</code> <code>bool</code> <p>Whether to also ban the edge, for ECCS.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after rejecting the edge.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejection, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def reject(\n    self,\n    src: str,\n    dst: str,\n    also_ban: bool,\n    interactive: bool = True,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark a causal graph edge as rejected.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        also_ban: Whether to also ban the edge, for ECCS.\n        interactive: Whether to display the graph interactively after rejecting the edge.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejection,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n\n    src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    self._edge_states.mark_edge(src_name, dst_name, \"Rejected\")\n    if self._eccs and also_ban:\n        self._eccs.ban_edge(src_name, dst_name)\n\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        (\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\"\n        ),\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.reject_undecided_incoming","title":"<code>reject_undecided_incoming(dst, also_ban, interactive=True)</code>","text":"<p>Mark all undecided incoming edges to a variable as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>also_ban</code> <code>bool</code> <p>Whether to also ban the edges, for ECCS.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after rejecting the edges.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejections, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def reject_undecided_incoming(\n    self, dst: str, also_ban: bool, interactive: bool = True\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark all undecided incoming edges to a variable as rejected.\n\n    Parameters:\n        dst: The name or tag of the destination variable.\n        also_ban: Whether to also ban the edges, for ECCS.\n        interactive: Whether to display the graph interactively after rejecting the edges.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejections,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    for v in self.prepared_variable_names:\n        if self._edge_states.get_edge_state(v, dst_name) == \"Undecided\":\n            self._edge_states.mark_edge(v, dst_name, \"Rejected\")\n            if self._eccs and also_ban:\n                self._eccs.ban_edge(v, dst_name)\n\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        (\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\"\n        ),\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.reject_undecided_outgoing","title":"<code>reject_undecided_outgoing(src, also_ban, interactive=True)</code>","text":"<p>Mark all undecided outgoing edges from a variable as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>also_ban</code> <code>bool</code> <p>Whether to also ban the edges, for ECCS.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after rejecting the edges.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejections, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def reject_undecided_outgoing(\n    self, src: str, also_ban: bool, interactive: bool = True\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark all undecided outgoing edges from a variable as rejected.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        also_ban: Whether to also ban the edges, for ECCS.\n        interactive: Whether to display the graph interactively after rejecting the edges.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejections,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n    src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n    for v in self.prepared_variable_names:\n        if self._edge_states.get_edge_state(src_name, v) == \"Undecided\":\n            self._edge_states.mark_edge(src_name, v, \"Rejected\")\n            if self._eccs and also_ban:\n                self._eccs.ban_edge(src_name, v)\n\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        (\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\"\n        ),\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.reject_all_prunable_edges","title":"<code>reject_all_prunable_edges(also_ban, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER)</code>","text":"<p>For every prepared variable, reject all incoming edges that start at a variable that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.</p> <p>Parameters:</p> Name Type Description Default <code>also_ban</code> <code>bool</code> <p>Whether to also ban the edges, for ECCS.</p> required <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for Lasso regression.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for Lasso regression.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejections, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def reject_all_prunable_edges(\n    self,\n    also_ban: bool,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    For every prepared variable, reject all incoming edges that start at a variable\n    that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.\n\n    Parameters:\n        also_ban: Whether to also ban the edges, for ECCS.\n        lasso_alpha: The alpha parameter to be used for Lasso regression.\n        lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejections,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n    num_processors = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=num_processors) as pool:\n        all_candidates = pool.starmap(\n            Pruner.prune_with_lasso,\n            tqdm(\n                [\n                    (self._prepared_log, [target], lasso_alpha, lasso_max_iter)\n                    for target in self.prepared_variable_names\n                ],\n                total=self.num_prepared_variables,\n                desc=\"Finding pruned variables...\",\n            ),\n        )\n\n    Printer.printv(all_candidates)\n\n    for candidates, target in zip(all_candidates, self.prepared_variable_names):\n        non_candidates = (\n            set(self._prepared_log.columns) - set(candidates) - set([target])\n        )\n        for nc in non_candidates:\n            self._edge_states.mark_edge(nc, target, \"Rejected\")\n            if self._eccs and also_ban:\n                self._eccs.ban_edge(nc, target)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        GraphRenderer.draw_graph(self._graph, self._prepared_variables),\n    )\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.rank_candidate_causes","title":"<code>rank_candidate_causes(target=None, ignore=None, method=CandidateCauseRankerMethod.LOGOS, prune_candidates=True, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER, model='gpt-4o-mini-2024-07-18', gpt_log_path=None)</code>","text":"<p>Present the user with ranked candidate causes for <code>target</code>. If no <code>target</code> is specified, the most recent suggestion of <code>suggest_next_exploration()</code> is used, if any. If <code>ignore</code> is specified, the variables in <code>ignore</code> are not considered as candidate causes.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>The name or tag of the target variable.</p> <code>None</code> <code>ignore</code> <code>Optional[List[str]]</code> <p>A list of variables to ignore.</p> <code>None</code> <code>method</code> <code>CandidateCauseRankerMethod</code> <p>The method to use for ranking candidate causes.</p> <code>LOGOS</code> <code>prune_candidates</code> <code>bool</code> <p>Whether to prune the candidate causes using Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code>.</p> <code>True</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code> and <code>prune_candidates</code> is True.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for Lasso regression. Only applies if <code>method</code> is <code>CandidateCauseRankerMethod.LOGOS</code> and <code>prune_candidates</code> is True.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>model</code> <code>str</code> <p>The model to use for the langmodel method. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>'gpt-4o-mini-2024-07-18'</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the log file to use for the langmodel method. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>None</code> <p>Returns:     A tuple containing:     (1) A dataframe containing the candidate causes for <code>target</code> and     (2) The time elapsed for exploration, as a string.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def rank_candidate_causes(\n    self,\n    target: Optional[str] = None,\n    ignore: Optional[List[str]] = None,\n    method: CandidateCauseRankerMethod = CandidateCauseRankerMethod.LOGOS,\n    prune_candidates: bool = True,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    model: str = \"gpt-4o-mini-2024-07-18\",\n    gpt_log_path: Optional[str] = None,\n) -&gt; Tuple[pd.DataFrame, str]:\n    \"\"\"\n    Present the user with ranked candidate causes for `target`. If no `target`\n    is specified, the most recent suggestion of `suggest_next_exploration()` is used, if any.\n    If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n    Parameters:\n        target: The name or tag of the target variable.\n        ignore: A list of variables to ignore.\n        method: The method to use for ranking candidate causes.\n        prune_candidates: Whether to prune the candidate causes using Lasso regression. Only\n            applies if `method` is `CandidateCauseRankerMethod.LOGOS`.\n        lasso_alpha: The alpha parameter to be used for Lasso regression. Only applies if\n            `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n        lasso_max_iter: The maximum number of iterations to be used for Lasso regression. Only\n            applies if `method` is `CandidateCauseRankerMethod.LOGOS` and `prune_candidates` is True.\n        model: The model to use for the langmodel method. Only applies if the method is\n            `CandidateCauseRankerMethod.LANGMODEL`.\n        gpt_log_path: The path to the log file to use for the langmodel method. Only applies if\n            the method is `CandidateCauseRankerMethod.LANGMODEL`.\n    Returns:\n        A tuple containing:\n        (1) A dataframe containing the candidate causes for `target` and\n        (2) The time elapsed for exploration, as a string.\n    \"\"\"\n\n    start_time = datetime.now()\n\n    # Handle the case where the user has not specified a target.\n    if target is None and self._next_exploration is None:\n        print(\"No target specified.\")\n        return pd.DataFrame(columns=CandidateCauseRanker.COLUMN_ORDER), \"\"\n    elif target is None:\n        target = self._next_exploration\n\n    # If the user provided the target as a tag, retrieve its name\n    target = TagUtils.name_of(self._prepared_variables, target, \"prepared\")\n\n    # Use the specified method to rank candidate causes\n    result_df, pruned = CandidateCauseRanker.rank(\n        self.prepared_log,\n        self.prepared_variables,\n        target,\n        ignore,\n        method,\n        prune_candidates,\n        lasso_alpha,\n        lasso_max_iter,\n        model,\n        (\n            gpt_log_path\n            if (gpt_log_path is not None)\n            else os.path.join(\n                self._workdir,\n                f\"ranker-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\",\n            )\n        ),\n    )\n\n    # Mark the edges rejected by the pruning step, if any.\n    for var in pruned:\n        self._edge_states.mark_edge(var, target, \"Rejected\")\n\n    # Add fields to the returned dataframe\n    result_df[\"Candidate-&gt;Target Edge Status\"] = result_df[\"Candidate\"].apply(\n        lambda x: self._edge_states.get_edge_state(x, target)\n    )\n    result_df[\"Target-&gt;Candidate Edge Status\"] = result_df[\"Candidate\"].apply(\n        lambda x: self._edge_states.get_edge_state(target, x)\n    )\n\n    ret_val = result_df[CandidateCauseRanker.COLUMN_ORDER]\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(f\"Candidate cause exploration complete in {elapsed} seconds!\")\n\n    return ret_val, elapsed\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_causal_graph_refinement_suggestion","title":"<code>get_causal_graph_refinement_suggestion(method=InteractiveCausalGraphRefinerMethod.LOGOS, treatment=None, outcome=None, model='gpt-4o-mini-2024-07-18', gpt_log_path=None)</code>","text":"<p>Present the user with an edge, the presence and direction of which they should assess.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>InteractiveCausalGraphRefinerMethod</code> <p>The method to use for producing a causal graph refinement suggestion.</p> <code>LOGOS</code> <code>treatment</code> <code>Optional[str]</code> <p>The name or tag of the treatment variable. Only applies if the method is <code>InteractiveCausalGraphRefinerMethod.LOGOS</code>.</p> <code>None</code> <code>outcome</code> <code>Optional[str]</code> <p>The name or tag of the outcome variable. Only applies if the method is <code>InteractiveCausalGraphRefinerMethod.LOGOS</code>.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for the langmodel method. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>'gpt-4o-mini-2024-07-18'</code> <code>gpt_log_path</code> <code>Optional[str]</code> <p>The path to the log file to use for the langmodel method. Only applies if the method is <code>CandidateCauseRankerMethod.LANGMODEL</code>.</p> <code>None</code> <p>Returns:     A tuple containing:     (1) The edge to assess, as an Edge object, and     (2) The time elapsed for generating the suggestion, as a string.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_causal_graph_refinement_suggestion(\n    self,\n    method: InteractiveCausalGraphRefinerMethod = InteractiveCausalGraphRefinerMethod.LOGOS,\n    treatment: Optional[str] = None,\n    outcome: Optional[str] = None,\n    model: str = \"gpt-4o-mini-2024-07-18\",\n    gpt_log_path: Optional[str] = None,\n) -&gt; Tuple[Edge, str]:\n    \"\"\"\n    Present the user with an edge, the presence and direction of which they should assess.\n\n    Parameters:\n        method: The method to use for producing a causal graph refinement suggestion.\n        treatment: The name or tag of the treatment variable. Only applies if the method is\n            `InteractiveCausalGraphRefinerMethod.LOGOS`.\n        outcome: The name or tag of the outcome variable. Only applies if the method is\n            `InteractiveCausalGraphRefinerMethod.LOGOS`.\n        model: The model to use for the langmodel method. Only applies if the method is\n            `CandidateCauseRankerMethod.LANGMODEL`.\n        gpt_log_path: The path to the log file to use for the langmodel method. Only applies if\n            the method is `CandidateCauseRankerMethod.LANGMODEL`.\n    Returns:\n        A tuple containing:\n        (1) The edge to assess, as an Edge object, and\n        (2) The time elapsed for generating the suggestion, as a string.\n    \"\"\"\n\n    start_time = datetime.now()\n\n    treatment_name = TagUtils.name_of(\n        self._prepared_variables, treatment, \"prepared\"\n    )\n    outcome_name = TagUtils.name_of(self._prepared_variables, outcome, \"prepared\")\n\n    edge = InteractiveCausalGraphRefiner.get_suggestion(\n        self.prepared_log,\n        method,\n        self._eccs,\n        treatment_name,\n        outcome_name,\n        self._graph,\n        model,\n        (\n            gpt_log_path\n            if (gpt_log_path is not None)\n            else os.path.join(\n                self._workdir,\n                f\"refiner-gpt-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\",\n            )\n        ),\n        self.prepared_variables\n    )\n\n    edge_tags = None\n    if edge:\n        edge_tags = tuple(\n            TagUtils.tag_of(self._prepared_variables, x, \"prepared\") for x in edge\n        )\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(f\"Candidate cause exploration complete in {elapsed} seconds!\")\n\n    return edge_tags, elapsed\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.suggest_next_exploration","title":"<code>suggest_next_exploration()</code>","text":"<p>Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.</p> <p>If all edges are decided, return None.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The name of the variable to explore next.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def suggest_next_exploration(self) -&gt; Optional[str]:\n    \"\"\"\n    Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph\n    that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable\n    with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.\n\n    If all edges are decided, return None.\n\n    Returns:\n        The name of the variable to explore next.\n    \"\"\"\n\n    # Try to find a suggestion from the partial causal graph.\n    node_names = list(self._graph.nodes)\n    graph_var_indices = [self._edge_states.idx(x) for x in node_names]\n    graph_var_incoming_edge_states = self._edge_states.m[:, graph_var_indices]\n    undecided_edges_per_col = (\n        np.sum(graph_var_incoming_edge_states == 0, axis=0)\n        if len(graph_var_incoming_edge_states) &gt; 0\n        else []\n    )\n    max_undecided = (\n        np.max(undecided_edges_per_col) if len(undecided_edges_per_col) &gt; 0 else 0\n    )\n\n    if max_undecided &gt; 0:\n        max_undecided_idx = np.argmax(undecided_edges_per_col)\n        self._next_exploration = node_names[max_undecided_idx]\n        return self._next_exploration\n\n    # If no suggestion was found, try to find a suggestion from the entire collection of prepared variables.\n    undecided_edges_per_col = np.sum(self._edge_states.m == 0, axis=0)\n    max_undecided = np.max(undecided_edges_per_col)\n\n    if max_undecided &gt; 0:\n        max_undecided_idx = np.argmax(undecided_edges_per_col)\n        self._next_exploration = self._prepared_variables.loc[\n            max_undecided_idx, \"Name\"\n        ]\n        return self._next_exploration\n\n    # If no suggestion was found, return None.\n    self._next_exploration = None\n    return None\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.discover_graph","title":"<code>discover_graph(method='hill_climb', max_cond_vars=3, model='gpt-3.5-turbo')</code>","text":"<p>Discover a causal graph based on the prepared table automatically.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".</p> <code>'hill_climb'</code> <code>max_cond_vars</code> <code>int</code> <p>The maximum number of conditioning variables to be used for PC.</p> <code>3</code> <code>model</code> <code>str</code> <p>The model to be used for GPT-based graph discovery.</p> <code>'gpt-3.5-turbo'</code> Source code in <code>src/logos/logos.py</code> <pre><code>def discover_graph(\n    self,\n    method: str = \"hill_climb\",\n    max_cond_vars: int = 3,\n    model: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Discover a causal graph based on the prepared table automatically.\n\n    Parameters:\n        method: The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".\n        max_cond_vars: The maximum number of conditioning variables to be used for PC.\n        model: The model to be used for GPT-based graph discovery.\n\n    \"\"\"\n\n    if method == \"PC\":\n        self._graph = CausalDiscoverer.pc(\n            self._prepared_log, max_cond_vars=max_cond_vars\n        )\n    elif method == \"hill_climb\":\n        self._graph = CausalDiscoverer.hill_climb(self._prepared_log)\n    elif method == \"exhaustive\":\n        self._graph = CausalDiscoverer.exhaustive(self._prepared_log)\n    elif method == \"GPT\":\n        self._graph = CausalDiscoverer.gpt(self._prepared_log, model=model)\n    else:\n        raise ValueError(f\"Invalid graph discovery method {method}\")\n\n    self._edge_states.clear_and_set_from_graph(self._graph)\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_adjusted_ate","title":"<code>get_adjusted_ate(treatment, outcome, confounder=None)</code>","text":"<p>Calculate the adjusted ATE of <code>treatment</code> on <code>outcome</code>, given the current partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>confounder</code> <code>Optional[str]</code> <p>The name or tag of a confounder variable. If specified, overrides the current partial causal graph in favor of a three-node graph with <code>treatment</code>, <code>outcome</code> and <code>confounder</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The adjusted ATE of <code>treatment</code> on <code>outcome</code>.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_adjusted_ate(\n    self,\n    treatment: str,\n    outcome: str,\n    confounder: Optional[str] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate the adjusted ATE of `treatment` on `outcome`, given the current partial causal graph.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n            causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n\n    Returns:\n        The adjusted ATE of `treatment` on `outcome`.\n    \"\"\"\n    return ATECalculator.get_ate_and_confidence(\n        self.prepared_log,\n        self.prepared_variables,\n        treatment,\n        outcome,\n        confounder,\n        graph=self._graph,\n        calculate_p_value=False,\n        calculate_std_error=False,\n    )[\"ATE\"]\n</code></pre>"},{"location":"reference/logos/logos/#logos.logos.LOGos.get_unadjusted_ate","title":"<code>get_unadjusted_ate(treatment, outcome)</code>","text":"<p>Calculate the unadjusted ATE of <code>treatment</code> on <code>outcome</code>, ignoring the current partial causal graph in favor of a two-node graph with just <code>treatment</code> and <code>outcome</code>.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The unadjusted ATE of <code>treatment</code> on <code>outcome</code>.</p> Source code in <code>src/logos/logos.py</code> <pre><code>def get_unadjusted_ate(\n    self,\n    treatment: str,\n    outcome: str,\n) -&gt; float:\n    \"\"\"\n    Calculate the unadjusted ATE of `treatment` on `outcome`, ignoring the current partial causal graph\n    in favor of a two-node graph with just `treatment` and `outcome`.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n\n    Returns:\n        The unadjusted ATE of `treatment` on `outcome`.\n    \"\"\"\n    return ATECalculator.get_ate_and_confidence(\n        self.prepared_log,\n        self.prepared_variables,\n        treatment,\n        outcome,\n        calculate_p_value=False,\n        calculate_std_error=False,\n    )[\"ATE\"]\n</code></pre>"},{"location":"reference/logos/pickler/","title":"Pickler","text":""},{"location":"reference/logos/pickler/#logos.pickler.Pickler","title":"<code>Pickler</code>","text":"<p>A class for loading and dumping dataframes to and from pkl files.</p> Source code in <code>src/logos/pickler.py</code> <pre><code>class Pickler:\n    \"\"\"\n    A class for loading and dumping dataframes to and from pkl files.\n    \"\"\"\n\n    @staticmethod\n    def load(filename: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads a dataframe from a pkl file.\n\n        Parameters:\n            filename: The name of the pkl file.\n\n        Returns:\n            The dataframe loaded from the pkl file.\n        \"\"\"\n        df = pd.DataFrame()\n        with open(filename, \"rb\") as f:\n            df = pickle.load(f)\n        return df\n\n    @staticmethod\n    def dump(df: pd.DataFrame, filename: str) -&gt; None:\n        \"\"\"\n        Dumps a dataframe to a pkl file.\n\n        Parameters:\n            df: The dataframe to be dumped.\n            filename: The name of the pkl file.\n        \"\"\"\n\n        if \"/\" in filename:\n            path = filename[: filename.rindex(\"/\")]\n            os.makedirs(path, exist_ok=True)\n\n        with open(filename, \"wb+\") as f:\n            pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"reference/logos/pickler/#logos.pickler.Pickler.load","title":"<code>load(filename)</code>  <code>staticmethod</code>","text":"<p>Loads a dataframe from a pkl file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the pkl file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe loaded from the pkl file.</p> Source code in <code>src/logos/pickler.py</code> <pre><code>@staticmethod\ndef load(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads a dataframe from a pkl file.\n\n    Parameters:\n        filename: The name of the pkl file.\n\n    Returns:\n        The dataframe loaded from the pkl file.\n    \"\"\"\n    df = pd.DataFrame()\n    with open(filename, \"rb\") as f:\n        df = pickle.load(f)\n    return df\n</code></pre>"},{"location":"reference/logos/pickler/#logos.pickler.Pickler.dump","title":"<code>dump(df, filename)</code>  <code>staticmethod</code>","text":"<p>Dumps a dataframe to a pkl file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be dumped.</p> required <code>filename</code> <code>str</code> <p>The name of the pkl file.</p> required Source code in <code>src/logos/pickler.py</code> <pre><code>@staticmethod\ndef dump(df: pd.DataFrame, filename: str) -&gt; None:\n    \"\"\"\n    Dumps a dataframe to a pkl file.\n\n    Parameters:\n        df: The dataframe to be dumped.\n        filename: The name of the pkl file.\n    \"\"\"\n\n    if \"/\" in filename:\n        path = filename[: filename.rindex(\"/\")]\n        os.makedirs(path, exist_ok=True)\n\n    with open(filename, \"wb+\") as f:\n        pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"reference/logos/printer/","title":"Printer","text":""},{"location":"reference/logos/printer/#logos.printer.Printer","title":"<code>Printer</code>","text":"<p>A class for controlling message printing.</p> Source code in <code>src/logos/printer.py</code> <pre><code>class Printer:\n    \"\"\"\n    A class for controlling message printing.\n    \"\"\"\n\n    \"\"\"\n    A flag indicating whether or not to print messages to the console.\n    \"\"\"\n    SAWMILL_VERBOSE = False\n\n    @classmethod\n    def printv(self, msg: Any) -&gt; None:\n        \"\"\"\n        Prints a message to the console if in verbose mode.\n\n        Parameters:\n            msg: The message to be printed.\n        \"\"\"\n        if Printer.SAWMILL_VERBOSE:\n            print(msg)\n\n    @classmethod\n    def set_verbose(self, val: bool) -&gt; None:\n        \"\"\"\n        Sets the verbosity of the printer.\n\n        Parameters:\n            val: The new verbosity value.\n        \"\"\"\n        Printer.SAWMILL_VERBOSE = val\n\n    @staticmethod\n    def set_warnings_to(self, value: str):\n        \"\"\"\n        Set selected warnings to `value`.\n\n        Parameters:\n            value: The value to set the warnings to.\n        \"\"\"\n        warnings.filterwarnings(\n            value, category=RuntimeWarning, message=\"mean of empty slice\"\n        )\n        warnings.filterwarnings(\n            value,\n            category=RuntimeWarning,\n            message=\"invalid value encountered in scalar divide\",\n        )\n</code></pre>"},{"location":"reference/logos/printer/#logos.printer.Printer.printv","title":"<code>printv(msg)</code>  <code>classmethod</code>","text":"<p>Prints a message to the console if in verbose mode.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>The message to be printed.</p> required Source code in <code>src/logos/printer.py</code> <pre><code>@classmethod\ndef printv(self, msg: Any) -&gt; None:\n    \"\"\"\n    Prints a message to the console if in verbose mode.\n\n    Parameters:\n        msg: The message to be printed.\n    \"\"\"\n    if Printer.SAWMILL_VERBOSE:\n        print(msg)\n</code></pre>"},{"location":"reference/logos/printer/#logos.printer.Printer.set_verbose","title":"<code>set_verbose(val)</code>  <code>classmethod</code>","text":"<p>Sets the verbosity of the printer.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>bool</code> <p>The new verbosity value.</p> required Source code in <code>src/logos/printer.py</code> <pre><code>@classmethod\ndef set_verbose(self, val: bool) -&gt; None:\n    \"\"\"\n    Sets the verbosity of the printer.\n\n    Parameters:\n        val: The new verbosity value.\n    \"\"\"\n    Printer.SAWMILL_VERBOSE = val\n</code></pre>"},{"location":"reference/logos/printer/#logos.printer.Printer.set_warnings_to","title":"<code>set_warnings_to(value)</code>  <code>staticmethod</code>","text":"<p>Set selected warnings to <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to set the warnings to.</p> required Source code in <code>src/logos/printer.py</code> <pre><code>@staticmethod\ndef set_warnings_to(self, value: str):\n    \"\"\"\n    Set selected warnings to `value`.\n\n    Parameters:\n        value: The value to set the warnings to.\n    \"\"\"\n    warnings.filterwarnings(\n        value, category=RuntimeWarning, message=\"mean of empty slice\"\n    )\n    warnings.filterwarnings(\n        value,\n        category=RuntimeWarning,\n        message=\"invalid value encountered in scalar divide\",\n    )\n</code></pre>"},{"location":"reference/logos/pruner/","title":"Pruner","text":""},{"location":"reference/logos/pruner/#logos.pruner.Pruner","title":"<code>Pruner</code>","text":"Source code in <code>src/logos/pruner.py</code> <pre><code>class Pruner:\n    LASSO_DEFAULT_ALPHA = 0.1\n    LASSO_DEFAULT_MAX_ITER = 100000\n\n    \"\"\"\n    A collection of pruning functions for prepared variables,\n    used for pruning and candidate suggestion.\n    \"\"\"\n\n    @staticmethod\n    def prune_with_lasso(\n        data: pd.DataFrame,\n        outcome_cols: list[str],\n        alpha: float = LASSO_DEFAULT_ALPHA,\n        max_iter: int = LASSO_DEFAULT_MAX_ITER,\n        top_n: int = 0,\n        ignore: Optional[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Prune variables using Lasso regression.\n\n        Parameters:\n            data: The dataframe containing the data.\n            outcome_cols: The names of the target variables.\n            alpha: The Lasso regularization parameter.\n            max_iter: The maximum number of iterations for Lasso.\n            top_n: The number of variables to return. If 0, return all variables.\n            ignore: The names of the variables to ignore.\n\n        Returns:\n            The names of the variables that Lasso identified as impactful, optionally\n            limited to the top `n` variables by absolute coefficient.\n        \"\"\"\n\n        # TODO: do this properly wherever this is called\n        outcome_col = outcome_cols[0]\n\n        # Separate the target variable and predictor variables.\n        # Optionally, do not consider variables already in the graph.\n        y = data[outcome_cols]\n        drop_cols = [] if ignore is None else ignore\n        to_ignore = outcome_cols\n        drop_cols.extend(to_ignore)\n\n        # Do not consider variables with the same base variable as an ignored variable.\n        for v in to_ignore:\n            vp = PreparedVariableName(v)\n            if vp.base_var() != \"TemplateId\":\n                drop_cols.extend([c for c in data.columns if vp.base_var() in c])\n        drop_cols = list(set(drop_cols))\n\n        # Iterate until multiple prepared variables with the same base variable are eliminated.\n        done = False\n\n        while not done:\n            Printer.printv(f\"Variables that Lasso will ignore: {drop_cols}\")\n            X = data.drop(drop_cols, axis=1)\n            X_cols = X.columns\n            if X.empty:\n                return []\n\n            scaler = StandardScaler()\n            X = scaler.fit_transform(X)\n\n            # Fit a Lasso model to the data\n            lasso = Lasso(alpha=alpha, max_iter=max_iter)\n            lasso.fit(X, y)\n            Printer.printv(f\"Lasso coefficients : {lasso.coef_}\")\n            Printer.printv(f\"Scale: {scaler.scale_}\")\n            final_coefs = lasso.coef_ / scaler.scale_\n            abs_coefs = np.abs(final_coefs)\n            Printer.printv(f\"Lasso coefficients unscaled: {final_coefs}\")\n\n            # Mask for nonzero elements\n            nonzero_mask = final_coefs != 0\n\n            # Mask for top n largest elements by absolute value\n            # Create an array of False values with the same shape as the coefficients\n            top_n_mask = [False] * len(final_coefs)\n            for i in np.argsort(abs_coefs)[-top_n:]:\n                top_n_mask[i] = True\n\n            # Retrieve columns based on conditions above\n            selected_names = list(X_cols[nonzero_mask &amp; top_n_mask])\n\n            # Only keep one aggregate per variable\n            d = set()\n            done = True\n            for var in selected_names:\n                base_var = PreparedVariableName(var).base_var()\n                if base_var in d:\n                    drop_cols.append(var)\n                    done = False\n                else:\n                    d.add(base_var)\n\n        Printer.printv(\"Lasso identified the following impactful variables:\")\n        Printer.printv(selected_names)\n\n        return selected_names\n\n    @staticmethod\n    def prune_with_triangle(\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        treatment_col: str,\n        outcome_col: str,\n        work_dir: str,\n        top_n: int = 0,\n        force: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Prune variables using triangle method.\n\n        Parameters:\n            data: The dataframe containing the data.\n            vars: The dataframe containing the variables.\n            treatment_col: The name of the treatment variable.\n            outcome_col: The name of the outcome variable.\n            work_dir: The directory to store intermediate files in.\n            top_n: The number of variables to return. If 0, return all variables.\n            force: Whether to force recalculation of the triangle method.\n\n        Returns:\n            The names of the variables that triangle method identified as impactful, optionally\n            limited to the top `n` variables.\n        \"\"\"\n\n        # Check whether we can use pre-calculated results\n        filename = os.path.join(\n            work_dir, f\"pickles/triangle_dags/{treatment_col}_{outcome_col}.pkl\"\n        )\n        if os.path.isfile(filename) and not force:\n            df = pickle.load(open(filename, \"rb\"))\n            print(\"Found pickled file\")\n            return list(df.index[:top_n].values)\n\n        Printer.printv(\"Starting to prune using triangle method\")\n        max_diffs = {}\n        base_ate = ATECalculator.get_ate_and_confidence(\n            data, vars, treatment_col, outcome_col, calculate_std_error=False\n        )[\"ATE\"]\n\n        for var in tqdm(data.columns, \"Processing triangle dags\"):\n            if var == treatment_col or var == outcome_col:\n                continue\n\n            # Construct the graphs to consider\n            graphs = []\n            # Second cause\n            graphs.append(\n                nx.DiGraph([(treatment_col, outcome_col), (var, outcome_col)])\n            )\n            # Confounder\n            graphs.append(\n                nx.DiGraph(\n                    [\n                        (treatment_col, outcome_col),\n                        (var, treatment_col),\n                        (var, outcome_col),\n                    ]\n                )\n            )\n            # Mediator with direct path\n            graphs.append(\n                nx.DiGraph(\n                    [\n                        (treatment_col, outcome_col),\n                        (treatment_col, var),\n                        (var, outcome_col),\n                    ]\n                )\n            )\n            # Mediator without direct path\n            graphs.append(nx.DiGraph([(treatment_col, var), (var, outcome_col)]))\n\n            # Calculate the corrsponding ATEs\n            ates = [base_ate]\n            for G in graphs:\n                try:\n                    ates.append(\n                        ATECalculator.get_ate_and_confidence(\n                            data,\n                            vars,\n                            treatment_col,\n                            outcome_col,\n                            graph=G,\n                            calculate_std_error=False,\n                        )[\"ATE\"]\n                    )\n                except:\n                    pass\n            max_diffs[var] = max(ates) - min(ates)\n        max_diffs = max_diffs\n        df = pd.DataFrame.from_dict(max_diffs, orient=\"index\", columns=[\"max_diff\"])\n        df = df.sort_values(by=\"max_diff\", ascending=False)\n\n        Pickler.dump(df, filename)\n\n        return list(df.index[:top_n].values)\n</code></pre>"},{"location":"reference/logos/pruner/#logos.pruner.Pruner.LASSO_DEFAULT_MAX_ITER","title":"<code>LASSO_DEFAULT_MAX_ITER = 100000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A collection of pruning functions for prepared variables, used for pruning and candidate suggestion.</p>"},{"location":"reference/logos/pruner/#logos.pruner.Pruner.prune_with_lasso","title":"<code>prune_with_lasso(data, outcome_cols, alpha=LASSO_DEFAULT_ALPHA, max_iter=LASSO_DEFAULT_MAX_ITER, top_n=0, ignore=None)</code>  <code>staticmethod</code>","text":"<p>Prune variables using Lasso regression.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>outcome_cols</code> <code>list[str]</code> <p>The names of the target variables.</p> required <code>alpha</code> <code>float</code> <p>The Lasso regularization parameter.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>max_iter</code> <code>int</code> <p>The maximum number of iterations for Lasso.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>top_n</code> <code>int</code> <p>The number of variables to return. If 0, return all variables.</p> <code>0</code> <code>ignore</code> <code>Optional[list[str]]</code> <p>The names of the variables to ignore.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The names of the variables that Lasso identified as impactful, optionally</p> <code>list[str]</code> <p>limited to the top <code>n</code> variables by absolute coefficient.</p> Source code in <code>src/logos/pruner.py</code> <pre><code>@staticmethod\ndef prune_with_lasso(\n    data: pd.DataFrame,\n    outcome_cols: list[str],\n    alpha: float = LASSO_DEFAULT_ALPHA,\n    max_iter: int = LASSO_DEFAULT_MAX_ITER,\n    top_n: int = 0,\n    ignore: Optional[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Prune variables using Lasso regression.\n\n    Parameters:\n        data: The dataframe containing the data.\n        outcome_cols: The names of the target variables.\n        alpha: The Lasso regularization parameter.\n        max_iter: The maximum number of iterations for Lasso.\n        top_n: The number of variables to return. If 0, return all variables.\n        ignore: The names of the variables to ignore.\n\n    Returns:\n        The names of the variables that Lasso identified as impactful, optionally\n        limited to the top `n` variables by absolute coefficient.\n    \"\"\"\n\n    # TODO: do this properly wherever this is called\n    outcome_col = outcome_cols[0]\n\n    # Separate the target variable and predictor variables.\n    # Optionally, do not consider variables already in the graph.\n    y = data[outcome_cols]\n    drop_cols = [] if ignore is None else ignore\n    to_ignore = outcome_cols\n    drop_cols.extend(to_ignore)\n\n    # Do not consider variables with the same base variable as an ignored variable.\n    for v in to_ignore:\n        vp = PreparedVariableName(v)\n        if vp.base_var() != \"TemplateId\":\n            drop_cols.extend([c for c in data.columns if vp.base_var() in c])\n    drop_cols = list(set(drop_cols))\n\n    # Iterate until multiple prepared variables with the same base variable are eliminated.\n    done = False\n\n    while not done:\n        Printer.printv(f\"Variables that Lasso will ignore: {drop_cols}\")\n        X = data.drop(drop_cols, axis=1)\n        X_cols = X.columns\n        if X.empty:\n            return []\n\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n        # Fit a Lasso model to the data\n        lasso = Lasso(alpha=alpha, max_iter=max_iter)\n        lasso.fit(X, y)\n        Printer.printv(f\"Lasso coefficients : {lasso.coef_}\")\n        Printer.printv(f\"Scale: {scaler.scale_}\")\n        final_coefs = lasso.coef_ / scaler.scale_\n        abs_coefs = np.abs(final_coefs)\n        Printer.printv(f\"Lasso coefficients unscaled: {final_coefs}\")\n\n        # Mask for nonzero elements\n        nonzero_mask = final_coefs != 0\n\n        # Mask for top n largest elements by absolute value\n        # Create an array of False values with the same shape as the coefficients\n        top_n_mask = [False] * len(final_coefs)\n        for i in np.argsort(abs_coefs)[-top_n:]:\n            top_n_mask[i] = True\n\n        # Retrieve columns based on conditions above\n        selected_names = list(X_cols[nonzero_mask &amp; top_n_mask])\n\n        # Only keep one aggregate per variable\n        d = set()\n        done = True\n        for var in selected_names:\n            base_var = PreparedVariableName(var).base_var()\n            if base_var in d:\n                drop_cols.append(var)\n                done = False\n            else:\n                d.add(base_var)\n\n    Printer.printv(\"Lasso identified the following impactful variables:\")\n    Printer.printv(selected_names)\n\n    return selected_names\n</code></pre>"},{"location":"reference/logos/pruner/#logos.pruner.Pruner.prune_with_triangle","title":"<code>prune_with_triangle(data, vars, treatment_col, outcome_col, work_dir, top_n=0, force=False)</code>  <code>staticmethod</code>","text":"<p>Prune variables using triangle method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>vars</code> <code>DataFrame</code> <p>The dataframe containing the variables.</p> required <code>treatment_col</code> <code>str</code> <p>The name of the treatment variable.</p> required <code>outcome_col</code> <code>str</code> <p>The name of the outcome variable.</p> required <code>work_dir</code> <code>str</code> <p>The directory to store intermediate files in.</p> required <code>top_n</code> <code>int</code> <p>The number of variables to return. If 0, return all variables.</p> <code>0</code> <code>force</code> <code>bool</code> <p>Whether to force recalculation of the triangle method.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The names of the variables that triangle method identified as impactful, optionally</p> <code>list[str]</code> <p>limited to the top <code>n</code> variables.</p> Source code in <code>src/logos/pruner.py</code> <pre><code>@staticmethod\ndef prune_with_triangle(\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    treatment_col: str,\n    outcome_col: str,\n    work_dir: str,\n    top_n: int = 0,\n    force: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Prune variables using triangle method.\n\n    Parameters:\n        data: The dataframe containing the data.\n        vars: The dataframe containing the variables.\n        treatment_col: The name of the treatment variable.\n        outcome_col: The name of the outcome variable.\n        work_dir: The directory to store intermediate files in.\n        top_n: The number of variables to return. If 0, return all variables.\n        force: Whether to force recalculation of the triangle method.\n\n    Returns:\n        The names of the variables that triangle method identified as impactful, optionally\n        limited to the top `n` variables.\n    \"\"\"\n\n    # Check whether we can use pre-calculated results\n    filename = os.path.join(\n        work_dir, f\"pickles/triangle_dags/{treatment_col}_{outcome_col}.pkl\"\n    )\n    if os.path.isfile(filename) and not force:\n        df = pickle.load(open(filename, \"rb\"))\n        print(\"Found pickled file\")\n        return list(df.index[:top_n].values)\n\n    Printer.printv(\"Starting to prune using triangle method\")\n    max_diffs = {}\n    base_ate = ATECalculator.get_ate_and_confidence(\n        data, vars, treatment_col, outcome_col, calculate_std_error=False\n    )[\"ATE\"]\n\n    for var in tqdm(data.columns, \"Processing triangle dags\"):\n        if var == treatment_col or var == outcome_col:\n            continue\n\n        # Construct the graphs to consider\n        graphs = []\n        # Second cause\n        graphs.append(\n            nx.DiGraph([(treatment_col, outcome_col), (var, outcome_col)])\n        )\n        # Confounder\n        graphs.append(\n            nx.DiGraph(\n                [\n                    (treatment_col, outcome_col),\n                    (var, treatment_col),\n                    (var, outcome_col),\n                ]\n            )\n        )\n        # Mediator with direct path\n        graphs.append(\n            nx.DiGraph(\n                [\n                    (treatment_col, outcome_col),\n                    (treatment_col, var),\n                    (var, outcome_col),\n                ]\n            )\n        )\n        # Mediator without direct path\n        graphs.append(nx.DiGraph([(treatment_col, var), (var, outcome_col)]))\n\n        # Calculate the corrsponding ATEs\n        ates = [base_ate]\n        for G in graphs:\n            try:\n                ates.append(\n                    ATECalculator.get_ate_and_confidence(\n                        data,\n                        vars,\n                        treatment_col,\n                        outcome_col,\n                        graph=G,\n                        calculate_std_error=False,\n                    )[\"ATE\"]\n                )\n            except:\n                pass\n        max_diffs[var] = max(ates) - min(ates)\n    max_diffs = max_diffs\n    df = pd.DataFrame.from_dict(max_diffs, orient=\"index\", columns=[\"max_diff\"])\n    df = df.sort_values(by=\"max_diff\", ascending=False)\n\n    Pickler.dump(df, filename)\n\n    return list(df.index[:top_n].values)\n</code></pre>"},{"location":"reference/logos/regression/","title":"Regression","text":""},{"location":"reference/logos/regression/#logos.regression.Regression","title":"<code>Regression</code>","text":"<p>A collection of regression-related functions.</p> Source code in <code>src/logos/regression.py</code> <pre><code>class Regression:\n    \"\"\"\n    A collection of regression-related functions.\n    \"\"\"\n\n    @staticmethod\n    def ols(X_name: str, X_data: pd.Series, Y_data: pd.Series) -&gt; dict:\n        \"\"\"\n        Calculate the slope and p-value of a linear regression of `X` on `Y`.\n\n        Parameters:\n            X_name: The name of the predictor variable.\n            X_data: The data for the predictor variable.\n            Y_data: The data for the target variable.\n\n        Returns:\n            A dictionary containing the slope and p-value of the regression. If\n            there is no slope parameter because X_data does not vary enough,\n            the slope and p-value will be None.\n        \"\"\"\n        X_data = sm.add_constant(X_data)\n        model = sm.OLS(Y_data, X_data).fit()\n        slope = None\n        p_value = None\n        if len(model.params) &gt; 1:\n            slope = model.params.iloc[1]\n            p_value = model.pvalues.iloc[1]\n        return {\n            \"Candidate\": X_name,\n            \"Slope\": slope,\n            \"P-value\": p_value,\n        }\n\n    @staticmethod\n    def get_normalized_copy(data: pd.DataFrame) -&gt; tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Return a normalized copy of the input data, with zero mean\n        and unit variance.\n\n        Parameters:\n            data: The data to normalize.\n\n        Returns:\n            A normalized copy of the input data.\n            The original standard deviations of the columns of the input data.\n        \"\"\"\n        data = data.copy(deep=True)\n        stdevs = data.std()\n\n        for column in data.columns:\n            if stdevs[column] == 0:\n                data.loc[:, column] = 0\n            else:\n                data.loc[:, column] = (data[column] - data[column].mean()) / stdevs[\n                    column\n                ]\n        return data, stdevs\n\n    @staticmethod\n    def multi_ols(\n        X_names: list[str], X_data: pd.DataFrame, Y_data: pd.Series\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the slopes and p-values of a multivariate linear regression\n        of the variables in `X` on `Y`. Normalize each column to zero mean and\n        unit variance before running the regression. Return both the normalized\n        and unnormalized slopes.\n\n        Parameters:\n            X_names: The names of the predictor variables.\n            X_data: The data for the predictor variables.\n            Y_data: The data for the target variable.\n\n        Returns:\n            A dataframe with the names, slopes, and p-values of the regressions.\n        \"\"\"\n        X_data, stdevs = Regression.get_normalized_copy(X_data)\n\n        X_data = sm.add_constant(X_data)\n        model = sm.OLS(Y_data, X_data).fit()\n\n        # Get the coefficients and p-values, ignoring the constant\n        coefficients = model.params.iloc[1:]\n        p_values = model.pvalues.iloc[1:]\n\n        # Unnormalize the slopes\n        coefficients_unnormalized = coefficients.copy()\n        for coeff in coefficients_unnormalized.index:\n            coefficients_unnormalized[coeff] = (\n                coefficients[coeff] / stdevs[coeff] if stdevs[coeff] != 0 else 0\n            )\n\n        return pd.DataFrame(\n            {\n                \"Candidate\": coefficients.index,\n                \"Slope\": coefficients_unnormalized.values,\n                \"P-value\": p_values.values,\n                \"Normalized Slope\": coefficients.values,\n                \"Absolute Normalized Slope\": coefficients.abs().values,\n            }\n        )\n</code></pre>"},{"location":"reference/logos/regression/#logos.regression.Regression.ols","title":"<code>ols(X_name, X_data, Y_data)</code>  <code>staticmethod</code>","text":"<p>Calculate the slope and p-value of a linear regression of <code>X</code> on <code>Y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_name</code> <code>str</code> <p>The name of the predictor variable.</p> required <code>X_data</code> <code>Series</code> <p>The data for the predictor variable.</p> required <code>Y_data</code> <code>Series</code> <p>The data for the target variable.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the slope and p-value of the regression. If</p> <code>dict</code> <p>there is no slope parameter because X_data does not vary enough,</p> <code>dict</code> <p>the slope and p-value will be None.</p> Source code in <code>src/logos/regression.py</code> <pre><code>@staticmethod\ndef ols(X_name: str, X_data: pd.Series, Y_data: pd.Series) -&gt; dict:\n    \"\"\"\n    Calculate the slope and p-value of a linear regression of `X` on `Y`.\n\n    Parameters:\n        X_name: The name of the predictor variable.\n        X_data: The data for the predictor variable.\n        Y_data: The data for the target variable.\n\n    Returns:\n        A dictionary containing the slope and p-value of the regression. If\n        there is no slope parameter because X_data does not vary enough,\n        the slope and p-value will be None.\n    \"\"\"\n    X_data = sm.add_constant(X_data)\n    model = sm.OLS(Y_data, X_data).fit()\n    slope = None\n    p_value = None\n    if len(model.params) &gt; 1:\n        slope = model.params.iloc[1]\n        p_value = model.pvalues.iloc[1]\n    return {\n        \"Candidate\": X_name,\n        \"Slope\": slope,\n        \"P-value\": p_value,\n    }\n</code></pre>"},{"location":"reference/logos/regression/#logos.regression.Regression.get_normalized_copy","title":"<code>get_normalized_copy(data)</code>  <code>staticmethod</code>","text":"<p>Return a normalized copy of the input data, with zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to normalize.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A normalized copy of the input data.</p> <code>Series</code> <p>The original standard deviations of the columns of the input data.</p> Source code in <code>src/logos/regression.py</code> <pre><code>@staticmethod\ndef get_normalized_copy(data: pd.DataFrame) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Return a normalized copy of the input data, with zero mean\n    and unit variance.\n\n    Parameters:\n        data: The data to normalize.\n\n    Returns:\n        A normalized copy of the input data.\n        The original standard deviations of the columns of the input data.\n    \"\"\"\n    data = data.copy(deep=True)\n    stdevs = data.std()\n\n    for column in data.columns:\n        if stdevs[column] == 0:\n            data.loc[:, column] = 0\n        else:\n            data.loc[:, column] = (data[column] - data[column].mean()) / stdevs[\n                column\n            ]\n    return data, stdevs\n</code></pre>"},{"location":"reference/logos/regression/#logos.regression.Regression.multi_ols","title":"<code>multi_ols(X_names, X_data, Y_data)</code>  <code>staticmethod</code>","text":"<p>Calculate the slopes and p-values of a multivariate linear regression of the variables in <code>X</code> on <code>Y</code>. Normalize each column to zero mean and unit variance before running the regression. Return both the normalized and unnormalized slopes.</p> <p>Parameters:</p> Name Type Description Default <code>X_names</code> <code>list[str]</code> <p>The names of the predictor variables.</p> required <code>X_data</code> <code>DataFrame</code> <p>The data for the predictor variables.</p> required <code>Y_data</code> <code>Series</code> <p>The data for the target variable.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the names, slopes, and p-values of the regressions.</p> Source code in <code>src/logos/regression.py</code> <pre><code>@staticmethod\ndef multi_ols(\n    X_names: list[str], X_data: pd.DataFrame, Y_data: pd.Series\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the slopes and p-values of a multivariate linear regression\n    of the variables in `X` on `Y`. Normalize each column to zero mean and\n    unit variance before running the regression. Return both the normalized\n    and unnormalized slopes.\n\n    Parameters:\n        X_names: The names of the predictor variables.\n        X_data: The data for the predictor variables.\n        Y_data: The data for the target variable.\n\n    Returns:\n        A dataframe with the names, slopes, and p-values of the regressions.\n    \"\"\"\n    X_data, stdevs = Regression.get_normalized_copy(X_data)\n\n    X_data = sm.add_constant(X_data)\n    model = sm.OLS(Y_data, X_data).fit()\n\n    # Get the coefficients and p-values, ignoring the constant\n    coefficients = model.params.iloc[1:]\n    p_values = model.pvalues.iloc[1:]\n\n    # Unnormalize the slopes\n    coefficients_unnormalized = coefficients.copy()\n    for coeff in coefficients_unnormalized.index:\n        coefficients_unnormalized[coeff] = (\n            coefficients[coeff] / stdevs[coeff] if stdevs[coeff] != 0 else 0\n        )\n\n    return pd.DataFrame(\n        {\n            \"Candidate\": coefficients.index,\n            \"Slope\": coefficients_unnormalized.values,\n            \"P-value\": p_values.values,\n            \"Normalized Slope\": coefficients.values,\n            \"Absolute Normalized Slope\": coefficients.abs().values,\n        }\n    )\n</code></pre>"},{"location":"reference/logos/tag_utils/","title":"TagUtils","text":""},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin","title":"<code>TagOrigin</code>","text":"<p>               Bases: <code>IntEnum</code></p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>class TagOrigin(IntEnum):\n    PRECEDING: int = 0\n    \"\"\"Indicates that the tag was derived from the preceding tokens in the corresponding template.\"\"\"\n\n    GPT_3POINT5_TURBO: int = 1\n    \"\"\"Indicates that the tag was derived using gpt-3.5-turbo.\"\"\"\n\n    GPT_4: int = 2\n    \"\"\"Indicates that the tag was derived using gpt-4.\"\"\"\n\n    NAME: int = 3\n    \"\"\"Indicates that the tag was derived from the name of the variable.\"\"\"\n\n    REGEX_VARIABLE: int = 4\n    \"\"\"Indicates that the tag was derived from the name of the variable because the name was given by the user.\"\"\"\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin.PRECEDING","title":"<code>PRECEDING: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived from the preceding tokens in the corresponding template.</p>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin.GPT_3POINT5_TURBO","title":"<code>GPT_3POINT5_TURBO: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived using gpt-3.5-turbo.</p>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin.GPT_4","title":"<code>GPT_4: int = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived using gpt-4.</p>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin.NAME","title":"<code>NAME: int = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived from the name of the variable.</p>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagOrigin.REGEX_VARIABLE","title":"<code>REGEX_VARIABLE: int = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived from the name of the variable because the name was given by the user.</p>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils","title":"<code>TagUtils</code>","text":"<p>A class for managing tags of parsed and prepared variables.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>class TagUtils:\n    \"\"\"\n    A class for managing tags of parsed and prepared variables.\n    \"\"\"\n\n    @staticmethod\n    def check_columns(df: pd.DataFrame, columns: list) -&gt; None:\n        \"\"\"\n        Check that the specified columns exist in the dataframe.\n\n        Parameters:\n            df: The dataframe to be checked.\n            columns: The columns to be checked.\n\n        Raises:\n            ValueError: If any of the columns are not present in the dataframe.\n        \"\"\"\n        if not set(columns).issubset(set(df.columns)):\n            raise ValueError(f\"Columns {columns} are not all present in the dataframe.\")\n\n    @staticmethod\n    def check_fields(series: pd.Series, fields: list) -&gt; None:\n        \"\"\"\n        Check that the specified fields exist in the specified series.\n\n        Parameters:\n            series: The series to be checked.\n            fields: The fields to be checked.\n\n        Raises:\n            ValueError: If any of the fields are not present in the series.\n        \"\"\"\n        if not set(fields).issubset(set(series.index)):\n            raise ValueError(f\"Fields {fields} are not all present in the series.\")\n\n    @staticmethod\n    def best_effort_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n        enable_gpt_tagging: bool,\n        gpt_model: str,\n    ) -&gt; tuple[str, bool]:\n        \"\"\"\n        Apply `gpt_tag` to `variable_row`, if possible, and return the result. If there is\n        no environment variable called OPENAI_API_KEY, or if `enable_gpt_tagging` is False,\n        apply `preceding_tokens_tag` instead.\n\n        Parameters:\n            templates_df: The dataframe containing information about the log templates.\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n            gpt_model: The GPT model to use.\n\n        Returns:\n            A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the\n            best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.\n        \"\"\"\n        if enable_gpt_tagging:\n            try:\n                return (TagUtils.gpt_tag(templates_df, variable_row, gpt_model), True)\n            except:\n                return (TagUtils.preceding_tokens_tag(variable_row), False)\n        else:\n            return (TagUtils.preceding_tokens_tag(variable_row), False)\n\n    @staticmethod\n    def waterfall_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n        banned_values: Optional[list[str]] = None,\n    ) -&gt; tuple[str, TagOrigin]:\n        \"\"\"\n        Apply each of the tagging methods in turn, in order of increasing cost, until a tag is found\n        that is not included in the banned values. In partidular, apply `preceding_tokens_tag` first,\n        then `gpt_tag` with the GPT-3.5 model, and finally `gpt_tag` with the GPT-4 model. If none of\n        these methods succeeds, return the name of the variable as the tag.\n\n        Parameters:\n            templates_df: The dataframe containing information about the log templates.\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            banned_values: A list of values that should not be used as tags.\n\n        Returns:\n            A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.\n        \"\"\"\n        name = variable_row[\"Name\"]\n        if variable_row[\"From regex\"]:\n            return (name, TagOrigin.REGEX_VARIABLE)\n\n        # Try to derive a tag from the preceding tokens in the corresponding template\n        tag, origin = TagUtils.preceding_tokens_tag(variable_row, banned_values)\n        if tag != name:\n            return (tag, origin)\n\n        # Try to derive a tag using GPT-3.5\n        try:\n            tag = TagUtils.gpt_tag(\n                templates_df, variable_row, \"gpt-3.5-turbo\", banned_values\n            )\n            if tag != name:\n                return (tag, TagOrigin.GPT_3POINT5_TURBO)\n        except Exception as e:\n            print(f\"Exception {e} came up while tagging {name} with GPT-3.5.\")\n            pass\n\n        # Try to derive a tag using GPT-4\n        try:\n            tag = TagUtils.gpt_tag(templates_df, variable_row, \"gpt-4\", banned_values)\n            if tag != name:\n                return (tag, TagOrigin.GPT_4)\n        except Exception as e:\n            print(f\"Exception {e} came up while tagging {name} with GPT-4.\")\n            pass\n\n        return (name, TagOrigin.NAME)\n\n    @staticmethod\n    def preceding_tokens_tag(\n        variable_row: pd.Series, banned_values: Optional[list[str]] = None\n    ) -&gt; tuple[str, TagOrigin]:\n        \"\"\"\n        Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.\n\n        Parameters:\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            banned_values: A list of values that should not be used as tags.\n\n        Returns:\n            A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.\n        \"\"\"\n\n        TagUtils.check_fields(variable_row, [\"Preceding 3 tokens\", \"Name\", \"From regex\"])\n        name = variable_row[\"Name\"]\n        if variable_row[\"From regex\"]:\n            return name, TagOrigin.REGEX_VARIABLE\n\n        pr = variable_row[\"Preceding 3 tokens\"]\n        tag = name\n        origin = TagOrigin.NAME\n        if len(pr) &gt;= 2 and (pr[-1] in \":=\") and (pr[-2][0] != \"&lt;\"):\n            tag = pr[-2]\n            origin = TagOrigin.PRECEDING\n        elif (\n            len(pr) == 3\n            and (pr[2] in \"\"\"\"'\"\"\")\n            and (pr[1] in \":=\")\n            and (pr[0][0] != \"&lt;\")\n        ):\n            tag = pr[0]\n            origin = TagOrigin.PRECEDING\n\n        # Double-check that the tag is not in the banned values\n        if banned_values is not None and tag in banned_values:\n            return name, TagOrigin.NAME\n\n        return tag, origin\n\n    @staticmethod\n    def gpt_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n        model: str = \"gpt-3.5-turbo\",\n        banned_values: Optional[list[str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Use GPT to derive a tag the variable described in `variable_row`,\n        using information about the corresponding log template, retrieved from `templates_df`.\n\n        Parameters:\n            templates_df: The dataframe containing information about the log templates.\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            model: The GPT model to use.\n            banned_values: A list of values that should not be used as tags.\n\n        Returns:\n            The GPT-generated tag for the parsed variable name.\n        \"\"\"\n\n        TagUtils.check_fields(variable_row, [\"Name\", \"Examples\"])\n        TagUtils.check_columns(templates_df, [\"TemplateId\", \"TemplateExample\"])\n\n        template_id = ParsedVariableName(variable_row[\"Name\"]).template_id()\n        idx = ParsedVariableName(variable_row[\"Name\"]).index()\n\n        line = templates_df[templates_df[\"TemplateId\"] == template_id][\n            \"TemplateExample\"\n        ].values[0]\n        line_toks = line.split()\n\n        # Define the messages to send to the model\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a backend engineer that knows all about the logging infrastructure of a distributed system.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate a tag for the variable that takes the value {line_toks[idx]} \"\"\"\n                f\"\"\"in the following log line:\\n {line}\\n\"\"\"\n                f\"\"\"Here are the 3 tokens that precede the variable: [{', '.join(line_toks[max(idx-3, 0):idx])} ]\\n\"\"\"\n                f\"\"\"Here are some more example values for this variable: [{', '.join(variable_row['Examples'])} ]\\n\"\"\"\n                #f\"\"\"Make sure the tag matches none of the following values: [{', '.join(banned_values) if banned_values is not None else ''} ]\\n\"\"\"\n                \"\"\"Return only the tag as a single word, possibly including underscores. DO NOT EVER REPLY WITH MORE THAN ONE WORD.\\n\"\"\",\n            },\n        ]\n\n        client = OpenAI()\n\n        tag = (\n            client.chat.completions.create(model=model, messages=messages)\n            .choices[0]\n            .message.content\n        )\n        tag_length = len(tag.split())\n        if tag_length &gt; 1:\n            # GPT didn't listen to us and returned a phrase describing the tag.\n            # Extract the word between the second-last and last occurrence of double quotes.\n            tag = tag.split('\"')[-2]\n\n\n        with open(\"gpt_log.txt\", \"a+\") as f:\n            f.write('----------------------------------\\n')\n            f.write(f\"Variable name: {variable_row['Name']}\\n\\n\")\n            f.write(f\"Model used: {model}\\n\\n\")\n            f.write(f\"Messages sent to the model:\\n{messages}\\n\\n\")\n            f.write(f\"Tag generated by the model:\\n{tag}\\n\\n\")\n            f.flush()\n\n        # Double-check that the tag is not in the banned values\n        if banned_values is not None and tag in banned_values:\n            with open(\"gpt_log.txt\", \"a+\") as f:\n                f.write('That tag is banned, returning name.\\n')\n            return variable_row[\"Name\"]\n\n        return tag\n\n    @staticmethod\n    def deduplicate_tags(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Ensure that the tags in df are unique, by making the tag column of any row\n        with a seen-before tag equal to the name column of that row.\n\n        Parameters:\n            df: The dataframe to be deduplicated.\n\n        Returns:\n            The deduplicated dataframe.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\", \"TagOrigin\"])\n        seen_tags = set()\n        for i, row in df.iterrows():\n            if row[\"Tag\"] in seen_tags:\n                df.loc[i, \"Tag\"] = row[\"Name\"]\n                df.loc[i, \"TagOrigin\"] = TagOrigin.NAME\n            else:\n                seen_tags.add(row[\"Tag\"])\n\n    @staticmethod\n    def set_tag(df: pd.DataFrame, name: str, tag: str, info: str = \"\") -&gt; None:\n        \"\"\"\n        Tag a parsed or prepared variable for easier access.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name: The name of the parsed or prepared variable.\n            tag: The tag to be set.\n            info: A string describing the type of variable being tagged (parsed or prepared).\n\n        Raises:\n            ValueError: If the name is not the name of a parsed or prepared variable.\n        \"\"\"\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        if name in df[\"Name\"].values:\n            df.loc[df[\"Name\"] == name, \"Tag\"] = tag\n            print(f\"Variable {name} tagged as {tag}\")\n        else:\n            raise ValueError(f\"{name} is not the name of a {info} variable.\")\n\n    @staticmethod\n    def get_tag(df: pd.DataFrame, name: str, info: str = \"\") -&gt; str:\n        \"\"\"\n        Retrieve the tag of a parsed or prepared variable.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name: The name of the parsed or prepared variable.\n            info: A string describing the type of variable being tagged (parsed or prepared).\n\n        Raises:\n            ValueError: If the name is not the name of a parsed or prepared variable.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        if name in df[\"Name\"].values:\n            return df.loc[df[\"Name\"] == name, \"Tag\"].values[0]\n        else:\n            raise ValueError(f\"{name} is not the name of a {info} variable.\")\n\n    @staticmethod\n    def name_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n        \"\"\"\n        Determine the name of a parsed or prepared variable, given either itself or its tag.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name_or_tag: The name or tag of the parsed or prepared variable.\n            info: A string describing the type of variable in question (parsed or prepared).\n\n        Returns:\n            The name of the parsed or prepared variable.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        name_or_tag = name_or_tag.strip()\n        if name_or_tag in df[\"Name\"].values:\n            return name_or_tag\n        elif name_or_tag in df[\"Tag\"].values:\n            return df.loc[df[\"Tag\"] == name_or_tag, \"Name\"].values[0]\n        else:\n            raise ValueError(\n                f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n            )\n\n    @staticmethod\n    def tag_of(df: pd.DataFrame, name_or_tag: Optional[str], info: str = \"\") -&gt; Optional[str]:\n        \"\"\"\n        Determine the tag of a parsed or prepared variable, given either itself or its name.\n        Retuirn None if the variable is None.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name_or_tag: The name or tag of the parsed or prepared variable.\n            info: A string describing the type of variable in question (parsed or prepared).\n\n        Returns:\n            The tag of the parsed or prepared variable.\n        \"\"\"\n\n        if name_or_tag is None:\n            return None\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        name_or_tag = name_or_tag.strip()\n        if name_or_tag in df[\"Tag\"].values:\n            return name_or_tag\n        elif name_or_tag in df[\"Name\"].values:\n            return df.loc[df[\"Name\"] == name_or_tag, \"Tag\"].values[0]\n        else:\n            raise ValueError(\n                f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n            )\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.check_columns","title":"<code>check_columns(df, columns)</code>  <code>staticmethod</code>","text":"<p>Check that the specified columns exist in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be checked.</p> required <code>columns</code> <code>list</code> <p>The columns to be checked.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the columns are not present in the dataframe.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef check_columns(df: pd.DataFrame, columns: list) -&gt; None:\n    \"\"\"\n    Check that the specified columns exist in the dataframe.\n\n    Parameters:\n        df: The dataframe to be checked.\n        columns: The columns to be checked.\n\n    Raises:\n        ValueError: If any of the columns are not present in the dataframe.\n    \"\"\"\n    if not set(columns).issubset(set(df.columns)):\n        raise ValueError(f\"Columns {columns} are not all present in the dataframe.\")\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.check_fields","title":"<code>check_fields(series, fields)</code>  <code>staticmethod</code>","text":"<p>Check that the specified fields exist in the specified series.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>The series to be checked.</p> required <code>fields</code> <code>list</code> <p>The fields to be checked.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the fields are not present in the series.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef check_fields(series: pd.Series, fields: list) -&gt; None:\n    \"\"\"\n    Check that the specified fields exist in the specified series.\n\n    Parameters:\n        series: The series to be checked.\n        fields: The fields to be checked.\n\n    Raises:\n        ValueError: If any of the fields are not present in the series.\n    \"\"\"\n    if not set(fields).issubset(set(series.index)):\n        raise ValueError(f\"Fields {fields} are not all present in the series.\")\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.best_effort_tag","title":"<code>best_effort_tag(templates_df, variable_row, enable_gpt_tagging, gpt_model)</code>  <code>staticmethod</code>","text":"<p>Apply <code>gpt_tag</code> to <code>variable_row</code>, if possible, and return the result. If there is no environment variable called OPENAI_API_KEY, or if <code>enable_gpt_tagging</code> is False, apply <code>preceding_tokens_tag</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>templates_df</code> <code>DataFrame</code> <p>The dataframe containing information about the log templates.</p> required <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT-3.5 tagging should be enabled.</p> required <code>gpt_model</code> <code>str</code> <p>The GPT model to use.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the</p> <code>bool</code> <p>best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef best_effort_tag(\n    templates_df: pd.DataFrame,\n    variable_row: pd.Series,\n    enable_gpt_tagging: bool,\n    gpt_model: str,\n) -&gt; tuple[str, bool]:\n    \"\"\"\n    Apply `gpt_tag` to `variable_row`, if possible, and return the result. If there is\n    no environment variable called OPENAI_API_KEY, or if `enable_gpt_tagging` is False,\n    apply `preceding_tokens_tag` instead.\n\n    Parameters:\n        templates_df: The dataframe containing information about the log templates.\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n        gpt_model: The GPT model to use.\n\n    Returns:\n        A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the\n        best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.\n    \"\"\"\n    if enable_gpt_tagging:\n        try:\n            return (TagUtils.gpt_tag(templates_df, variable_row, gpt_model), True)\n        except:\n            return (TagUtils.preceding_tokens_tag(variable_row), False)\n    else:\n        return (TagUtils.preceding_tokens_tag(variable_row), False)\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.waterfall_tag","title":"<code>waterfall_tag(templates_df, variable_row, banned_values=None)</code>  <code>staticmethod</code>","text":"<p>Apply each of the tagging methods in turn, in order of increasing cost, until a tag is found that is not included in the banned values. In partidular, apply <code>preceding_tokens_tag</code> first, then <code>gpt_tag</code> with the GPT-3.5 model, and finally <code>gpt_tag</code> with the GPT-4 model. If none of these methods succeeds, return the name of the variable as the tag.</p> <p>Parameters:</p> Name Type Description Default <code>templates_df</code> <code>DataFrame</code> <p>The dataframe containing information about the log templates.</p> required <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>banned_values</code> <code>Optional[list[str]]</code> <p>A list of values that should not be used as tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, TagOrigin]</code> <p>A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef waterfall_tag(\n    templates_df: pd.DataFrame,\n    variable_row: pd.Series,\n    banned_values: Optional[list[str]] = None,\n) -&gt; tuple[str, TagOrigin]:\n    \"\"\"\n    Apply each of the tagging methods in turn, in order of increasing cost, until a tag is found\n    that is not included in the banned values. In partidular, apply `preceding_tokens_tag` first,\n    then `gpt_tag` with the GPT-3.5 model, and finally `gpt_tag` with the GPT-4 model. If none of\n    these methods succeeds, return the name of the variable as the tag.\n\n    Parameters:\n        templates_df: The dataframe containing information about the log templates.\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        banned_values: A list of values that should not be used as tags.\n\n    Returns:\n        A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.\n    \"\"\"\n    name = variable_row[\"Name\"]\n    if variable_row[\"From regex\"]:\n        return (name, TagOrigin.REGEX_VARIABLE)\n\n    # Try to derive a tag from the preceding tokens in the corresponding template\n    tag, origin = TagUtils.preceding_tokens_tag(variable_row, banned_values)\n    if tag != name:\n        return (tag, origin)\n\n    # Try to derive a tag using GPT-3.5\n    try:\n        tag = TagUtils.gpt_tag(\n            templates_df, variable_row, \"gpt-3.5-turbo\", banned_values\n        )\n        if tag != name:\n            return (tag, TagOrigin.GPT_3POINT5_TURBO)\n    except Exception as e:\n        print(f\"Exception {e} came up while tagging {name} with GPT-3.5.\")\n        pass\n\n    # Try to derive a tag using GPT-4\n    try:\n        tag = TagUtils.gpt_tag(templates_df, variable_row, \"gpt-4\", banned_values)\n        if tag != name:\n            return (tag, TagOrigin.GPT_4)\n    except Exception as e:\n        print(f\"Exception {e} came up while tagging {name} with GPT-4.\")\n        pass\n\n    return (name, TagOrigin.NAME)\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.preceding_tokens_tag","title":"<code>preceding_tokens_tag(variable_row, banned_values=None)</code>  <code>staticmethod</code>","text":"<p>Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.</p> <p>Parameters:</p> Name Type Description Default <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>banned_values</code> <code>Optional[list[str]]</code> <p>A list of values that should not be used as tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, TagOrigin]</code> <p>A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef preceding_tokens_tag(\n    variable_row: pd.Series, banned_values: Optional[list[str]] = None\n) -&gt; tuple[str, TagOrigin]:\n    \"\"\"\n    Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.\n\n    Parameters:\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        banned_values: A list of values that should not be used as tags.\n\n    Returns:\n        A tuple containing (i) the tag for the parsed variable, and (ii) the origin of the tag.\n    \"\"\"\n\n    TagUtils.check_fields(variable_row, [\"Preceding 3 tokens\", \"Name\", \"From regex\"])\n    name = variable_row[\"Name\"]\n    if variable_row[\"From regex\"]:\n        return name, TagOrigin.REGEX_VARIABLE\n\n    pr = variable_row[\"Preceding 3 tokens\"]\n    tag = name\n    origin = TagOrigin.NAME\n    if len(pr) &gt;= 2 and (pr[-1] in \":=\") and (pr[-2][0] != \"&lt;\"):\n        tag = pr[-2]\n        origin = TagOrigin.PRECEDING\n    elif (\n        len(pr) == 3\n        and (pr[2] in \"\"\"\"'\"\"\")\n        and (pr[1] in \":=\")\n        and (pr[0][0] != \"&lt;\")\n    ):\n        tag = pr[0]\n        origin = TagOrigin.PRECEDING\n\n    # Double-check that the tag is not in the banned values\n    if banned_values is not None and tag in banned_values:\n        return name, TagOrigin.NAME\n\n    return tag, origin\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.gpt_tag","title":"<code>gpt_tag(templates_df, variable_row, model='gpt-3.5-turbo', banned_values=None)</code>  <code>staticmethod</code>","text":"<p>Use GPT to derive a tag the variable described in <code>variable_row</code>, using information about the corresponding log template, retrieved from <code>templates_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>templates_df</code> <code>DataFrame</code> <p>The dataframe containing information about the log templates.</p> required <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> <code>banned_values</code> <code>Optional[list[str]]</code> <p>A list of values that should not be used as tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The GPT-generated tag for the parsed variable name.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef gpt_tag(\n    templates_df: pd.DataFrame,\n    variable_row: pd.Series,\n    model: str = \"gpt-3.5-turbo\",\n    banned_values: Optional[list[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Use GPT to derive a tag the variable described in `variable_row`,\n    using information about the corresponding log template, retrieved from `templates_df`.\n\n    Parameters:\n        templates_df: The dataframe containing information about the log templates.\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        model: The GPT model to use.\n        banned_values: A list of values that should not be used as tags.\n\n    Returns:\n        The GPT-generated tag for the parsed variable name.\n    \"\"\"\n\n    TagUtils.check_fields(variable_row, [\"Name\", \"Examples\"])\n    TagUtils.check_columns(templates_df, [\"TemplateId\", \"TemplateExample\"])\n\n    template_id = ParsedVariableName(variable_row[\"Name\"]).template_id()\n    idx = ParsedVariableName(variable_row[\"Name\"]).index()\n\n    line = templates_df[templates_df[\"TemplateId\"] == template_id][\n        \"TemplateExample\"\n    ].values[0]\n    line_toks = line.split()\n\n    # Define the messages to send to the model\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a backend engineer that knows all about the logging infrastructure of a distributed system.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate a tag for the variable that takes the value {line_toks[idx]} \"\"\"\n            f\"\"\"in the following log line:\\n {line}\\n\"\"\"\n            f\"\"\"Here are the 3 tokens that precede the variable: [{', '.join(line_toks[max(idx-3, 0):idx])} ]\\n\"\"\"\n            f\"\"\"Here are some more example values for this variable: [{', '.join(variable_row['Examples'])} ]\\n\"\"\"\n            #f\"\"\"Make sure the tag matches none of the following values: [{', '.join(banned_values) if banned_values is not None else ''} ]\\n\"\"\"\n            \"\"\"Return only the tag as a single word, possibly including underscores. DO NOT EVER REPLY WITH MORE THAN ONE WORD.\\n\"\"\",\n        },\n    ]\n\n    client = OpenAI()\n\n    tag = (\n        client.chat.completions.create(model=model, messages=messages)\n        .choices[0]\n        .message.content\n    )\n    tag_length = len(tag.split())\n    if tag_length &gt; 1:\n        # GPT didn't listen to us and returned a phrase describing the tag.\n        # Extract the word between the second-last and last occurrence of double quotes.\n        tag = tag.split('\"')[-2]\n\n\n    with open(\"gpt_log.txt\", \"a+\") as f:\n        f.write('----------------------------------\\n')\n        f.write(f\"Variable name: {variable_row['Name']}\\n\\n\")\n        f.write(f\"Model used: {model}\\n\\n\")\n        f.write(f\"Messages sent to the model:\\n{messages}\\n\\n\")\n        f.write(f\"Tag generated by the model:\\n{tag}\\n\\n\")\n        f.flush()\n\n    # Double-check that the tag is not in the banned values\n    if banned_values is not None and tag in banned_values:\n        with open(\"gpt_log.txt\", \"a+\") as f:\n            f.write('That tag is banned, returning name.\\n')\n        return variable_row[\"Name\"]\n\n    return tag\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.deduplicate_tags","title":"<code>deduplicate_tags(df)</code>  <code>staticmethod</code>","text":"<p>Ensure that the tags in df are unique, by making the tag column of any row with a seen-before tag equal to the name column of that row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be deduplicated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The deduplicated dataframe.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef deduplicate_tags(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Ensure that the tags in df are unique, by making the tag column of any row\n    with a seen-before tag equal to the name column of that row.\n\n    Parameters:\n        df: The dataframe to be deduplicated.\n\n    Returns:\n        The deduplicated dataframe.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\", \"TagOrigin\"])\n    seen_tags = set()\n    for i, row in df.iterrows():\n        if row[\"Tag\"] in seen_tags:\n            df.loc[i, \"Tag\"] = row[\"Name\"]\n            df.loc[i, \"TagOrigin\"] = TagOrigin.NAME\n        else:\n            seen_tags.add(row[\"Tag\"])\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.set_tag","title":"<code>set_tag(df, name, tag, info='')</code>  <code>staticmethod</code>","text":"<p>Tag a parsed or prepared variable for easier access.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name</code> <code>str</code> <p>The name of the parsed or prepared variable.</p> required <code>tag</code> <code>str</code> <p>The tag to be set.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable being tagged (parsed or prepared).</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is not the name of a parsed or prepared variable.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef set_tag(df: pd.DataFrame, name: str, tag: str, info: str = \"\") -&gt; None:\n    \"\"\"\n    Tag a parsed or prepared variable for easier access.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name: The name of the parsed or prepared variable.\n        tag: The tag to be set.\n        info: A string describing the type of variable being tagged (parsed or prepared).\n\n    Raises:\n        ValueError: If the name is not the name of a parsed or prepared variable.\n    \"\"\"\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    if name in df[\"Name\"].values:\n        df.loc[df[\"Name\"] == name, \"Tag\"] = tag\n        print(f\"Variable {name} tagged as {tag}\")\n    else:\n        raise ValueError(f\"{name} is not the name of a {info} variable.\")\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.get_tag","title":"<code>get_tag(df, name, info='')</code>  <code>staticmethod</code>","text":"<p>Retrieve the tag of a parsed or prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name</code> <code>str</code> <p>The name of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable being tagged (parsed or prepared).</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is not the name of a parsed or prepared variable.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef get_tag(df: pd.DataFrame, name: str, info: str = \"\") -&gt; str:\n    \"\"\"\n    Retrieve the tag of a parsed or prepared variable.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name: The name of the parsed or prepared variable.\n        info: A string describing the type of variable being tagged (parsed or prepared).\n\n    Raises:\n        ValueError: If the name is not the name of a parsed or prepared variable.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    if name in df[\"Name\"].values:\n        return df.loc[df[\"Name\"] == name, \"Tag\"].values[0]\n    else:\n        raise ValueError(f\"{name} is not the name of a {info} variable.\")\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.name_of","title":"<code>name_of(df, name_or_tag, info='')</code>  <code>staticmethod</code>","text":"<p>Determine the name of a parsed or prepared variable, given either itself or its tag.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name_or_tag</code> <code>str</code> <p>The name or tag of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable in question (parsed or prepared).</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The name of the parsed or prepared variable.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef name_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n    \"\"\"\n    Determine the name of a parsed or prepared variable, given either itself or its tag.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name_or_tag: The name or tag of the parsed or prepared variable.\n        info: A string describing the type of variable in question (parsed or prepared).\n\n    Returns:\n        The name of the parsed or prepared variable.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    name_or_tag = name_or_tag.strip()\n    if name_or_tag in df[\"Name\"].values:\n        return name_or_tag\n    elif name_or_tag in df[\"Tag\"].values:\n        return df.loc[df[\"Tag\"] == name_or_tag, \"Name\"].values[0]\n    else:\n        raise ValueError(\n            f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n        )\n</code></pre>"},{"location":"reference/logos/tag_utils/#logos.tag_utils.TagUtils.tag_of","title":"<code>tag_of(df, name_or_tag, info='')</code>  <code>staticmethod</code>","text":"<p>Determine the tag of a parsed or prepared variable, given either itself or its name. Retuirn None if the variable is None.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name_or_tag</code> <code>Optional[str]</code> <p>The name or tag of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable in question (parsed or prepared).</p> <code>''</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The tag of the parsed or prepared variable.</p> Source code in <code>src/logos/tag_utils.py</code> <pre><code>@staticmethod\ndef tag_of(df: pd.DataFrame, name_or_tag: Optional[str], info: str = \"\") -&gt; Optional[str]:\n    \"\"\"\n    Determine the tag of a parsed or prepared variable, given either itself or its name.\n    Retuirn None if the variable is None.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name_or_tag: The name or tag of the parsed or prepared variable.\n        info: A string describing the type of variable in question (parsed or prepared).\n\n    Returns:\n        The tag of the parsed or prepared variable.\n    \"\"\"\n\n    if name_or_tag is None:\n        return None\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    name_or_tag = name_or_tag.strip()\n    if name_or_tag in df[\"Tag\"].values:\n        return name_or_tag\n    elif name_or_tag in df[\"Name\"].values:\n        return df.loc[df[\"Name\"] == name_or_tag, \"Tag\"].values[0]\n    else:\n        raise ValueError(\n            f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n        )\n</code></pre>"},{"location":"reference/logos/types/","title":"Types","text":""},{"location":"reference/logos/types/#logos.types.Types","title":"<code>Types</code>","text":"Source code in <code>src/logos/types.py</code> <pre><code>class Types:\n    Edge = tuple[str, str]\n    \"\"\"Type alias for a directed edge.\"\"\"\n\n    LeafLabelingFunction = Callable[[int], str]\n    \"\"\"Type alias for a leaf labeling function in `ATE`.\"\"\"\n\n    EdgeCountDict = defaultdict[Edge, int]\n    \"\"\"Type alias for a dictionary counting edge occurrences.\"\"\"\n</code></pre>"},{"location":"reference/logos/types/#logos.types.Types.Edge","title":"<code>Edge = tuple[str, str]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a directed edge.</p>"},{"location":"reference/logos/types/#logos.types.Types.LeafLabelingFunction","title":"<code>LeafLabelingFunction = Callable[[int], str]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a leaf labeling function in <code>ATE</code>.</p>"},{"location":"reference/logos/types/#logos.types.Types.EdgeCountDict","title":"<code>EdgeCountDict = defaultdict[Edge, int]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a dictionary counting edge occurrences.</p>"},{"location":"reference/logos/aggimp/","title":"Index","text":""},{"location":"reference/logos/aggimp/agg_funcs/","title":"Aggregation Functions","text":""},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.mean","title":"<code>mean(x)</code>","text":"<p>Calculates the mean of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mean will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mean of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def mean(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mean of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mean will be calculated.\n\n    Returns:\n        The mean of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mean(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.min","title":"<code>min(x)</code>","text":"<p>Calculates the minimum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the minimum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The minimum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def min(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the minimum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the minimum will be calculated.\n\n    Returns:\n        The minimum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.min(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.max","title":"<code>max(x)</code>","text":"<p>Calculates the maximum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the maximum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The maximum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def max(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the maximum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the maximum will be calculated.\n\n    Returns:\n        The maximum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.max(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.median","title":"<code>median(x)</code>","text":"<p>Calculates the median of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the median will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The median of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def median(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the median of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the median will be calculated.\n\n    Returns:\n        The median of the series, or None if the series is all NA.\n    \"\"\"\n    return x.median(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.mode","title":"<code>mode(x)</code>","text":"<p>Calculates the mode of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mode will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mode of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def mode(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mode of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mode will be calculated.\n\n    Returns:\n        The mode of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mode(dropna=True)[0] if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.std","title":"<code>std(x)</code>","text":"<p>Calculates the standard deviation of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the standard deviation will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The standard deviation of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def std(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the standard deviation of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the standard deviation will be calculated.\n\n    Returns:\n        The standard deviation of the series, or None if the series is all NA.\n    \"\"\"\n    return x.std(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.last","title":"<code>last(x)</code>","text":"<p>Returns the last non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the last non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The last non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def last(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the last non-NA value in a series.\n\n    Parameters:\n        x: The series for which the last non-NA value will be returned.\n\n    Returns:\n        The last non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().tail(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.first","title":"<code>first(x)</code>","text":"<p>Returns the first non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the first non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The first non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def first(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the first non-NA value in a series.\n\n    Parameters:\n        x: The series for which the first non-NA value will be returned.\n\n    Returns:\n        The first non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().head(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/agg_funcs/#logos.aggimp.agg_funcs.sum","title":"<code>sum(x)</code>","text":"<p>Calculates the sum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the sum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The sum of the series, or None if the series is all NA.</p> Source code in <code>src/logos/aggimp/agg_funcs.py</code> <pre><code>def sum(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the sum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the sum will be calculated.\n\n    Returns:\n        The sum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.sum(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/logos/aggimp/imp_funcs/","title":"Imputation Functions","text":""},{"location":"reference/logos/aggimp/imp_funcs/#logos.aggimp.imp_funcs.ffill_imp","title":"<code>ffill_imp(x)</code>","text":"<p>Impute the NA values in a series by forward-filling and return the series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the NA values will be imputed.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series, with NA values imputed.</p> Source code in <code>src/logos/aggimp/imp_funcs.py</code> <pre><code>def ffill_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Impute the NA values in a series by forward-filling and return the series.\n\n    Parameters:\n        x: The series for which the NA values will be imputed.\n\n    Returns:\n        The series, with NA values imputed.\n    \"\"\"\n    return x.ffill()\n</code></pre>"},{"location":"reference/logos/aggimp/imp_funcs/#logos.aggimp.imp_funcs.zero_imp","title":"<code>zero_imp(x)</code>","text":"<p>Impute the NA values in a series with zeroes and return the series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the NA values will be imputed.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series, with NA values imputed.</p> Source code in <code>src/logos/aggimp/imp_funcs.py</code> <pre><code>def zero_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Impute the NA values in a series with zeroes and return the series.\n\n    Parameters:\n        x: The series for which the NA values will be imputed.\n\n    Returns:\n        The series, with NA values imputed.\n    \"\"\"\n    return x.fillna(0)\n</code></pre>"},{"location":"reference/logos/aggimp/imp_funcs/#logos.aggimp.imp_funcs.no_imp","title":"<code>no_imp(x)</code>","text":"<p>No-op.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series to be returned.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series passed as a parameter.</p> Source code in <code>src/logos/aggimp/imp_funcs.py</code> <pre><code>def no_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    No-op.\n\n    Parameters:\n        x: The series to be returned.\n\n    Returns:\n        The series passed as a parameter.\n    \"\"\"\n    return x\n</code></pre>"},{"location":"reference/logos/variable_name/","title":"Index","text":""},{"location":"reference/logos/variable_name/parsed_variable_name/","title":"ParsedVariableName","text":""},{"location":"reference/logos/variable_name/parsed_variable_name/#logos.variable_name.parsed_variable_name.ParsedVariableName","title":"<code>ParsedVariableName</code>","text":"<p>Performs operations on a atring interpreted as a parsed variable name.</p> <p>The relevant string format is {template_id}[_{index}].</p> Source code in <code>src/logos/variable_name/parsed_variable_name.py</code> <pre><code>class ParsedVariableName:\n    \"\"\"\n    Performs operations on a atring interpreted as a parsed variable name.\n\n    The relevant string format is {template_id}[_{index}].\n    \"\"\"\n    def __init__(self, s: str) -&gt; None:\n        \"\"\"\n        Initializes a ParsedVariableName object.\n\n        Parameters:\n            s: The string interpretation of the parsed variable name.\n        \"\"\"\n        toks = s.split(\"_\")\n        self._s = s\n        self._template_id = toks[0]\n        self._index = int(toks[1]) if len(toks) &gt; 1 else -1\n\n    def template_id(self) -&gt; str:\n        \"\"\"\n        Returns the template ID of the parsed variable name.\n\n        Returns:\n            The template ID of the parsed variable name.\n        \"\"\"\n        return self._template_id\n\n    def index(self) -&gt; Optional[int]:\n        \"\"\"\n        Returns the index of the parsed variable name.\n\n        Returns:\n            The index of the parsed variable name, or None if the index is not\n            present.\n        \"\"\"\n        return self._index if self._index != -1 else None\n\n    def str(self) -&gt; str:\n        \"\"\"\n        Returns the string representation of the parsed variable name.\n\n        Returns:\n            The string representation of the parsed variable name.\n        \"\"\"\n        return self._s\n</code></pre>"},{"location":"reference/logos/variable_name/parsed_variable_name/#logos.variable_name.parsed_variable_name.ParsedVariableName.__init__","title":"<code>__init__(s)</code>","text":"<p>Initializes a ParsedVariableName object.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string interpretation of the parsed variable name.</p> required Source code in <code>src/logos/variable_name/parsed_variable_name.py</code> <pre><code>def __init__(self, s: str) -&gt; None:\n    \"\"\"\n    Initializes a ParsedVariableName object.\n\n    Parameters:\n        s: The string interpretation of the parsed variable name.\n    \"\"\"\n    toks = s.split(\"_\")\n    self._s = s\n    self._template_id = toks[0]\n    self._index = int(toks[1]) if len(toks) &gt; 1 else -1\n</code></pre>"},{"location":"reference/logos/variable_name/parsed_variable_name/#logos.variable_name.parsed_variable_name.ParsedVariableName.template_id","title":"<code>template_id()</code>","text":"<p>Returns the template ID of the parsed variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The template ID of the parsed variable name.</p> Source code in <code>src/logos/variable_name/parsed_variable_name.py</code> <pre><code>def template_id(self) -&gt; str:\n    \"\"\"\n    Returns the template ID of the parsed variable name.\n\n    Returns:\n        The template ID of the parsed variable name.\n    \"\"\"\n    return self._template_id\n</code></pre>"},{"location":"reference/logos/variable_name/parsed_variable_name/#logos.variable_name.parsed_variable_name.ParsedVariableName.index","title":"<code>index()</code>","text":"<p>Returns the index of the parsed variable name.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The index of the parsed variable name, or None if the index is not</p> <code>Optional[int]</code> <p>present.</p> Source code in <code>src/logos/variable_name/parsed_variable_name.py</code> <pre><code>def index(self) -&gt; Optional[int]:\n    \"\"\"\n    Returns the index of the parsed variable name.\n\n    Returns:\n        The index of the parsed variable name, or None if the index is not\n        present.\n    \"\"\"\n    return self._index if self._index != -1 else None\n</code></pre>"},{"location":"reference/logos/variable_name/parsed_variable_name/#logos.variable_name.parsed_variable_name.ParsedVariableName.str","title":"<code>str()</code>","text":"<p>Returns the string representation of the parsed variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the parsed variable name.</p> Source code in <code>src/logos/variable_name/parsed_variable_name.py</code> <pre><code>def str(self) -&gt; str:\n    \"\"\"\n    Returns the string representation of the parsed variable name.\n\n    Returns:\n        The string representation of the parsed variable name.\n    \"\"\"\n    return self._s\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/","title":"PreparedVariableName","text":""},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName","title":"<code>PreparedVariableName</code>","text":"<p>Performs operations on a string interpreted as a prepared variable name.</p> <p>The relevant string format is {template_id}[_{index}][={pre-agg value}]+{aggregate}[={post_agg value}].</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>class PreparedVariableName:\n    \"\"\"\n    Performs operations on a string interpreted as a prepared variable name.\n\n    The relevant string format is {template_id}[_{index}][={pre-agg value}]+{aggregate}[={post_agg value}].\n    \"\"\"\n\n    def __init__(self, s: str) -&gt; None:\n        \"\"\"\n        Initializes a PreparedVariableName object.\n\n        Parameters:\n            s: The string representation of the prepared variable name.\n        \"\"\"\n        mid_split = s.split(\"+\")\n\n        left_split = mid_split[0].split(\"=\")\n        right_split = mid_split[1].split(\"=\") if len(mid_split) &gt; 1 else [\"\", \"\"]\n\n        self._base_var = left_split[0]\n        self._pre_agg_value = left_split[1] if len(left_split) &gt; 1 else \"\"\n        self._aggregate = right_split[0]\n        self._post_agg_value = right_split[1] if len(right_split) &gt; 1 else \"\"\n\n    def base_var(self) -&gt; str:\n        \"\"\"\n        Returns the base variable of the prepared variable name.\n\n        Returns:\n            The base variable of the prepared variable name.\n        \"\"\"\n        return self._base_var\n\n    def template_id(self) -&gt; str:\n        \"\"\"\n        Returns the template ID of the prepared variable name. If the base variable\n        is 'TemplateId', then this will match the pre_agg_value.\n\n        Returns:\n            The template ID of the prepared variable name.\n        \"\"\"\n        if self._base_var == \"TemplateId\":\n            return self._pre_agg_value\n        else:\n            return ParsedVariableName(self._base_var).template_id()\n\n    def index(self) -&gt; Optional[int]:\n        \"\"\"\n        Returns the index of the prepared variable name.\n\n        Returns:\n            The index of the prepared variable name, or None if the index is not\n            present.\n        \"\"\"\n        return ParsedVariableName(self._base_var).index()\n\n    def pre_agg_value(self) -&gt; str:\n        \"\"\"\n        Returns the pre-aggregate value of the prepared variable name.\n\n        Returns:\n            The pre-aggregate value of the prepared variable name.\n        \"\"\"\n        return self._pre_agg_value\n\n    def aggregate(self) -&gt; str:\n        \"\"\"\n        Returns the aggregate of the prepared variable name.\n\n        Returns:\n            The aggregation function implied by the prepared variable name.\n        \"\"\"\n        return self._aggregate\n\n    def post_agg_value(self) -&gt; str:\n        \"\"\"\n        Returns the post-aggregate value of the prepared variable name.\n\n        Returns:\n            The post-aggregate value of the prepared variable name.\n        \"\"\"\n        return self._post_agg_value\n\n    def no_pre_post_aggs(self) -&gt; bool:\n        \"\"\"\n        Check whether the prepared variable has no pre- or post-aggregates.\n\n        Returns:\n            Whether the prepared variable has no pre- or post-aggregates.\n        \"\"\"\n        return self.pre_agg_value() == \"\" and self.post_agg_value() == \"\"\n\n    def has_base_var(self, x: str | Self) -&gt; bool:\n        \"\"\"\n        Check whether the prepared variable has the given base variable.\n\n        Parameters:\n            x: The base variable to check.\n\n        Returns:\n            Whether the prepared variable has the given base variable.\n        \"\"\"\n        return PreparedVariableName.same_base_var(self, x)\n\n    @staticmethod\n    def same_base_var(var1: str | Self, var2: str | Self) -&gt; bool:\n        \"\"\"\n        Check whether two prepared variables have the same base variable.\n\n        Parameters:\n            var1: The first variable to check.\n            var2: The second variable to check.\n\n        Returns:\n            Whether the two variables have the same base variable.\n        \"\"\"\n\n        if isinstance(var1, str):\n            var1 = PreparedVariableName(var1)\n        if isinstance(var2, str):\n            var2 = PreparedVariableName(var2)\n\n        return var1.base_var() == var2.base_var()\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.__init__","title":"<code>__init__(s)</code>","text":"<p>Initializes a PreparedVariableName object.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string representation of the prepared variable name.</p> required Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def __init__(self, s: str) -&gt; None:\n    \"\"\"\n    Initializes a PreparedVariableName object.\n\n    Parameters:\n        s: The string representation of the prepared variable name.\n    \"\"\"\n    mid_split = s.split(\"+\")\n\n    left_split = mid_split[0].split(\"=\")\n    right_split = mid_split[1].split(\"=\") if len(mid_split) &gt; 1 else [\"\", \"\"]\n\n    self._base_var = left_split[0]\n    self._pre_agg_value = left_split[1] if len(left_split) &gt; 1 else \"\"\n    self._aggregate = right_split[0]\n    self._post_agg_value = right_split[1] if len(right_split) &gt; 1 else \"\"\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.base_var","title":"<code>base_var()</code>","text":"<p>Returns the base variable of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The base variable of the prepared variable name.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def base_var(self) -&gt; str:\n    \"\"\"\n    Returns the base variable of the prepared variable name.\n\n    Returns:\n        The base variable of the prepared variable name.\n    \"\"\"\n    return self._base_var\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.template_id","title":"<code>template_id()</code>","text":"<p>Returns the template ID of the prepared variable name. If the base variable is 'TemplateId', then this will match the pre_agg_value.</p> <p>Returns:</p> Type Description <code>str</code> <p>The template ID of the prepared variable name.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def template_id(self) -&gt; str:\n    \"\"\"\n    Returns the template ID of the prepared variable name. If the base variable\n    is 'TemplateId', then this will match the pre_agg_value.\n\n    Returns:\n        The template ID of the prepared variable name.\n    \"\"\"\n    if self._base_var == \"TemplateId\":\n        return self._pre_agg_value\n    else:\n        return ParsedVariableName(self._base_var).template_id()\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.index","title":"<code>index()</code>","text":"<p>Returns the index of the prepared variable name.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The index of the prepared variable name, or None if the index is not</p> <code>Optional[int]</code> <p>present.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def index(self) -&gt; Optional[int]:\n    \"\"\"\n    Returns the index of the prepared variable name.\n\n    Returns:\n        The index of the prepared variable name, or None if the index is not\n        present.\n    \"\"\"\n    return ParsedVariableName(self._base_var).index()\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.pre_agg_value","title":"<code>pre_agg_value()</code>","text":"<p>Returns the pre-aggregate value of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The pre-aggregate value of the prepared variable name.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def pre_agg_value(self) -&gt; str:\n    \"\"\"\n    Returns the pre-aggregate value of the prepared variable name.\n\n    Returns:\n        The pre-aggregate value of the prepared variable name.\n    \"\"\"\n    return self._pre_agg_value\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.aggregate","title":"<code>aggregate()</code>","text":"<p>Returns the aggregate of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The aggregation function implied by the prepared variable name.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def aggregate(self) -&gt; str:\n    \"\"\"\n    Returns the aggregate of the prepared variable name.\n\n    Returns:\n        The aggregation function implied by the prepared variable name.\n    \"\"\"\n    return self._aggregate\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.post_agg_value","title":"<code>post_agg_value()</code>","text":"<p>Returns the post-aggregate value of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The post-aggregate value of the prepared variable name.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def post_agg_value(self) -&gt; str:\n    \"\"\"\n    Returns the post-aggregate value of the prepared variable name.\n\n    Returns:\n        The post-aggregate value of the prepared variable name.\n    \"\"\"\n    return self._post_agg_value\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.no_pre_post_aggs","title":"<code>no_pre_post_aggs()</code>","text":"<p>Check whether the prepared variable has no pre- or post-aggregates.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the prepared variable has no pre- or post-aggregates.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def no_pre_post_aggs(self) -&gt; bool:\n    \"\"\"\n    Check whether the prepared variable has no pre- or post-aggregates.\n\n    Returns:\n        Whether the prepared variable has no pre- or post-aggregates.\n    \"\"\"\n    return self.pre_agg_value() == \"\" and self.post_agg_value() == \"\"\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.has_base_var","title":"<code>has_base_var(x)</code>","text":"<p>Check whether the prepared variable has the given base variable.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str | Self</code> <p>The base variable to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the prepared variable has the given base variable.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>def has_base_var(self, x: str | Self) -&gt; bool:\n    \"\"\"\n    Check whether the prepared variable has the given base variable.\n\n    Parameters:\n        x: The base variable to check.\n\n    Returns:\n        Whether the prepared variable has the given base variable.\n    \"\"\"\n    return PreparedVariableName.same_base_var(self, x)\n</code></pre>"},{"location":"reference/logos/variable_name/prepared_variable_name/#logos.variable_name.prepared_variable_name.PreparedVariableName.same_base_var","title":"<code>same_base_var(var1, var2)</code>  <code>staticmethod</code>","text":"<p>Check whether two prepared variables have the same base variable.</p> <p>Parameters:</p> Name Type Description Default <code>var1</code> <code>str | Self</code> <p>The first variable to check.</p> required <code>var2</code> <code>str | Self</code> <p>The second variable to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the two variables have the same base variable.</p> Source code in <code>src/logos/variable_name/prepared_variable_name.py</code> <pre><code>@staticmethod\ndef same_base_var(var1: str | Self, var2: str | Self) -&gt; bool:\n    \"\"\"\n    Check whether two prepared variables have the same base variable.\n\n    Parameters:\n        var1: The first variable to check.\n        var2: The second variable to check.\n\n    Returns:\n        Whether the two variables have the same base variable.\n    \"\"\"\n\n    if isinstance(var1, str):\n        var1 = PreparedVariableName(var1)\n    if isinstance(var2, str):\n        var2 = PreparedVariableName(var2)\n\n    return var1.base_var() == var2.base_var()\n</code></pre>"}]}