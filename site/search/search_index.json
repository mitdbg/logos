{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>This is Sawmill - a system for processing logs to help extract causal insights!</p> <p>You can find a quick demo below, as well as documentation in the \"Docs\" tab.</p>"},{"location":"#synthetic-example","title":"Synthetic Example","text":"<p>Let's start with a basic example of a synthetic log. This log contains three different log line templates, each of which reports the value of one of the variables <code>x</code>, <code>y</code> and <code>z</code>. Here is an example of each line:</p> Log Excerpt<pre><code>2023-03-14T20:55:49.234591Z DATA The current line will include the value of z = 100.0\n2023-03-14T20:55:49.233591Z DATA Short message with x = 199.05342369055703\n2023-03-14T20:55:49.232591Z DATA This is a log message that reports y = 399.82103707673997\n</code></pre> <p>Each millisecond, we decide whether or not to \"flip\" the value of <code>z</code> between 100 and 200, with probability <code>1%</code>. If we end up flipping it, we print a log line that reports the new value (first template above).</p> <p>Each millisecond, we print the value of <code>x</code> (second template above), which is generated each time by taking the most recent value of <code>z</code> and adding random noise in <code>[-1,1]</code>.</p> <p>Finally, each millisecond we print the value of <code>y</code> with probability <code>50%</code>(third template above). The value of <code>y</code> is generated each time by taking the most recent value of <code>z</code>, multiplying it by <code>2</code> and adding random noise in <code>[-1,1]</code>.</p> <p>Let's create a Sawmill instance and initialize it with the path to this log:</p> <p>Code<pre><code>s = Sawmill(\"datasets_raw/xyzw_logs/log_2023-03-14_20:55:49.log\", \n            workdir='datasets/xyzw_logs/log_2023-03-14_20:55:49')\n</code></pre> Output<pre><code>Initialized Sawmill with log file datasets_raw/xyzw_logs/log_2023-03-14_20:55:49.log\nWork directory set to datasets/xyzw_logs/log_2023-03-14_20:55:49\n</code></pre></p>"},{"location":"#parsing-and-tagging-variables","title":"Parsing and tagging variables","text":"<p>We can call the <code>parse()</code> function to parse the log into a table using the Drain algorithm. </p> <p>The Drain algorithm can first extract named variables from each log line using regular expressions. By default, Sawmill provides a single regular expression to capture timestamps in the format shown above, naming the resulting field <code>Timestamp</code>, but users can parse additional regular expressions to the <code>parse()</code> call as a dictionary, via the <code>regex_dict</code> parameter.</p> <p>Then, the Drain algorithm separated log lines into \"templates\" and \"variables\", based on the similarity of each new line to the lines seen before it. You can find more information in the paper.</p> <p>Code<pre><code>s.parse(force=True, regex_dict={'Date': r'\\d{4}-\\d{2}-\\d{2}', 'Time': r'\\d{2}:\\d{2}:\\d{2}.\\d{6}'})\ns.parsed_log.head()\n</code></pre> Output<pre><code>Parsing file: datasets_raw/xyzw_logs/log_2023-03-14_20:55:49.log\n\n\nReading and tokenizing log lines...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15121/15121 [00:00&lt;00:00, 63539.36it/s]\nDetermining template for each line...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15121/15121 [00:00&lt;00:00, 100521.41it/s]\nExtracting variables from each line...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 108.38it/s]\n\n\nVariables generated from regexes: 2\nVariables generated by Drain: 3\nTemplates with at least 1 non-regex variable: 3\nTemplates with at least 2 occurrences: 3\n\n\nDetermining variable types...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 6142.80it/s]\nCasting date variables...:   0%|          | 0/1 [00:00&lt;?, ?it/s]~/.local/lib/python3.11/site-packages/tqdm/std.py:915: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  return func(*args, **kwargs)\nCasting date variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 216.70it/s]\nCasting time variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 52.90it/s]\nCasting numerical variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 443.33it/s]\nTagging variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 10597.03it/s]\nDetecting identifiers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 1601.37it/s]\nDumping pkl file to datasets/xyzw_logs/log_2023-03-14_20:55:49/log_2023-03-14_20:55:49.log_parsed_log_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 576.22it/s]\nDumping pkl file to datasets/xyzw_logs/log_2023-03-14_20:55:49/log_2023-03-14_20:55:49.log_parsed_templates_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1376.54it/s]\nDumping pkl file to datasets/xyzw_logs/log_2023-03-14_20:55:49/log_2023-03-14_20:55:49.log_parsed_variables_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1593.58it/s]\n\nParsing complete in 0.791723 seconds!\n</code></pre></p> LineId TemplateId Date Time ac34c703_7 62ff3fb7_11 ff661264_12 1 ac34c703 2023-03-14 0 days 20:55:49.165591 100.002860 NaN NaN 2 62ff3fb7 2023-03-14 0 days 20:55:49.165591 NaN 200.189963 NaN 3 ac34c703 2023-03-14 0 days 20:55:49.166591 100.812299 NaN NaN 4 62ff3fb7 2023-03-14 0 days 20:55:49.166591 NaN 200.458986 NaN 5 ac34c703 2023-03-14 0 days 20:55:49.167591 100.260164 NaN NaN <p>Let's take a look at the extracted templates. Indeed, Drain has detected the three templates in the synthetic log. Each template is given a hash-based template ID. For each template, we see that a variable has been detected (at word indices 7, 11 and 12, respectively) and replaced with <code>&lt;*&gt;</code> in the template text. </p> <p>For each template, we also got a regular expression match at index 0, corresponding to the regular expression that matches timestamps. The timestamps were correspondingly replaced by <code>&lt;*0&gt;</code>, since that was the 0th (and only) regular expression we provided.</p> Code<pre><code>s.parsed_templates\n</code></pre> TemplateText TemplateId Occurrences VariableIndices RegexIndices &lt;*0&gt;T&lt;*1&gt;Z DATA Short message with x = &lt;*&gt; ac34c703 10000 [7] [] &lt;*0&gt;T&lt;*1&gt;Z DATA This is a log message that reports y = &lt;*&gt; 62ff3fb7 5006 [11] [] &lt;*0&gt;T&lt;*1&gt;Z DATA The current line will include the value of z = &lt;*&gt; ff661264 115 [12] [] <p>We can also look at the extracted variables. For variables that were extracted via a regular expression, like the timestamp, the user has already provided a name (in this case, <code>Timestamp</code>). For the rest, a variable name is generated from the corresponding template ID and the index that the variable appear in, within the template. For each variable, we also report the preceding 3 tokens and some example values. </p> <p>Since the automatically-generated names are not meaningful, we also allow for each variable to carry a tag. Initial values of these tags are guessed from the preceding tokens of each variable:</p> <pre><code>s.parsed_variables\n</code></pre> Name Tag Type IsUninteresting Occurrences Preceding 3 tokens Examples From regex Date Date date True 15121 [] [2023-03-14] True Time Time time True 15121 [] [20:55:49.165591, 20:55:49.166591, 20:55:49.167591, 20:55:49.168591, 20:55:49.169591] True ac34c703_7 x num False 10000 [with, x, =] [100.00285967800117, 100.81229891323964, 100.26016363495995, 199.4482406365968, 200.98748260974] False 62ff3fb7_11 y num False 5006 [reports, y, =] [200.1899632341074, 200.45898567905772, 399.6905765982258, 400.22712382793407, 400.08343040509993] False ff661264_12 z num False 115 [of, z, =] [200.0, 100.0] False <p>However, the user is free to change or provide the tag of a variable as they like:</p> <p>Code<pre><code>s.tag_parsed_variable(\"ac34c703_7\", \"X\")\ns.parsed_variables\n</code></pre> Output<pre><code>Variable ac34c703_7 tagged as X\n</code></pre></p> Name Tag Type IsUninteresting Occurrences Preceding 3 tokens Examples From regex Date Date date True 15121 [] [2023-03-14] True Time Time time True 15121 [] [20:55:49.165591, 20:55:49.166591, 20:55:49.167591, 20:55:49.168591, 20:55:49.169591] True ac34c703_7 X num False 10000 [with, x, =] [100.00285967800117, 100.81229891323964, 100.26016363495995, 199.4482406365968, 200.98748260974] False 62ff3fb7_11 y num False 5006 [reports, y, =] [200.1899632341074, 200.45898567905772, 399.6905765982258, 400.22712382793407, 400.08343040509993] False ff661264_12 z num False 115 [of, z, =] [200.0, 100.0] False"},{"location":"#defining-the-causal-unit-and-aggregating","title":"Defining the causal unit and aggregating","text":"<p>To continue our analysis, we would like to structure and complete the parsed information to form \"causal units\", which will be the units on which our causal analysis will be performed. If we think of causality in other contexts, the causal units could be patients in a medical context, or individuals in an economic/social study.</p> <p>Causal units are defined by one of the available attributes. In a medical context, this could be \"patient name\". In a systems context, we could pick one of the variables parsed from the log and call <code>set_causal_unit()</code>. For example, the call below indicates that each causal unit should be a <code>1 ms</code>-long time interval (not that this choice may not be appropriate in every setting):</p> <p>Code<pre><code>s.set_causal_unit(\"Time\", time_granularity=1)\n</code></pre> Output<pre><code>Causal unit set to Time (tag: Time)  with time_granularity 1 ms\n</code></pre></p> <p>Given the causal unit, we can then ask Sawmill to perpare the log for analysis by using <code>prepare()</code>. Preparing the log involves two distinct tasks:</p> <ol> <li> <p>Aggregation: Based on our choice of causal unit, there might be variables that take a multitude of values on different log lines associated with the same causal unit. For example, had we chosen a 10ms window as our causal unit, there would have been 10 lines reporting values of <code>x</code>. From this multitude of values, a fixed set of values must be derived (e.g. we could always keep the mean, or the last value seen). By default, Sawmill will generate the <code>min</code>, <code>max</code> and <code>mean</code> for numerical variables; the most recent value for string variables; and the least recent value for date-typed variables.</p> </li> <li> <p>Imputation: On the other end of the spectrum, there might be variables that are never observed within some causal unit. For example, <code>z</code> is only reported every approximately <code>100 ms</code>, so it should be missing most of the time, if our causal unit is a <code>1 ms</code> window. Whether and how to impute such missing values is application-dependent, since we must avoid information leakage from one causal unit to another or risk violating SUTVA. In this case, we know that <code>z</code> should be interpreted as a \"sticky\" value, but <code>x</code> and <code>y</code> should not be imputed.</p> </li> </ol> <p>After aggregating and imputing, we drop any causal units that still have missing values.</p> <p>Code<pre><code>imputation_functions = {'z': 'ffill_imp'}\ns.prepare(custom_imp=imputation_functions, force=True)\ns.prepared_log.head(10)\n</code></pre> Output<pre><code>Dropped 1 identifier columns.\nCalculating aggregates for each causal unit...\n\n\nImputing missing values...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:00&lt;00:00, 3577.06it/s]\nOne-hot encoding categorical variables...: 0it [00:00, ?it/s]\n~/causal-log/src/sawmill/sawmill.py:658: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  self._prepared_log[date_cols] = self._prepared_log[date_cols].applymap(\n~/causal-log/src/sawmill/sawmill.py:666: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  self._prepared_log[time_cols] = self._prepared_log[time_cols].applymap(\nDumping pkl file to datasets/xyzw_logs/log_2023-03-14_20:55:49/log_2023-03-14_20:55:49.log_prepared_log_Time.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 865.16it/s]\nDumping pkl file to datasets/xyzw_logs/log_2023-03-14_20:55:49/log_2023-03-14_20:55:49.log_prepared_variables_Time.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1365.78it/s]\n\nSuccessfully prepared the log with causal unit Time \n            (tag: Time) with time_granularity 1 ms\nPreparation complete in 8.887855 seconds!\n</code></pre></p> ac34c703_7+mean ac34c703_7+min ac34c703_7+max 62ff3fb7_11+mean 62ff3fb7_11+min 62ff3fb7_11+max ff661264_12+mean ff661264_12+min ff661264_12+max Time+earliest 0 days 20:55:49.170591 199.659262 199.659262 199.659262 399.690577 399.690577 399.690577 200.0 200.0 200.0 0 days 20:55:49.173591 200.068149 200.068149 200.068149 400.227124 400.227124 400.227124 200.0 200.0 200.0 0 days 20:55:49.175591 199.504039 199.504039 199.504039 400.083430 400.083430 400.083430 200.0 200.0 200.0 0 days 20:55:49.178591 200.158634 200.158634 200.158634 399.188180 399.188180 399.188180 200.0 200.0 200.0 0 days 20:55:49.179591 200.934658 200.934658 200.934658 400.514334 400.514334 400.514334 200.0 200.0 200.0 0 days 20:55:49.181591 200.065517 200.065517 200.065517 399.894171 399.894171 399.894171 200.0 200.0 200.0 0 days 20:55:49.182591 200.937334 200.937334 200.937334 400.577257 400.577257 400.577257 200.0 200.0 200.0 0 days 20:55:49.183591 199.831092 199.831092 199.831092 399.077464 399.077464 399.077464 200.0 200.0 200.0 0 days 20:55:49.188591 199.372097 199.372097 199.372097 399.650472 399.650472 399.650472 200.0 200.0 200.0 0 days 20:55:49.193591 200.476665 200.476665 200.476665 400.613001 400.613001 400.613001 200.0 200.0 200.0"},{"location":"#graph-exploration-and-ate-calculation","title":"Graph exploration and ATE calculation","text":"<p>We're now ready to proceed to our analysis! First, let's pick the variable that interests us, and ask the system for candidate causes for it. Remember from our data generation process, that <code>y</code> appears to be roughly equal to <code>2x</code>, but that both <code>x</code> and <code>y</code> are in fact driven by the value of <code>z</code>:</p> Code<pre><code>s.explore_candidate_causes(\"y mean\") \n</code></pre> Candidate Tag Slope P-value Candidate-&gt;Target Edge Status Target-&gt;Candidate Edge Status 0 ac34c703_7+mean X mean 2.000074 0.0 Accepted Rejected 1 ff661264_12+mean z mean 2.000228 0.0 Accepted Rejected <p>As expected, the system cannot meaningfully distinguish between the impact of <code>X</code> and <code>z</code> on <code>y</code>, since <code>X</code> is essentially a slightly noisy version of <code>z</code>. Both are reported with similar relationship strengths, so let's accept both edges into the causal graph.</p> Code<pre><code>s.accept('X mean', 'y mean', interactive=False)\ns.accept('z mean', 'y mean')\n</code></pre> <p></p> <p>Based on this trivial causal graph, we can calculate the ATE:</p> Code<pre><code>s.get_unadjusted_ate(\"X mean\", \"y mean\")\n</code></pre> Output<pre><code>2.00007436079689\n</code></pre> <p>However, let's now assume that we are (thankfully) not yet fully convinced, and would like to look for any possible confounding. Let's ask the system for candidates causes of <code>X</code>:</p> Code<pre><code>s.explore_candidate_causes(\"X mean\")\n</code></pre> Candidate Tag Slope P-value Candidate-&gt;Target Edge Status Target-&gt;Candidate Edge Status 0 62ff3fb7_11+mean y mean 0.499895 0.0 Rejected Accepted 1 ff661264_12+mean z mean 0.999940 0.0 Undecided Undecided <p><code>z</code> is successfully detected once more, and we can use domain knowledge to judge that <code>z</code> influencing <code>X</code> is the correct direction. Indeed, let's see what happens if we add it to the graph:</p> Code<pre><code>s.accept('z mean', 'X mean')\n</code></pre> <p></p> Code<pre><code>s.get_unadjusted_ate(\"X mean\", \"y mean\")\n</code></pre> Output<pre><code>2.00007436079689\n</code></pre> Code<pre><code>s.get_adjusted_ate(\"X mean\", \"y mean\")\n</code></pre> Output<pre><code>-0.0249128594251147\n</code></pre> <p>That is, after adjusting for <code>z</code>, the effect of <code>x</code> on <code>y</code> is negligible.</p>"},{"location":"#postgresqltpc-ds-example","title":"PostgreSQL/TPC-DS Example","text":"<p>Now that we saw the basics of the framework, let's try to apply it to logs from a real system. </p> <p>We set up a PostgreSQL instance and load it with the data from TPC-DS with scale factor 1. We then run a workload in which we issue the queries in the TPC-DS workload once each (except for 4 particularly long-running queries, to expedite development \ud83d\ude05). </p> <p>We use PostgreSQL's logging to capture the execution of this workload:</p> Log Excerpt<pre><code>2023-11-01 17:54:53.027 EDT [ 6542c92d.1f943 3/5186 ] postgres@tpcds1 LOG:  statement: BEGIN\n2023-11-01 17:54:53.027 EDT [ 6542c92d.1f943 3/5186 ] postgres@tpcds1 LOG:  duration: 0.033 ms\n2023-11-01 17:54:53.028 EDT [ 6542c92d.1f943 3/5186 ] postgres@tpcds1 LOG:  statement: -- Filename: query080.sql\n\n  with ssr as\n   (select  s_store_id as store_id,\n            sum(ss_ext_sales_price) as sales,\n            sum(coalesce(sr_return_amt, 0)) as returns,\n            sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit\n    from store_sales left outer join store_returns on\n</code></pre> <p>We configure logging to print out the latency of every query, and the prefix of each line is set to <code>%m [ %c %v ] %q%u@%d</code>, where:</p> <ul> <li>%m = timestamp with milliseconds</li> <li>%c = session ID</li> <li>%v = virtual transaction ID</li> <li>%q = stop here in non-session processes</li> <li>%u = user name</li> <li>%d = database name</li> </ul> <p>We run this workload 8 times total, each using a new connection -  2 runs each for each of 4 different parameter configurations. </p> <p>For each parameter configuration, we decide on a total memory budget (128 kB or 256 kB). We then set the number of <code>max_parallel_workers</code> to 1 or 2, and the amount of <code>work_mem</code> to the budget divided by the number of max parallel workers. For example:</p> Log Excerpt<pre><code>2023-11-01 17:54:53.018 EDT [ 6542c92d.1f943 3/5184 ] postgres@tpcds1 LOG:  statement: BEGIN\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5184 ] postgres@tpcds1 LOG:  duration: 0.076 ms\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5184 ] postgres@tpcds1 LOG:  statement: SET max_parallel_workers = 1;\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5184 ] postgres@tpcds1 LOG:  duration: 0.062 ms\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5184 ] postgres@tpcds1 LOG:  statement: COMMIT\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/0 ] postgres@tpcds1 LOG:  duration: 0.030 ms\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5185 ] postgres@tpcds1 LOG:  statement: BEGIN\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5185 ] postgres@tpcds1 LOG:  duration: 0.023 ms\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5185 ] postgres@tpcds1 LOG:  statement: SET work_mem = '128.0';\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5185 ] postgres@tpcds1 LOG:  duration: 0.044 ms\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/5185 ] postgres@tpcds1 LOG:  statement: COMMIT\n2023-11-01 17:54:53.019 EDT [ 6542c92d.1f943 3/0 ] postgres@tpcds1 LOG:  duration: 0.026 ms\n</code></pre> <p>Can we use Sawmill to find the causal effect of these modifications on query runtime?</p>"},{"location":"#parsing-and-preparation","title":"Parsing and preparation","text":"<p>Since we run each iteration of the workload in a separate connection, we can extract the connection ID from each log line and use it to define our causal units. We also specify that each log message begins with a timestamp, so that Samwill can collapse multi-line log messages into a single message:</p> <p>Code<pre><code>s2 = Sawmill(\"datasets_raw/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log\", workdir=\"datasets/tpc-ds\")\ns2.parse(regex_dict={\"Date\": r'\\d{4}-\\d{2}-\\d{2}',\n        \"Time\": r'\\d{2}:\\d{2}:\\d{2}\\.\\d{3}(?= EDT \\[ )', \n        \"sessionID\" : r'(?&lt;=EDT \\[ )\\S+\\.\\S+',\n        \"tID\": r'3/\\d+(?= ] )'\n        },message_prefix=r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{3}', force=True)\ns2.set_causal_unit(\"sessionID\")\ns2.prepare(force = True)\n</code></pre> Output<pre><code>Initialized Sawmill with log file datasets_raw/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log\nWork directory set to datasets/tpc-ds\nParsing file: datasets_raw/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log\n\n\nReading and tokenizing log lines...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42108/42108 [00:00&lt;00:00, 85860.79it/s]\nDetermining template for each line...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4687/4687 [00:00&lt;00:00, 65351.69it/s]\nExtracting variables from each line...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 349.71it/s]\n\n\nVariables generated from regexes: 4\nVariables generated by Drain: 63\nTemplates with at least 1 non-regex variable: 100\nTemplates with at least 2 occurrences: 99\n\n\nDetermining variable types...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67/67 [00:00&lt;00:00, 49414.17it/s]\nCasting date variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 431.11it/s]\nCasting time variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00, 275.95it/s]\nCasting numerical variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 1644.05it/s]\nTagging variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67/67 [00:00&lt;00:00, 83018.72it/s]\nDetecting identifiers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67/67 [00:00&lt;00:00, 4675.30it/s]\nDumping pkl file to datasets/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log_parsed_log_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 95.75it/s]\nDumping pkl file to datasets/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log_parsed_templates_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1182.83it/s]\nDumping pkl file to datasets/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log_parsed_variables_None.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1450.81it/s]\n\n\nParsing complete in 1.088751 seconds!\nCausal unit set to sessionID (tag: sessionID) \nDropped 3 identifier columns.\nCalculating aggregates for each causal unit...\n\n\nImputing missing values...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73/73 [00:00&lt;00:00, 16921.86it/s]\nOne-hot encoding categorical variables...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [00:00&lt;00:00, 996.50it/s]\nDumping pkl file to datasets/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log_prepared_log_sessionID.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 2413.29it/s]\nDumping pkl file to datasets/tpc-ds/work_mem_2_256kB_2_128kB_parallel_1_2.log_prepared_variables_sessionID.pkl...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 1171.59it/s]\n\nSuccessfully prepared the log with causal unit sessionID \n            (tag: sessionID)\nPreparation complete in 0.343447 seconds!\n</code></pre></p> <p>Now, the prepared log consists of one causal unit per run of the workload:</p> Code<pre><code>s2.prepared_log \n</code></pre> Date+earliest 8ea7bb0e_23+mean 8ea7bb0e_23+min 8ea7bb0e_23+max 424c4f52_12+mean 424c4f52_12+min 424c4f52_12+max f1c25a57_15+mean f1c25a57_15+min f1c25a57_15+max 80cfbe3b_16+mean 80cfbe3b_16+min 80cfbe3b_16+max 758612aa_26+mean 758612aa_26+min 758612aa_26+max f20ea3d3_12+latest=COMMIT 7fce1bc0_15+latest=query020.sql 7fce1bc0_29+latest=cs_ext_sales_price 7fce1bc0_36+latest=cs_ext_sales_price 7fce1bc0_42+latest=cs_ext_sales_price 7fce1bc0_54+latest=catalog_sales 7fce1bc0_60+latest=cs_item_sk 7fce1bc0_80+latest=cs_sold_date_sk f8592a76_15+latest=query053.sql f8592a76_21+latest=i_manufact_id f8592a76_40+latest=i_manufact_id f8592a76_42+latest=avg_quarterly_sales f8592a76_220+latest=i_manufact_id f8592a76_222+latest=d_qoy f8592a76_228+latest=avg_quarterly_sales f8592a76_236+latest=avg_quarterly_sales f8592a76_239+latest=avg_quarterly_sales f8592a76_247+latest=avg_quarterly_sales f8592a76_249+latest=sum_sales f8592a76_251+latest=i_manufact_id 7e11a9a5_15+latest=query026.sql 7e11a9a5_21+latest=cs_quantity 7e11a9a5_27+latest=cs_list_price 7e11a9a5_33+latest=cs_coupon_amt 7e11a9a5_39+latest=cs_sales_price 7e11a9a5_43+latest=catalog_sales 7e11a9a5_53+latest=cs_sold_date_sk 7e11a9a5_57+latest=cs_item_sk 7e11a9a5_61+latest=cs_bill_cdemo_sk 7e11a9a5_65+latest=cs_promo_sk 252996ec_15+latest=query099.sql 252996ec_28+latest=cc_name 252996ec_35+latest=cs_ship_date_sk 252996ec_37+latest=cs_sold_date_sk 252996ec_59+latest=cs_ship_date_sk 252996ec_61+latest=cs_sold_date_sk 252996ec_67+latest=cs_ship_date_sk 252996ec_69+latest=cs_sold_date_sk 252996ec_91+latest=cs_ship_date_sk 252996ec_93+latest=cs_sold_date_sk 252996ec_99+latest=cs_ship_date_sk 252996ec_101+latest=cs_sold_date_sk 252996ec_123+latest=cs_ship_date_sk 252996ec_125+latest=cs_sold_date_sk 252996ec_131+latest=cs_ship_date_sk 252996ec_133+latest=cs_sold_date_sk 252996ec_155+latest=cs_ship_date_sk 252996ec_157+latest=cs_sold_date_sk 252996ec_173+latest=catalog_sales 252996ec_179+latest=call_center 252996ec_191+latest=cs_ship_date_sk 252996ec_195+latest=cs_warehouse_sk 252996ec_199+latest=cs_ship_mode_sk 252996ec_203+latest=cs_call_center_sk 252996ec_205+latest=cc_call_center_sk 252996ec_219+latest=cc_name 252996ec_233+latest=cc_name sessionID+latest 6542c92d.1f943 1.698797e+12 42396.0 42396.0 42396.0 13483.595990 0.020 2935111.471 1.0 1.0 1.0 128.0 128.0 128.0 42396.0 42396.0 42396.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 6542d880.1fc1b 1.698797e+12 51214.0 51214.0 51214.0 13126.180309 0.021 2876689.649 1.0 1.0 1.0 128.0 128.0 128.0 51214.0 51214.0 51214.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 6542e76c.1fe63 1.698797e+12 54898.0 54898.0 54898.0 22983.408399 0.023 4460683.070 2.0 2.0 2.0 64.0 64.0 64.0 54898.0 54898.0 54898.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 6543018c.20227 1.698797e+12 47566.0 47566.0 47566.0 23075.070997 0.023 4477061.260 2.0 2.0 2.0 64.0 64.0 64.0 47566.0 47566.0 47566.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 65431bc7.20615 1.698797e+12 60122.0 60122.0 60122.0 2707.956093 0.021 423565.150 1.0 1.0 1.0 256.0 256.0 256.0 60122.0 60122.0 60122.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 65431ef0.2073a 1.698883e+12 47102.0 47102.0 47102.0 2781.880526 0.019 423792.790 1.0 1.0 1.0 256.0 256.0 256.0 47102.0 47102.0 47102.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 6543221a.2085f 1.698883e+12 43972.0 43972.0 43972.0 13000.099663 0.024 2868369.399 2.0 2.0 2.0 128.0 128.0 128.0 43972.0 43972.0 43972.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True 654330e1.20b8f 1.698883e+12 37380.0 37380.0 37380.0 13005.023144 0.024 2871176.389 2.0 2.0 2.0 128.0 128.0 128.0 37380.0 37380.0 37380.0 True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True"},{"location":"#graph-exploration-and-ate-calculation_1","title":"Graph exploration and ATE calculation","text":"<p>We ask for candidate causes of query duration:</p> Code<pre><code>s2.explore_candidate_causes(\"duration mean\")\n</code></pre> Candidate Tag Slope P-value Candidate-&gt;Target Edge Status Target-&gt;Candidate Edge Status 0 80cfbe3b_16+mean work_mem mean -1.005257e+02 0.000029 Undecided Undecided 1 Date+earliest Date earliest 7.664300e-09 0.001963 Undecided Undecided 2 f1c25a57_15+mean max_parallel_workers mean 9.990997e+03 0.055041 Undecided Undecided 3 8ea7bb0e_23+mean port mean -1.459892e-01 0.745097 Undecided Undecided 4 758612aa_26+mean port mean -1.459892e-01 0.745097 Undecided Undecided <p>Let's learn more about some of the top candidate causes:</p> <p>Code<pre><code>s2.inspect('work_mem mean')\n</code></pre> Output<pre><code>Information about prepared variable 80cfbe3b_16+mean:\n\n--&gt; Variable Information about 80cfbe3b_16:\n</code></pre></p> Name Tag Type IsUninteresting Occurrences Preceding 3 tokens Examples From regex Aggregates 80cfbe3b_16 work_mem num False 8 [work_mem, =, '] [128.0, 64.0, 256.0] False [mean, min, max] Output<pre><code>--&gt; Template Information about 80cfbe3b:\n</code></pre> TemplateText TemplateId Occurrences VariableIndices RegexIndices &lt;*0&gt; &lt;*1&gt; EDT [ &lt;*2&gt; &lt;*3&gt; ] postgres@tpcds1 LOG : statement : SET work_mem = ' &lt;*&gt; ' ; 80cfbe3b 8 [16] [0, 1, 4, 5] Output<pre><code>--&gt; Causal Unit Partial Information:\n</code></pre> 80cfbe3b_16+mean (candidate) sessionID+latest 6542c92d.1f943 128.0 6542d880.1fc1b 128.0 6542e76c.1fe63 64.0 6543018c.20227 64.0 65431bc7.20615 256.0 65431ef0.2073a 256.0 6543221a.2085f 128.0 654330e1.20b8f 128.0 <p>Code<pre><code>s2.inspect('max_parallel_workers mean')\n</code></pre> Output<pre><code>Information about prepared variable f1c25a57_15+mean:\n\n--&gt; Variable Information about f1c25a57_15:\n</code></pre></p> Name Tag Type IsUninteresting Occurrences Preceding 3 tokens Examples From regex Aggregates f1c25a57_15 max_parallel_workers num False 8 [SET, max_parallel_workers, =] [1, 2] False [mean, min, max] Output<pre><code>--&gt; Template Information about f1c25a57:\n</code></pre> TemplateText TemplateId Occurrences VariableIndices RegexIndices &lt;*0&gt; &lt;*1&gt; EDT [ &lt;*2&gt; &lt;*3&gt; ] postgres@tpcds1 LOG : statement : SET max_parallel_workers = &lt;*&gt; ; f1c25a57 8 [15] [0, 1, 4, 5] Output<pre><code>--&gt; Causal Unit Partial Information:\n</code></pre> f1c25a57_15+mean (candidate) sessionID+latest 6542c92d.1f943 1.0 6542d880.1fc1b 1.0 6542e76c.1fe63 2.0 6543018c.20227 2.0 65431bc7.20615 1.0 65431ef0.2073a 1.0 6543221a.2085f 2.0 654330e1.20b8f 2.0 <p>Indeed, two of them correspond to the two parameters we are tweaking. Assuming we do not know the experiment set up, let's accept them both as causes.</p> Code<pre><code>s2.accept('work_mem mean', 'duration mean', interactive=False)\ns2.accept('max_parallel_workers mean', 'duration mean')\n</code></pre> <p></p> <p>Let's now look at the corresponding ATE of the number of <code>max_parallel_workers</code> on the mean query latency. Intuitively, more parallelism should help so this should be negative:</p> Code<pre><code>s2.get_adjusted_ate(\"max_parallel_workers mean\", \"duration mean\")\n</code></pre> Output<pre><code>-1588.99225407086\n</code></pre> <p>What if we hadn't considered the amount of working memory as a cause?</p> Code<pre><code>s2.get_unadjusted_ate(\"max_parallel_workers mean\", \"duration mean\")\n</code></pre> Output<pre><code>9990.99732122586\n</code></pre> <p>Surprising discrepancy! Let's look at the underlying data though:</p> Code<pre><code>t = s2.prepared_log[[\"80cfbe3b_16+mean\", \"f1c25a57_15+mean\", \"424c4f52_12+mean\"]]\nt.columns = [\"work_mem\", \"max_parallel_workers\", \"mean_latency\"]\nt\n</code></pre> work_mem max_parallel_workers mean_latency sessionID+latest 6542c92d.1f943 128.0 1.0 13483.595990 6542d880.1fc1b 128.0 1.0 13126.180309 6542e76c.1fe63 64.0 2.0 22983.408399 6543018c.20227 64.0 2.0 23075.070997 65431bc7.20615 256.0 1.0 2707.956093 65431ef0.2073a 256.0 1.0 2781.880526 6543221a.2085f 128.0 2.0 13000.099663 654330e1.20b8f 128.0 2.0 13005.023144 <p>Indeed, for a fixed amount of working memory, more parallelism leads to lower latency. But because of how we set up the experiment, more parallelism comes at the expense of working memory, so naively looking at only the 2 right-hand columns above would indeed make us think that more parallelism means higher latency!</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>src<ul> <li>sawmill<ul> <li>aggimp<ul> <li>agg_funcs</li> <li>imp_funcs</li> </ul> </li> <li>aggregate_selector</li> <li>ate</li> <li>causal_discoverer</li> <li>causal_unit_suggester</li> <li>clustering_params</li> <li>drain</li> <li>edge_occurrence_tree</li> <li>edge_state_matrix</li> <li>graph_renderer</li> <li>pickler</li> <li>printer</li> <li>regression</li> <li>sawmill</li> <li>tag_utils</li> <li>types</li> <li>variable_name<ul> <li>parsed_variable_name</li> <li>prepared_variable_name</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/src/sawmill/","title":"Home","text":""},{"location":"reference/src/sawmill/aggregate_selector/","title":"AggregateSelector","text":""},{"location":"reference/src/sawmill/aggregate_selector/#src.sawmill.aggregate_selector.AggregateSelector","title":"<code>AggregateSelector</code>","text":"Source code in <code>src/sawmill/aggregate_selector.py</code> <pre><code>class AggregateSelector:\n    DEFAULT_AGGREGATES = {\n        \"num\": [\n            \"mean\",\n            \"max\",\n            \"min\",\n        ],\n        \"str\": [\n            \"last\",\n            \"mode\",\n            \"first\",\n        ],\n    }\n\n    def _entropy(col: pd.Series) -&gt; float:\n        \"\"\"\n        Calculates the entropy of a column.\n\n        Parameters:\n            col: The column for which to calculate the entropy.\n\n        Returns:\n            The entropy of `col`.\n        \"\"\"\n\n        rel_value_counts = col.value_counts(normalize=True)\n        if rel_value_counts.empty:\n            return 0\n        return -np.sum(rel_value_counts * np.log2(rel_value_counts))\n\n    def find_uninformative_aggregates(\n        prepared_log: pd.DataFrame, parsed_variables: pd.DataFrame, causal_unit_var: str\n    ) -&gt; list[str]:\n        \"\"\"\n        Find aggregates that are uninformative for each column in `prepared_log`.\n        Aggregates are uninformative unless they maximize the empirical entropy across causal units.\n\n        Parameters:\n            prepared_log: The prepared log.\n            parsed_variables: The parsed variables.\n            causal_unit_var: The name of the causal unit variable.\n\n        Returns:\n            A list of uninformative aggregates for `prepared_log`.\n        \"\"\"\n\n        drop_list = []\n\n        for row in parsed_variables.itertuples():\n            aggs = row.Aggregates\n            if len(aggs) == 0 or row.Name == causal_unit_var:\n                continue\n\n            vars = [f\"{row.Name}+{agg}\" for agg in aggs]\n            best_var = f\"{row.Name}+{AggregateSelector.DEFAULT_AGGREGATES[row.Type][0]}\"\n            max_entropy = -np.inf\n\n            for var in vars:\n                entropy = AggregateSelector._entropy(prepared_log[var])\n\n                if entropy &gt; max_entropy:\n                    best_var = var\n                    max_entropy = entropy\n\n            drop_list.extend([var for var in vars if var != best_var])\n\n        return drop_list\n</code></pre>"},{"location":"reference/src/sawmill/aggregate_selector/#src.sawmill.aggregate_selector.AggregateSelector._entropy","title":"<code>_entropy(col)</code>","text":"<p>Calculates the entropy of a column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column for which to calculate the entropy.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The entropy of <code>col</code>.</p> Source code in <code>src/sawmill/aggregate_selector.py</code> <pre><code>def _entropy(col: pd.Series) -&gt; float:\n    \"\"\"\n    Calculates the entropy of a column.\n\n    Parameters:\n        col: The column for which to calculate the entropy.\n\n    Returns:\n        The entropy of `col`.\n    \"\"\"\n\n    rel_value_counts = col.value_counts(normalize=True)\n    if rel_value_counts.empty:\n        return 0\n    return -np.sum(rel_value_counts * np.log2(rel_value_counts))\n</code></pre>"},{"location":"reference/src/sawmill/aggregate_selector/#src.sawmill.aggregate_selector.AggregateSelector.find_uninformative_aggregates","title":"<code>find_uninformative_aggregates(prepared_log, parsed_variables, causal_unit_var)</code>","text":"<p>Find aggregates that are uninformative for each column in <code>prepared_log</code>. Aggregates are uninformative unless they maximize the empirical entropy across causal units.</p> <p>Parameters:</p> Name Type Description Default <code>prepared_log</code> <code>DataFrame</code> <p>The prepared log.</p> required <code>parsed_variables</code> <code>DataFrame</code> <p>The parsed variables.</p> required <code>causal_unit_var</code> <code>str</code> <p>The name of the causal unit variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of uninformative aggregates for <code>prepared_log</code>.</p> Source code in <code>src/sawmill/aggregate_selector.py</code> <pre><code>def find_uninformative_aggregates(\n    prepared_log: pd.DataFrame, parsed_variables: pd.DataFrame, causal_unit_var: str\n) -&gt; list[str]:\n    \"\"\"\n    Find aggregates that are uninformative for each column in `prepared_log`.\n    Aggregates are uninformative unless they maximize the empirical entropy across causal units.\n\n    Parameters:\n        prepared_log: The prepared log.\n        parsed_variables: The parsed variables.\n        causal_unit_var: The name of the causal unit variable.\n\n    Returns:\n        A list of uninformative aggregates for `prepared_log`.\n    \"\"\"\n\n    drop_list = []\n\n    for row in parsed_variables.itertuples():\n        aggs = row.Aggregates\n        if len(aggs) == 0 or row.Name == causal_unit_var:\n            continue\n\n        vars = [f\"{row.Name}+{agg}\" for agg in aggs]\n        best_var = f\"{row.Name}+{AggregateSelector.DEFAULT_AGGREGATES[row.Type][0]}\"\n        max_entropy = -np.inf\n\n        for var in vars:\n            entropy = AggregateSelector._entropy(prepared_log[var])\n\n            if entropy &gt; max_entropy:\n                best_var = var\n                max_entropy = entropy\n\n        drop_list.extend([var for var in vars if var != best_var])\n\n    return drop_list\n</code></pre>"},{"location":"reference/src/sawmill/ate/","title":"ATE","text":""},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.Pruner","title":"<code>Pruner</code>","text":"Source code in <code>src/sawmill/ate.py</code> <pre><code>class Pruner:\n    LASSO_DEFAULT_ALPHA = 0.3\n    LASSO_DEFAULT_MAX_ITER = 100000\n\n    \"\"\"\n    A collection of pruning functions for prepared variables,\n    used for pruning and candidate suggestion.\n    \"\"\"\n\n    @staticmethod\n    def prune_with_lasso(\n        data: pd.DataFrame,\n        outcome_cols: list[str],\n        alpha: float = LASSO_DEFAULT_ALPHA,\n        max_iter: int = LASSO_DEFAULT_MAX_ITER,\n        top_n: int = 0,\n        ignore: Optional[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Prune variables using Lasso regression.\n\n        Parameters:\n            data: The dataframe containing the data.\n            outcome_cols: The names of the target variables.\n            alpha: The Lasso regularization parameter.\n            max_iter: The maximum number of iterations for Lasso.\n            top_n: The number of variables to return. If 0, return all variables.\n\n        Returns:\n            The names of the variables that Lasso identified as impactful, optionally\n            limited to the top `n` variables by absolute coefficient.\n        \"\"\"\n\n        # TODO: do this properly wherever this is called\n        outcome_col = outcome_cols[0]\n\n        # Separate the target variable and predictor variables.\n        # Optionally, do not consider variables already in the graph.\n        y = data[outcome_cols]\n        drop_cols = [] if ignore is None else ignore\n        to_ignore = outcome_cols\n        drop_cols.extend(to_ignore)\n\n        # Do not consider variables with the same base variable as an ignored variable.\n        for v in to_ignore:\n            vp = PreparedVariableName(v)\n            if vp.base_var() != \"TemplateId\":\n                drop_cols.extend([c for c in data.columns if vp.base_var() in c])\n        drop_cols = list(set(drop_cols))\n\n        # Iterate until multiple prepared variables with the same base variable are eliminated.\n        done = False\n\n        while not done:\n            Printer.printv(f\"Variables that Lasso will ignore: {drop_cols}\")\n            X = data.drop(drop_cols, axis=1)\n            X_cols = X.columns\n            if X.empty:\n                return []\n\n            scaler = StandardScaler()\n            X = scaler.fit_transform(X)\n\n            # Fit a Lasso model to the data\n            lasso = Lasso(alpha=alpha, max_iter=max_iter)\n            lasso.fit(X, y)\n            Printer.printv(f\"Lasso coefficients : {lasso.coef_}\")\n            Printer.printv(f\"Scale: {scaler.scale_}\")\n            final_coefs = lasso.coef_ / scaler.scale_\n            abs_coefs = np.abs(final_coefs)\n            Printer.printv(f\"Lasso coefficients unscaled: {final_coefs}\")\n\n            # Mask for nonzero elements\n            nonzero_mask = final_coefs != 0\n\n            # Mask for top n largest elements by absolute value\n            # Create an array of False values with the same shape as the coefficients\n            top_n_mask = [False] * len(final_coefs)\n            for i in np.argsort(abs_coefs)[-top_n:]:\n                top_n_mask[i] = True\n\n            # Retrieve columns based on conditions above\n            selected_names = list(X_cols[nonzero_mask &amp; top_n_mask])\n\n            # Only keep one aggregate per variable\n            d = set()\n            done = True\n            for var in selected_names:\n                base_var = PreparedVariableName(var).base_var()\n                if base_var in d:\n                    drop_cols.append(var)\n                    done = False\n                else:\n                    d.add(base_var)\n\n        Printer.printv(\"Lasso identified the following impactful variables:\")\n        Printer.printv(selected_names)\n\n        return selected_names\n\n    @staticmethod\n    def prune_with_triangle(\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        treatment_col: str,\n        outcome_col: str,\n        work_dir: str,\n        top_n: int = 0,\n        force: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Prune variables using triangle method.\n\n        Parameters:\n            data: The dataframe containing the data.\n            vars: The dataframe containing the variables.\n            treatment_col: The name of the treatment variable.\n            outcome_col: The name of the outcome variable.\n            work_dir: The directory to store intermediate files in.\n            top_n: The number of variables to return. If 0, return all variables.\n            force: Whether to force recalculation of the triangle method.\n\n        Returns:\n            The names of the variables that triangle method identified as impactful, optionally\n            limited to the top `n` variables.\n        \"\"\"\n\n        # Check whether we can use pre-calculated results\n        filename = os.path.join(\n            work_dir, f\"pickles/triangle_dags/{treatment_col}_{outcome_col}.pkl\"\n        )\n        if os.path.isfile(filename) and not force:\n            df = pickle.load(open(filename, \"rb\"))\n            print(\"Found pickled file\")\n            return list(df.index[:top_n].values)\n\n        Printer.printv(\"Starting to prune using triangle method\")\n        max_diffs = {}\n        base_ate = ATECalculator.get_ate_and_confidence(\n            data, vars, treatment_col, outcome_col, calculate_std_error=False\n        )[\"ATE\"]\n\n        for var in tqdm(data.columns, \"Processing triangle dags\"):\n            if var == treatment_col or var == outcome_col:\n                continue\n\n            # Construct the graphs to consider\n            graphs = []\n            # Second cause\n            graphs.append(\n                nx.DiGraph([(treatment_col, outcome_col), (var, outcome_col)])\n            )\n            # Confounder\n            graphs.append(\n                nx.DiGraph(\n                    [\n                        (treatment_col, outcome_col),\n                        (var, treatment_col),\n                        (var, outcome_col),\n                    ]\n                )\n            )\n            # Mediator with direct path\n            graphs.append(\n                nx.DiGraph(\n                    [\n                        (treatment_col, outcome_col),\n                        (treatment_col, var),\n                        (var, outcome_col),\n                    ]\n                )\n            )\n            # Mediator without direct path\n            graphs.append(nx.DiGraph([(treatment_col, var), (var, outcome_col)]))\n\n            # Calculate the corrsponding ATEs\n            ates = [base_ate]\n            for G in graphs:\n                try:\n                    ates.append(\n                        ATECalculator.get_ate_and_confidence(\n                            data,\n                            vars,\n                            treatment_col,\n                            outcome_col,\n                            graph=G,\n                            calculate_std_error=False,\n                        )[\"ATE\"]\n                    )\n                except:\n                    pass\n            max_diffs[var] = max(ates) - min(ates)\n        max_diffs = max_diffs\n        df = pd.DataFrame.from_dict(max_diffs, orient=\"index\", columns=[\"max_diff\"])\n        df = df.sort_values(by=\"max_diff\", ascending=False)\n\n        Pickler.dump(df, filename)\n\n        return list(df.index[:top_n].values)\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.Pruner.LASSO_DEFAULT_MAX_ITER","title":"<code>LASSO_DEFAULT_MAX_ITER = 100000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A collection of pruning functions for prepared variables, used for pruning and candidate suggestion.</p>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.Pruner.prune_with_lasso","title":"<code>prune_with_lasso(data, outcome_cols, alpha=LASSO_DEFAULT_ALPHA, max_iter=LASSO_DEFAULT_MAX_ITER, top_n=0, ignore=None)</code>  <code>staticmethod</code>","text":"<p>Prune variables using Lasso regression.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>outcome_cols</code> <code>list[str]</code> <p>The names of the target variables.</p> required <code>alpha</code> <code>float</code> <p>The Lasso regularization parameter.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>max_iter</code> <code>int</code> <p>The maximum number of iterations for Lasso.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>top_n</code> <code>int</code> <p>The number of variables to return. If 0, return all variables.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The names of the variables that Lasso identified as impactful, optionally</p> <code>list[str]</code> <p>limited to the top <code>n</code> variables by absolute coefficient.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>@staticmethod\ndef prune_with_lasso(\n    data: pd.DataFrame,\n    outcome_cols: list[str],\n    alpha: float = LASSO_DEFAULT_ALPHA,\n    max_iter: int = LASSO_DEFAULT_MAX_ITER,\n    top_n: int = 0,\n    ignore: Optional[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Prune variables using Lasso regression.\n\n    Parameters:\n        data: The dataframe containing the data.\n        outcome_cols: The names of the target variables.\n        alpha: The Lasso regularization parameter.\n        max_iter: The maximum number of iterations for Lasso.\n        top_n: The number of variables to return. If 0, return all variables.\n\n    Returns:\n        The names of the variables that Lasso identified as impactful, optionally\n        limited to the top `n` variables by absolute coefficient.\n    \"\"\"\n\n    # TODO: do this properly wherever this is called\n    outcome_col = outcome_cols[0]\n\n    # Separate the target variable and predictor variables.\n    # Optionally, do not consider variables already in the graph.\n    y = data[outcome_cols]\n    drop_cols = [] if ignore is None else ignore\n    to_ignore = outcome_cols\n    drop_cols.extend(to_ignore)\n\n    # Do not consider variables with the same base variable as an ignored variable.\n    for v in to_ignore:\n        vp = PreparedVariableName(v)\n        if vp.base_var() != \"TemplateId\":\n            drop_cols.extend([c for c in data.columns if vp.base_var() in c])\n    drop_cols = list(set(drop_cols))\n\n    # Iterate until multiple prepared variables with the same base variable are eliminated.\n    done = False\n\n    while not done:\n        Printer.printv(f\"Variables that Lasso will ignore: {drop_cols}\")\n        X = data.drop(drop_cols, axis=1)\n        X_cols = X.columns\n        if X.empty:\n            return []\n\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n        # Fit a Lasso model to the data\n        lasso = Lasso(alpha=alpha, max_iter=max_iter)\n        lasso.fit(X, y)\n        Printer.printv(f\"Lasso coefficients : {lasso.coef_}\")\n        Printer.printv(f\"Scale: {scaler.scale_}\")\n        final_coefs = lasso.coef_ / scaler.scale_\n        abs_coefs = np.abs(final_coefs)\n        Printer.printv(f\"Lasso coefficients unscaled: {final_coefs}\")\n\n        # Mask for nonzero elements\n        nonzero_mask = final_coefs != 0\n\n        # Mask for top n largest elements by absolute value\n        # Create an array of False values with the same shape as the coefficients\n        top_n_mask = [False] * len(final_coefs)\n        for i in np.argsort(abs_coefs)[-top_n:]:\n            top_n_mask[i] = True\n\n        # Retrieve columns based on conditions above\n        selected_names = list(X_cols[nonzero_mask &amp; top_n_mask])\n\n        # Only keep one aggregate per variable\n        d = set()\n        done = True\n        for var in selected_names:\n            base_var = PreparedVariableName(var).base_var()\n            if base_var in d:\n                drop_cols.append(var)\n                done = False\n            else:\n                d.add(base_var)\n\n    Printer.printv(\"Lasso identified the following impactful variables:\")\n    Printer.printv(selected_names)\n\n    return selected_names\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.Pruner.prune_with_triangle","title":"<code>prune_with_triangle(data, vars, treatment_col, outcome_col, work_dir, top_n=0, force=False)</code>  <code>staticmethod</code>","text":"<p>Prune variables using triangle method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>vars</code> <code>DataFrame</code> <p>The dataframe containing the variables.</p> required <code>treatment_col</code> <code>str</code> <p>The name of the treatment variable.</p> required <code>outcome_col</code> <code>str</code> <p>The name of the outcome variable.</p> required <code>work_dir</code> <code>str</code> <p>The directory to store intermediate files in.</p> required <code>top_n</code> <code>int</code> <p>The number of variables to return. If 0, return all variables.</p> <code>0</code> <code>force</code> <code>bool</code> <p>Whether to force recalculation of the triangle method.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The names of the variables that triangle method identified as impactful, optionally</p> <code>list[str]</code> <p>limited to the top <code>n</code> variables.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>@staticmethod\ndef prune_with_triangle(\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    treatment_col: str,\n    outcome_col: str,\n    work_dir: str,\n    top_n: int = 0,\n    force: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Prune variables using triangle method.\n\n    Parameters:\n        data: The dataframe containing the data.\n        vars: The dataframe containing the variables.\n        treatment_col: The name of the treatment variable.\n        outcome_col: The name of the outcome variable.\n        work_dir: The directory to store intermediate files in.\n        top_n: The number of variables to return. If 0, return all variables.\n        force: Whether to force recalculation of the triangle method.\n\n    Returns:\n        The names of the variables that triangle method identified as impactful, optionally\n        limited to the top `n` variables.\n    \"\"\"\n\n    # Check whether we can use pre-calculated results\n    filename = os.path.join(\n        work_dir, f\"pickles/triangle_dags/{treatment_col}_{outcome_col}.pkl\"\n    )\n    if os.path.isfile(filename) and not force:\n        df = pickle.load(open(filename, \"rb\"))\n        print(\"Found pickled file\")\n        return list(df.index[:top_n].values)\n\n    Printer.printv(\"Starting to prune using triangle method\")\n    max_diffs = {}\n    base_ate = ATECalculator.get_ate_and_confidence(\n        data, vars, treatment_col, outcome_col, calculate_std_error=False\n    )[\"ATE\"]\n\n    for var in tqdm(data.columns, \"Processing triangle dags\"):\n        if var == treatment_col or var == outcome_col:\n            continue\n\n        # Construct the graphs to consider\n        graphs = []\n        # Second cause\n        graphs.append(\n            nx.DiGraph([(treatment_col, outcome_col), (var, outcome_col)])\n        )\n        # Confounder\n        graphs.append(\n            nx.DiGraph(\n                [\n                    (treatment_col, outcome_col),\n                    (var, treatment_col),\n                    (var, outcome_col),\n                ]\n            )\n        )\n        # Mediator with direct path\n        graphs.append(\n            nx.DiGraph(\n                [\n                    (treatment_col, outcome_col),\n                    (treatment_col, var),\n                    (var, outcome_col),\n                ]\n            )\n        )\n        # Mediator without direct path\n        graphs.append(nx.DiGraph([(treatment_col, var), (var, outcome_col)]))\n\n        # Calculate the corrsponding ATEs\n        ates = [base_ate]\n        for G in graphs:\n            try:\n                ates.append(\n                    ATECalculator.get_ate_and_confidence(\n                        data,\n                        vars,\n                        treatment_col,\n                        outcome_col,\n                        graph=G,\n                        calculate_std_error=False,\n                    )[\"ATE\"]\n                )\n            except:\n                pass\n        max_diffs[var] = max(ates) - min(ates)\n    max_diffs = max_diffs\n    df = pd.DataFrame.from_dict(max_diffs, orient=\"index\", columns=[\"max_diff\"])\n    df = df.sort_values(by=\"max_diff\", ascending=False)\n\n    Pickler.dump(df, filename)\n\n    return list(df.index[:top_n].values)\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ATECalculator","title":"<code>ATECalculator</code>","text":"<p>A class to calculate ATEs and determine the impact of adding/removing/reversing DAG edges on these calculations.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>class ATECalculator:\n    \"\"\"\n    A class to calculate ATEs and determine the impact of adding/removing/reversing DAG edges\n    on these calculations.\n    \"\"\"\n\n    @staticmethod\n    def get_ate_and_confidence(\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        treatment: str,\n        outcome: str,\n        confounder: Optional[str] = None,\n        graph: Optional[nx.DiGraph()] = None,\n        calculate_p_value: bool = True,\n        calculate_std_error: bool = True,\n        get_estimand: bool = False,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Calculate the ATE of `treatment` on `outcome`, alongside confidence measures.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n                causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n            graph: The graph to be used for causal analysis. If not specified, a two-node graph with just\n                `treatment` and `outcome` is used.\n            calculate_p_value: Whether to calculate the P-value of the ATE.\n            calculate_std_error: Whether to calculate the standard error of the ATE.\n            get_estimand: Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.\n\n        Returns:\n            A dictionary containing the ATE of `treatment` on `outcome`, alongside confidence measures. If\n            `get_estimand` is True, the estimand used to calculate the ATE is also returned.\n        \"\"\"\n\n        # If the user provided the tag of any variable, retrieve their names\n        treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n        outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n        if confounder is not None:\n            confounder = TagUtils.name_of(vars, confounder, \"prepared\")\n\n        # Should the effects be calculated based on the current partial causal graph,\n        # some other graph provided as a function parameter,\n        # or on an ad-hoc subset relevant for the question at hand?\n        if graph is None:\n            graph = nx.DiGraph()\n            graph.add_node(treatment)\n            graph.add_node(outcome)\n            graph.add_edge(treatment, outcome)\n\n            if confounder is not None:\n                graph.add_node(confounder)\n                graph.add_edge(confounder, outcome)\n                graph.add_edge(confounder, treatment)\n\n        # Use dowhy to get the ATE, P-value and standard error.\n        with open(\"/dev/null\", \"w+\") as f:\n            try:\n                with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                    model = CausalModel(\n                        data=data[list(graph.nodes)],\n                        treatment=treatment,\n                        outcome=outcome,\n                        graph=nx.nx_pydot.to_pydot(graph).to_string(),\n                    )\n                    identified_estimand = model.identify_effect(\n                        proceed_when_unidentifiable=True\n                    )\n                    estimate = model.estimate_effect(\n                        identified_estimand,\n                        method_name=\"backdoor.linear_regression\",\n                        test_significance=True,\n                    )\n                    p_value = (\n                        estimate.test_stat_significance()[\"p_value\"].astype(float)[0]\n                        if calculate_p_value\n                        else None\n                    )\n                    stderr = (\n                        estimate.get_standard_error() if calculate_std_error else None\n                    )\n                    d = {\n                        \"ATE\": float(estimate.value),\n                        \"P-value\": p_value,\n                        \"Standard Error\": stderr,\n                    }\n                    if get_estimand:\n                        d[\"Estimand\"] = identified_estimand\n                    return d\n            except:\n                raise ValueError\n\n    @staticmethod\n    def challenge_ate(\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        true_graph: Optional[nx.DiGraph],\n        treatment: str,\n        outcome: str,\n        work_dir: str,\n        num_outputs: int = 10,\n        method: str = \"step\",\n        cp: Optional[ClusteringParams] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Identify a ranked list of up to `num_outputs` possible edges among variables in the prepared log\n        which, if set to a different state (i.e. included, reversed or omitted) would most noticeably\n        impact the ATE of `treatment` on `outcome`.\n\n        Parameters:\n            data: The dataframe containing the data.\n            vars: The dataframe containing information about the variables.\n            true_graph: The starting graph to be used for causal analysis.\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            work_dir: The directory to store intermediate files in.\n            num_outputs: The number of candidate changes to output.\n            method: The method to use for ATE calculation. Can be either \"step\" or \"clustering\".\n            cp: The parameters to use for clustering. Only used if `method` is \"clustering\".\n        Returns:\n            A dataframe containing the edge changes that would most impact the ATE.\n        \"\"\"\n\n        if method == \"step\":\n            challenger = StepATEChallenger(\n                data, vars, true_graph, treatment, outcome, num_outputs\n            )\n            return challenger.challenge()\n        elif method == \"clustering\":\n            cp = cp if cp is not None else ClusteringParams()\n            challenger = ClusteringATEChallenger(\n                data, vars, true_graph, treatment, outcome, work_dir, num_outputs, cp\n            )\n            return challenger.challenge()\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n    @staticmethod\n    def _is_acceptable(graph: nx.DiGraph, treatment: str, outcome: str) -&gt; bool:\n        \"\"\"\n        Check if a graph is acceptable for ATE calculation. A graph is acceptable\n        if it is a DAG, contains the treatment and outcome variables, and has a directed path\n        from the treatment to the outcome.\n\n        Parameters:\n            graph: The graph to be checked.\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n\n        Returns:\n            Whether the graph is acceptable for ATE calculation.\n        \"\"\"\n        return (\n            nx.is_directed_acyclic_graph(graph)\n            and graph.has_node(treatment)\n            and graph.has_node(outcome)\n            and nx.has_path(graph, treatment, outcome)\n        )\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ATECalculator.get_ate_and_confidence","title":"<code>get_ate_and_confidence(data, vars, treatment, outcome, confounder=None, graph=None, calculate_p_value=True, calculate_std_error=True, get_estimand=False)</code>  <code>staticmethod</code>","text":"<p>Calculate the ATE of <code>treatment</code> on <code>outcome</code>, alongside confidence measures.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>confounder</code> <code>Optional[str]</code> <p>The name or tag of a confounder variable. If specified, overrides the current partial causal graph in favor of a three-node graph with <code>treatment</code>, <code>outcome</code> and <code>confounder</code>.</p> <code>None</code> <code>graph</code> <code>Optional[DiGraph()]</code> <p>The graph to be used for causal analysis. If not specified, a two-node graph with just <code>treatment</code> and <code>outcome</code> is used.</p> <code>None</code> <code>calculate_p_value</code> <code>bool</code> <p>Whether to calculate the P-value of the ATE.</p> <code>True</code> <code>calculate_std_error</code> <code>bool</code> <p>Whether to calculate the standard error of the ATE.</p> <code>True</code> <code>get_estimand</code> <code>bool</code> <p>Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the ATE of <code>treatment</code> on <code>outcome</code>, alongside confidence measures. If</p> <code>dict[str, Any]</code> <p><code>get_estimand</code> is True, the estimand used to calculate the ATE is also returned.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>@staticmethod\ndef get_ate_and_confidence(\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    treatment: str,\n    outcome: str,\n    confounder: Optional[str] = None,\n    graph: Optional[nx.DiGraph()] = None,\n    calculate_p_value: bool = True,\n    calculate_std_error: bool = True,\n    get_estimand: bool = False,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Calculate the ATE of `treatment` on `outcome`, alongside confidence measures.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n            causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n        graph: The graph to be used for causal analysis. If not specified, a two-node graph with just\n            `treatment` and `outcome` is used.\n        calculate_p_value: Whether to calculate the P-value of the ATE.\n        calculate_std_error: Whether to calculate the standard error of the ATE.\n        get_estimand: Whether to return the estimand used to calculate the ATE, as part of the returned dictionary.\n\n    Returns:\n        A dictionary containing the ATE of `treatment` on `outcome`, alongside confidence measures. If\n        `get_estimand` is True, the estimand used to calculate the ATE is also returned.\n    \"\"\"\n\n    # If the user provided the tag of any variable, retrieve their names\n    treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n    outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n    if confounder is not None:\n        confounder = TagUtils.name_of(vars, confounder, \"prepared\")\n\n    # Should the effects be calculated based on the current partial causal graph,\n    # some other graph provided as a function parameter,\n    # or on an ad-hoc subset relevant for the question at hand?\n    if graph is None:\n        graph = nx.DiGraph()\n        graph.add_node(treatment)\n        graph.add_node(outcome)\n        graph.add_edge(treatment, outcome)\n\n        if confounder is not None:\n            graph.add_node(confounder)\n            graph.add_edge(confounder, outcome)\n            graph.add_edge(confounder, treatment)\n\n    # Use dowhy to get the ATE, P-value and standard error.\n    with open(\"/dev/null\", \"w+\") as f:\n        try:\n            with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n                model = CausalModel(\n                    data=data[list(graph.nodes)],\n                    treatment=treatment,\n                    outcome=outcome,\n                    graph=nx.nx_pydot.to_pydot(graph).to_string(),\n                )\n                identified_estimand = model.identify_effect(\n                    proceed_when_unidentifiable=True\n                )\n                estimate = model.estimate_effect(\n                    identified_estimand,\n                    method_name=\"backdoor.linear_regression\",\n                    test_significance=True,\n                )\n                p_value = (\n                    estimate.test_stat_significance()[\"p_value\"].astype(float)[0]\n                    if calculate_p_value\n                    else None\n                )\n                stderr = (\n                    estimate.get_standard_error() if calculate_std_error else None\n                )\n                d = {\n                    \"ATE\": float(estimate.value),\n                    \"P-value\": p_value,\n                    \"Standard Error\": stderr,\n                }\n                if get_estimand:\n                    d[\"Estimand\"] = identified_estimand\n                return d\n        except:\n            raise ValueError\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ATECalculator.challenge_ate","title":"<code>challenge_ate(data, vars, true_graph, treatment, outcome, work_dir, num_outputs=10, method='step', cp=None)</code>  <code>staticmethod</code>","text":"<p>Identify a ranked list of up to <code>num_outputs</code> possible edges among variables in the prepared log which, if set to a different state (i.e. included, reversed or omitted) would most noticeably impact the ATE of <code>treatment</code> on <code>outcome</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>vars</code> <code>DataFrame</code> <p>The dataframe containing information about the variables.</p> required <code>true_graph</code> <code>Optional[DiGraph]</code> <p>The starting graph to be used for causal analysis.</p> required <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>work_dir</code> <code>str</code> <p>The directory to store intermediate files in.</p> required <code>num_outputs</code> <code>int</code> <p>The number of candidate changes to output.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use for ATE calculation. Can be either \"step\" or \"clustering\".</p> <code>'step'</code> <code>cp</code> <code>Optional[ClusteringParams]</code> <p>The parameters to use for clustering. Only used if <code>method</code> is \"clustering\".</p> <code>None</code> <p>Returns:     A dataframe containing the edge changes that would most impact the ATE.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>@staticmethod\ndef challenge_ate(\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    true_graph: Optional[nx.DiGraph],\n    treatment: str,\n    outcome: str,\n    work_dir: str,\n    num_outputs: int = 10,\n    method: str = \"step\",\n    cp: Optional[ClusteringParams] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Identify a ranked list of up to `num_outputs` possible edges among variables in the prepared log\n    which, if set to a different state (i.e. included, reversed or omitted) would most noticeably\n    impact the ATE of `treatment` on `outcome`.\n\n    Parameters:\n        data: The dataframe containing the data.\n        vars: The dataframe containing information about the variables.\n        true_graph: The starting graph to be used for causal analysis.\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        work_dir: The directory to store intermediate files in.\n        num_outputs: The number of candidate changes to output.\n        method: The method to use for ATE calculation. Can be either \"step\" or \"clustering\".\n        cp: The parameters to use for clustering. Only used if `method` is \"clustering\".\n    Returns:\n        A dataframe containing the edge changes that would most impact the ATE.\n    \"\"\"\n\n    if method == \"step\":\n        challenger = StepATEChallenger(\n            data, vars, true_graph, treatment, outcome, num_outputs\n        )\n        return challenger.challenge()\n    elif method == \"clustering\":\n        cp = cp if cp is not None else ClusteringParams()\n        challenger = ClusteringATEChallenger(\n            data, vars, true_graph, treatment, outcome, work_dir, num_outputs, cp\n        )\n        return challenger.challenge()\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ATECalculator._is_acceptable","title":"<code>_is_acceptable(graph, treatment, outcome)</code>  <code>staticmethod</code>","text":"<p>Check if a graph is acceptable for ATE calculation. A graph is acceptable if it is a DAG, contains the treatment and outcome variables, and has a directed path from the treatment to the outcome.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be checked.</p> required <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the graph is acceptable for ATE calculation.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>@staticmethod\ndef _is_acceptable(graph: nx.DiGraph, treatment: str, outcome: str) -&gt; bool:\n    \"\"\"\n    Check if a graph is acceptable for ATE calculation. A graph is acceptable\n    if it is a DAG, contains the treatment and outcome variables, and has a directed path\n    from the treatment to the outcome.\n\n    Parameters:\n        graph: The graph to be checked.\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n\n    Returns:\n        Whether the graph is acceptable for ATE calculation.\n    \"\"\"\n    return (\n        nx.is_directed_acyclic_graph(graph)\n        and graph.has_node(treatment)\n        and graph.has_node(outcome)\n        and nx.has_path(graph, treatment, outcome)\n    )\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.StepATEChallenger","title":"<code>StepATEChallenger</code>","text":"<p>A class to calculate edge changes impactful to an ATE calculation using the step method.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>class StepATEChallenger:\n    \"\"\"\n    A class to calculate edge changes impactful to an ATE calculation using the step method.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        true_graph: Optional[nx.DiGraph],\n        treatment: str,\n        outcome: str,\n        num_outputs: int = 10,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a StepATEChallenger.\n\n        Parameters:\n            data: The dataframe containing the data.\n            vars: The dataframe containing information about the variables.\n            true_graph: The starting graph to be used for causal analysis.\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            num_outputs: The number of candidate changes to output.\n        \"\"\"\n\n        self.data = data\n        self.vars = vars\n        self.true_graph = true_graph\n        self.treatment = TagUtils.name_of(self.vars, treatment, \"prepared\")\n        self.outcome = TagUtils.name_of(self.vars, outcome, \"prepared\")\n        self.num_outputs = num_outputs\n\n    def _enumerate_graphs(self) -&gt; tuple[list[dict[str, str]], list[nx.DiGraph]]:\n        \"\"\"\n        Enumerate graphs that are one edge different from the current graph,\n        either by edge inclusion or by edge direction.\n        \"\"\"\n\n        changes = []\n        graphs = []\n        for edge in self.true_graph.edges:\n            # Remove the edge\n            graph = self.true_graph.copy()\n            graph.remove_edge(*edge)\n            graphs.append(graph)\n            changes.append({\"Source\": edge[0], \"Target\": edge[1], \"Change\": \"Remove\"})\n\n            # Reverse the edge\n            graph = self.true_graph.copy()\n            graph.remove_edge(*edge)\n            graph.add_edge(*reversed(edge))\n            graphs.append(graph)\n            changes.append({\"Source\": edge[0], \"Target\": edge[1], \"Change\": \"Reverse\"})\n\n        # Add all possible edges\n        for node in self.true_graph.nodes:\n            for other_node in self.vars['Name'].values.tolist():\n                if node != other_node and not self.true_graph.has_edge(\n                    node, other_node\n                ):\n                    graph = self.true_graph.copy()\n                    graph.add_edge(node, other_node)\n                    graphs.append(graph)\n                    changes.append(\n                        {\"Source\": node, \"Target\": other_node, \"Change\": \"Add\"}\n                    )\n\n        print(f\"Enumerated {len(graphs)} graphs with {len(graph.nodes)} nodes.\")\n        return changes, graphs\n\n    def challenge(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the ATE of `treatment` on `outcome` for all possible 1-step\n        edge changes to `true_graph`, keeping the set of nodes the same.\n\n        Returns:\n            A dataframe containing the edge changes that would most impact the ATE.\n        \"\"\"\n\n        changes, graphs = self._enumerate_graphs()\n        baseline_ate = ATECalculator.get_ate_and_confidence(\n            self.data, self.vars, self.treatment, self.outcome, graph=self.true_graph\n        )\n        graph_stats = []\n        for i, graph in tqdm(enumerate(graphs)):\n            if not ATECalculator._is_acceptable(graph, self.treatment, self.outcome):\n                continue\n            d = ATECalculator.get_ate_and_confidence(\n                self.data, self.vars, self.treatment, self.outcome, graph=graph\n            )\n            d[\"Source\"] = changes[i][\"Source\"]\n            d[\"Source Tag\"] = TagUtils.get_tag(self.vars, d[\"Source\"], \"prepared\")\n            d[\"Target\"] = changes[i][\"Target\"]\n            d[\"Target Tag\"] = TagUtils.get_tag(self.vars, d[\"Target\"], \"prepared\")\n            d[\"Change\"] = changes[i][\"Change\"]\n\n            graph_stats.append(d.copy())\n\n        graphs_df = pd.DataFrame(graph_stats)\n        graphs_df[\"Baseline ATE\"] = baseline_ate[\"ATE\"]\n        graphs_df[\"ATE Ratio\"] = graphs_df.apply(\n            lambda row: max(\n                abs(row[\"ATE\"] / row[\"Baseline ATE\"]),\n                abs(row[\"Baseline ATE\"] / row[\"ATE\"]),\n            ),\n            axis=1,\n        )\n        graphs_df.sort_values(by=\"ATE Ratio\", ascending=False, inplace=True)\n\n        column_order = [\n            \"Source\",\n            \"Source Tag\",\n            \"Target\",\n            \"Target Tag\",\n            \"Change\",\n            \"ATE\",\n            \"Baseline ATE\",\n            \"ATE Ratio\",\n        ]\n\n        return graphs_df[column_order].head(self.num_outputs)\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.StepATEChallenger.__init__","title":"<code>__init__(data, vars, true_graph, treatment, outcome, num_outputs=10)</code>","text":"<p>Initializes a StepATEChallenger.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>vars</code> <code>DataFrame</code> <p>The dataframe containing information about the variables.</p> required <code>true_graph</code> <code>Optional[DiGraph]</code> <p>The starting graph to be used for causal analysis.</p> required <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>num_outputs</code> <code>int</code> <p>The number of candidate changes to output.</p> <code>10</code> Source code in <code>src/sawmill/ate.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    true_graph: Optional[nx.DiGraph],\n    treatment: str,\n    outcome: str,\n    num_outputs: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initializes a StepATEChallenger.\n\n    Parameters:\n        data: The dataframe containing the data.\n        vars: The dataframe containing information about the variables.\n        true_graph: The starting graph to be used for causal analysis.\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        num_outputs: The number of candidate changes to output.\n    \"\"\"\n\n    self.data = data\n    self.vars = vars\n    self.true_graph = true_graph\n    self.treatment = TagUtils.name_of(self.vars, treatment, \"prepared\")\n    self.outcome = TagUtils.name_of(self.vars, outcome, \"prepared\")\n    self.num_outputs = num_outputs\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.StepATEChallenger._enumerate_graphs","title":"<code>_enumerate_graphs()</code>","text":"<p>Enumerate graphs that are one edge different from the current graph, either by edge inclusion or by edge direction.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _enumerate_graphs(self) -&gt; tuple[list[dict[str, str]], list[nx.DiGraph]]:\n    \"\"\"\n    Enumerate graphs that are one edge different from the current graph,\n    either by edge inclusion or by edge direction.\n    \"\"\"\n\n    changes = []\n    graphs = []\n    for edge in self.true_graph.edges:\n        # Remove the edge\n        graph = self.true_graph.copy()\n        graph.remove_edge(*edge)\n        graphs.append(graph)\n        changes.append({\"Source\": edge[0], \"Target\": edge[1], \"Change\": \"Remove\"})\n\n        # Reverse the edge\n        graph = self.true_graph.copy()\n        graph.remove_edge(*edge)\n        graph.add_edge(*reversed(edge))\n        graphs.append(graph)\n        changes.append({\"Source\": edge[0], \"Target\": edge[1], \"Change\": \"Reverse\"})\n\n    # Add all possible edges\n    for node in self.true_graph.nodes:\n        for other_node in self.vars['Name'].values.tolist():\n            if node != other_node and not self.true_graph.has_edge(\n                node, other_node\n            ):\n                graph = self.true_graph.copy()\n                graph.add_edge(node, other_node)\n                graphs.append(graph)\n                changes.append(\n                    {\"Source\": node, \"Target\": other_node, \"Change\": \"Add\"}\n                )\n\n    print(f\"Enumerated {len(graphs)} graphs with {len(graph.nodes)} nodes.\")\n    return changes, graphs\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.StepATEChallenger.challenge","title":"<code>challenge()</code>","text":"<p>Calculate the ATE of <code>treatment</code> on <code>outcome</code> for all possible 1-step edge changes to <code>true_graph</code>, keeping the set of nodes the same.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the edge changes that would most impact the ATE.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def challenge(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the ATE of `treatment` on `outcome` for all possible 1-step\n    edge changes to `true_graph`, keeping the set of nodes the same.\n\n    Returns:\n        A dataframe containing the edge changes that would most impact the ATE.\n    \"\"\"\n\n    changes, graphs = self._enumerate_graphs()\n    baseline_ate = ATECalculator.get_ate_and_confidence(\n        self.data, self.vars, self.treatment, self.outcome, graph=self.true_graph\n    )\n    graph_stats = []\n    for i, graph in tqdm(enumerate(graphs)):\n        if not ATECalculator._is_acceptable(graph, self.treatment, self.outcome):\n            continue\n        d = ATECalculator.get_ate_and_confidence(\n            self.data, self.vars, self.treatment, self.outcome, graph=graph\n        )\n        d[\"Source\"] = changes[i][\"Source\"]\n        d[\"Source Tag\"] = TagUtils.get_tag(self.vars, d[\"Source\"], \"prepared\")\n        d[\"Target\"] = changes[i][\"Target\"]\n        d[\"Target Tag\"] = TagUtils.get_tag(self.vars, d[\"Target\"], \"prepared\")\n        d[\"Change\"] = changes[i][\"Change\"]\n\n        graph_stats.append(d.copy())\n\n    graphs_df = pd.DataFrame(graph_stats)\n    graphs_df[\"Baseline ATE\"] = baseline_ate[\"ATE\"]\n    graphs_df[\"ATE Ratio\"] = graphs_df.apply(\n        lambda row: max(\n            abs(row[\"ATE\"] / row[\"Baseline ATE\"]),\n            abs(row[\"Baseline ATE\"] / row[\"ATE\"]),\n        ),\n        axis=1,\n    )\n    graphs_df.sort_values(by=\"ATE Ratio\", ascending=False, inplace=True)\n\n    column_order = [\n        \"Source\",\n        \"Source Tag\",\n        \"Target\",\n        \"Target Tag\",\n        \"Change\",\n        \"ATE\",\n        \"Baseline ATE\",\n        \"ATE Ratio\",\n    ]\n\n    return graphs_df[column_order].head(self.num_outputs)\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.DendrogramRenderer","title":"<code>DendrogramRenderer</code>","text":"<p>A class to hold information for rendering a dendrogram.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>class DendrogramRenderer:\n    \"\"\"\n    A class to hold information for rendering a dendrogram.\n    \"\"\"\n\n    def __init__(\n        self, linkage: np.ndarray, num_clusters: int, llf: Types.LeafLabelingFunction\n    ) -&gt; None:\n        \"\"\"\n        Initializes a DendrogramRenderer object.\n\n        Parameters:\n            linkage: linkage matrix as returned by scipy.cluster.hierarchy.linkage\n            num_clusters: max num clusters to render\n            llf: leaf labeling function, from leaf index to leaf label.\n        \"\"\"\n        self.linkage = linkage\n        self.num_cluster = num_clusters\n        self.llf = llf\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.DendrogramRenderer.__init__","title":"<code>__init__(linkage, num_clusters, llf)</code>","text":"<p>Initializes a DendrogramRenderer object.</p> <p>Parameters:</p> Name Type Description Default <code>linkage</code> <code>ndarray</code> <p>linkage matrix as returned by scipy.cluster.hierarchy.linkage</p> required <code>num_clusters</code> <code>int</code> <p>max num clusters to render</p> required <code>llf</code> <code>LeafLabelingFunction</code> <p>leaf labeling function, from leaf index to leaf label.</p> required Source code in <code>src/sawmill/ate.py</code> <pre><code>def __init__(\n    self, linkage: np.ndarray, num_clusters: int, llf: Types.LeafLabelingFunction\n) -&gt; None:\n    \"\"\"\n    Initializes a DendrogramRenderer object.\n\n    Parameters:\n        linkage: linkage matrix as returned by scipy.cluster.hierarchy.linkage\n        num_clusters: max num clusters to render\n        llf: leaf labeling function, from leaf index to leaf label.\n    \"\"\"\n    self.linkage = linkage\n    self.num_cluster = num_clusters\n    self.llf = llf\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger","title":"<code>ClusteringATEChallenger</code>","text":"<p>A class to calculate edge changes impactful to an ATE calculation using the clustering method.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>class ClusteringATEChallenger:\n    \"\"\"\n    A class to calculate edge changes impactful to an ATE calculation using the clustering method.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        vars: pd.DataFrame,\n        true_graph: Optional[nx.DiGraph],\n        treatment: str,\n        outcome: str,\n        work_dir: str,\n        num_outputs: int = 10,\n        cp: Optional[ClusteringParams] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a ClusteringATEChallenger.\n\n        Parameters:\n            data: The dataframe containing the data.\n            vars: The dataframe containing information about the variables.\n            true_graph: The starting graph to be used for causal analysis.\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            work_dir: The directory to store intermediate files in.\n            num_outputs: The number of candidate changes to output.\n            cp: The parameters to use for clustering.\n        \"\"\"\n\n        self.data = data\n        self.vars = vars\n        self.true_graph = true_graph\n        self.treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n        self.outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n        self.work_dir = work_dir\n        self.num_outputs = num_outputs\n        self.cp = cp\n        if self.cp is None:\n            self.cp = ClusteringParams()\n\n    def challenge(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Use clustering to identify classes of ATEs based on presence/absence of specific\n        important edges, and return the most impactful edge changes to transition between\n        these classes.\n\n        Returns:\n            A dataframe containing the edge changes that would most impact the ATE.\n        \"\"\"\n\n        filename = os.path.join(\n            self.work_dir,\n            f\"pickles/effects/{self.treatment}_{self.outcome}_{self.cp.var_pruning_method}_{self.cp.triangle_n}.pkl\",\n        )\n\n        # Check whether we can use pre-calculated results\n        if os.path.isfile(filename) and not self.cp.force:\n            self.effects = Pickler.load(filename)\n        else:\n            nodes_to_consider = self._find_nodes_to_consider()\n            edges_to_consider = self._find_edges_to_consider(nodes_to_consider)\n            dags_to_consider = self._find_dags_to_consider(edges_to_consider)\n            self.effects = self._calculate_ates(dags_to_consider)\n            Pickler.dump(self.effects, filename)\n\n        # Cluster graphs by ATE\n        self.dendrogram = self._cluster()\n        Printer.printv(\n            f\"Successfully clustered ATEs into {self.num_clusters} with means {[val[0] for val in self.cluster_data.values()]}\\n\"\n        )\n\n        # Determine outliers\n        self.tree.count_edge_occurrences(self.treatment, self.outcome, self.true_graph)\n        self.tree.calculate_edge_expectancy()\n        self.tree.find_outliers_in_tree(self.cp.threshold)\n        self.edge_counts, self.outliers = self.tree.find_outliers_per_cluster(\n            self.true_graph\n        )\n\n        for cluster, outlier_dict in self.outliers.items():\n            Printer.printv(\n                f\"\"\"For cluster centered around {self.cluster_data[cluster][0]:.3f} with {self.cluster_data[cluster][1]} points, \"\"\"\n                \"\"\"the following edges were outliers with the corresponding frequency over/under expectation:\"\"\"\n            )\n            for outlier, percentage in outlier_dict.items():\n                Printer.printv(f\"{outlier}: {percentage * 100:.2f}%\")\n\n        # self.scored_edges = self._score_edges()\n        self._display_important_edges()\n\n    def _find_nodes_to_consider(self) -&gt; list[str]:\n        \"\"\"\n        Find the nodes to consider when enumerating DAGs under the clustering method.\n\n        Returns:\n            A list of nodes to consider.\n        \"\"\"\n\n        if self.cp.var_pruning_method is None:\n            return self.vars[\"Name\"].values.tolist()\n\n        # Remove variables related to treatment and outcome\n        nodes_to_consider = [\n            var\n            for var in vars[\"Name\"].values.tolist()\n            if self.treatment not in var and self.outcome not in var\n        ]\n\n        # Remove variables that are already in the DAG or that are timestamp variables\n        if self.true_graph is not None:\n            nodes_to_consider = [\n                var for var in nodes_to_consider if var not in self.true_graph.nodes\n            ]\n        if self.cp.ignore_ts:\n            nodes_to_consider = [\n                var for var in nodes_to_consider if \"Timestamp\" not in var\n            ]\n\n        # Prune aggregates\n        nodes_to_consider = self._prune_aggregates(nodes_to_consider)\n\n        # Prune further using the given method\n        if self.cp.var_pruning_method_type == \"lasso\":\n            nodes_to_consider = Regression.prune_with_lasso(\n                self.data, [self.outcome], top_n=self.cp.n\n            )\n        elif self.cp.var_pruning_method_type == \"triangle\":\n            nodes_to_consider = Regression.prune_with_triangle(\n                self.data,\n                self.vars,\n                self.treatment,\n                self.outcome,\n                self.work_dir,\n                top_n=self.cp.n,\n                force=self.cp.force_triangle,\n            )\n        else:\n            raise Exception(f\"Invalid prune type: {self.cp.var_pruning_method_type}\")\n        return list(set(nodes_to_consider + [self.treatment, self.outcome]))\n\n    def _find_edges_to_consider(\n        self,\n        nodes_to_consider: list[str],\n    ) -&gt; list[Types.Edge]:\n        \"\"\"\n        Find the edges to consider when enumerating DAGs under the clustering method.\n\n        Parameters:\n            nodes_to_consider: The nodes to consider.\n\n        Returns:\n            A list of edges to consider.\n        \"\"\"\n\n        # Enumerate all possible edges between nodes\n        edges_to_consider = list(combinations(nodes_to_consider, 2))\n        edges_to_consider = [\n            edge\n            for edge in edges_to_consider\n            if not PreparedVariableName.same_base_var(edge[0], edge[1])\n            and not (  # remove treatment-outcome edges of any kind\n                PreparedVariableName.same_base_var(edge[0], self.treatment)\n                and PreparedVariableName.same_base_var(edge[1], self.outcome)\n            )\n            and not (\n                PreparedVariableName.same_base_var(edge[0], self.outcome)\n                and PreparedVariableName.same_base_var(edge[1], self.treatment)\n            )\n        ]\n\n        # If partial dag is given, remove its edges (and their reverses) from consideration.\n        if self.true_graph is not None:\n            edges_to_consider = [\n                edge\n                for edge in edges_to_consider\n                if (edge not in self.true_graph.edges)\n                and (edge[::-1] not in self.true_graph.edges)\n            ]\n\n        return edges_to_consider\n\n    def _find_dags_to_consider(\n        self,\n        edges_to_consider: list[Types.Edge],\n    ) -&gt; list[nx.DiGraph]:\n        \"\"\"\n        Find the DAGs to consider when enumerating DAGs under the clustering method.\n\n        Parameters:\n            edges_to_consider: The edges to consider.\n\n        Returns:\n            A list of DAGs to consider.\n        \"\"\"\n\n        dags = []\n\n        # Enumerate all possible DAGs based on edge presence.\n        n = len(edges_to_consider)\n        k = self.cp.num_edges\n\n        # Initialize a list to store valid sequences\n        valid_sequences = []\n        stack = [([], 0, 0)]  # (sequence, non_none_count, index)\n\n        while stack:\n            sequence, count, index = stack.pop()\n            # If the sequence is complete, add it to valid_sequences\n            if index == n:\n                valid_sequences.append(tuple(sequence))\n                continue\n            stack.append((sequence + [None], count, index + 1))\n            if count &lt; k:\n                stack.append((sequence + [-1], count + 1, index + 1))\n            if count &lt; k:\n                stack.append((sequence + [1], count + 1, index + 1))\n\n        for sequence in valid_sequences:\n            G = nx.DiGraph()\n            edges = [\n                edge[::edge_dir]\n                for edge, edge_dir in zip(edges_to_consider, sequence)\n                if edge_dir\n            ]\n            # Add partial dag in\n            if self.true_graph is not None:\n                edges.extend(self.true_graph.edges)\n\n            edges.append((self.treatment, self.outcome))\n            G.add_edges_from(edges)\n\n            if nx.is_directed_acyclic_graph(G):\n                dags.append(G)\n\n        Printer.printv(f\"Found {len(dags)} potential DAGs\")\n        return dags\n\n    def _calculate_ates(\n        self,\n        dags: list[nx.DiGraph],\n    ) -&gt; dict[nx.DiGraph, tuple[float, float]]:\n        \"\"\"\n        Calculate the ATEs of `treatment` on `outcome` for all DAGs in `dags`.\n\n        Parameters:\n            dags: The DAGs to consider.\n\n        Returns:\n            A dictionary mapping DAGs to their ATEs and P-values.\n        \"\"\"\n\n        ates = {}\n        for dag in tqdm(dags, \"Processing DAGs\"):\n            results = ATECalculator.get_ate_and_confidence(\n                self.data,\n                self.vars,\n                self.treatment,\n                self.outcome,\n                graph=dag,\n                calculate_std_error=False,\n            )\n            ates[dag] = (results[\"ATE\"], results[\"P-value\"])\n        return ates\n\n    def _prune_aggregates(self, vars: list[str]) -&gt; list[str]:\n        \"\"\"\n        Prune aggregates by comparing average max abs difference between aggregates,\n        and allowing for those in the bottom 10% compared to other variables\n        to be represented by a single aggregate.\n\n        Parameters:\n            vars: The list of variables to prune.\n\n        Returns:\n            The pruned list of variables.\n        \"\"\"\n\n        print(\"Starting aggregate pruning\")\n        Printer.set_warnings_to(\"ignore\")\n        base_vars = [PreparedVariableName(var).base_var() for var in vars]\n\n        # Calculate the mean of the max abs difference for each base variable\n        mean_diffs = {\n            base_var: np.mean(\n                self.data[\n                    [\n                        column\n                        for column in self.sawmill.prepared_variable_names_with_base_x_and_no_pre_post_agg(\n                            x=base_var\n                        )\n                    ]\n                ].apply(lambda row: abs(row.max() - row.min()), axis=1)\n            )\n            for base_var in base_vars\n        }\n        mean_diffs = {key: val for key, val in mean_diffs.items() if not np.isnan(val)}\n\n        # For each base variable, calculate the mean of the absolute values of the variables\n        # with that base variable\n        means = {\n            base_var: np.mean(\n                np.abs(\n                    self.sawmill._prepared_log[\n                        [\n                            column\n                            for column in self.sawmill.prepared_variable_names_with_base_x_and_no_pre_post_agg(\n                                x=base_var\n                            )\n                        ]\n                    ].values\n                )\n            )\n            for base_var in base_vars\n        }\n\n        # normalize\n        mean_diffs = {key: val / means[key] for key, val in mean_diffs.items()}\n        mean_list = [mean for mean in mean_diffs.values() if mean != 0]\n        cutoff = np.percentile(mean_list, 10) if mean_list else 0\n        Printer.set_warnings_to(\"default\")\n\n        # Identify nodes based on the cutoff\n        to_keep = []\n        seen_base_vars = set()\n        for var in vars:\n            base_var = PreparedVariableName(var).base_var()\n\n            if base_var != \"Timestamp\":\n                if base_var not in mean_diffs or mean_diffs[base_var] &gt; cutoff:\n                    to_keep.append(var)\n                elif base_var not in seen_base_vars:\n                    print(f\"- Using {var} as only aggregate for {base_var}\")\n                    to_keep.append(var)\n                    seen_base_vars.add(base_var)\n\n        print(f\"Done pruning aggregates\")\n        return to_keep\n\n    def _cluster(\n        self,\n    ) -&gt; DendrogramRenderer:\n        \"\"\"\n        Hierarchical Clustering using \"ward\" linkage method, measuring the distance\n        between clusters based on the sum of squares within each cluster. As a result,\n        it forms a hierarchy of clusters where data points are initially treated as individual\n        clusters and then merged based on their similarity.\n\n        Determine Optimal Clusters by calculate inconsistencies, which represent how dissimilar\n        the merged clusters are. By comparing inconsistencies, you determine a threshold that helps\n        you identify the optimal number of clusters. This threshold is based on a threshold multiplier\n        (e.g., 1.0 times the maximum inconsistency)\n\n        Returns:\n            A DendrogramRenderer containing the dendrogram.\n        \"\"\"\n\n        # Extract ATEs from effects and create linkage matrix\n        ates = [val[0] for val in self.effects.values()]\n        data = np.array(ates).reshape(-1, 1)\n        linked: np.ndarray = linkage(data, method=\"ward\")\n\n        # Perform clustering\n        if not self.cp.num_clusters:\n            # If no number of clusters is specified, find the optimal number\n            distances = linked[:, 2]\n            threshold_multiplier = 0.5\n            optimal_clusters = fcluster(\n                linked, t=threshold_multiplier * np.max(distances), criterion=\"distance\"\n            )\n        else:\n            optimal_clusters = fcluster(\n                linked, t=self.cp.num_clusters, criterion=\"maxclust\"\n            )\n\n        self.num_clusters = int(np.max(optimal_clusters))\n        print(f\"Found {self.num_clusters} clusters\")\n\n        # Save cluster mappings\n        optimal_clusters = np.array(optimal_clusters - 1)  # make it 0-indexed\n        cluster_mapping = {\n            dag: cluster for dag, cluster in zip(self.effects.keys(), optimal_clusters)\n        }\n        self.cluster_data = {\n            i: (np.mean(data[optimal_clusters == i]), len(data[optimal_clusters == i]))\n            for i in range(self.num_clusters)\n        }\n\n        # Create the dendrogram with the number of clusters we found, using 'lastp' truncation\n        R = dendrogram(\n            linked,\n            orientation=\"top\",\n            labels=None,\n            show_leaf_counts=False,\n            truncate_mode=\"lastp\",\n            p=self.num_clusters,\n            leaf_rotation=90,\n            no_plot=True,\n        )\n\n        label_mapping = {\n            R[\"leaves\"][i]: self.cluster_data[i] for i in range(len(R[\"leaves\"]))\n        }\n        self.tree, _ = EdgeOccurrenceTree.build_tree(linked, R[\"leaves\"])\n        self.tree.assign_dags_to_nodes(cluster_mapping)\n\n        return DendrogramRenderer(\n            linked,\n            self.num_clusters,\n            lambda x: \"Mean: {:.3f}\\n Count: {}\".format(\n                label_mapping[x][0], label_mapping[x][1], no_plot=True\n            ),\n        )\n\n    def _display_important_edges(self):\n        \"\"\"\n        Displays the results of the edge analysis.\n\n        \"\"\"\n\n        # display ATE histogram and dendrogram\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n        ates = [val[0] for val in self.effects.values()]\n        ax1.hist(ates)\n        ax1.set_xlabel(\"ATE\")\n        ax1.set_ylabel(\"Frequency\")\n        ax1.set_title(\"Distribution of ATE Values\")\n        dendrogram(\n            self.dendrogram.linkage,\n            orientation=\"top\",\n            labels=None,\n            distance_sort=\"ascending\",\n            show_leaf_counts=True,\n            truncate_mode=\"lastp\",\n            p=self.dendrogram.num_cluster,\n            leaf_label_func=self.dendrogram.llf,\n            ax=ax2,\n            leaf_font_size=6,\n        )\n        ax2.set_title(\"Hierarchical clustering of ATE\")\n\n        # display edge frequency for each cluster\n        fig, axes = plt.subplots(1, len(self.edge_counts.keys()), figsize=(12, 5))\n        sorted_edge_keys = sorted(\n            self.edge_counts[0].keys(), key=lambda k: self.edge_counts[0][k]\n        )\n        named_edge_keys = [\n            (\n                TagUtils.get_tag(self.vars, edge1, \"prepared\"),\n                TagUtils.get_tag(self.vars, edge2, \"prepared\"),\n            )\n            for (edge1, edge2) in sorted_edge_keys\n        ]\n\n        for i, ax in enumerate(axes):\n            edge_counts = {key: self.edge_counts[i][key] for key in sorted_edge_keys}\n            ax.bar(range(len(sorted_edge_keys)), edge_counts.values())\n            ax.set_xticks(range(len(sorted_edge_keys)))\n            ax.set_xticklabels(named_edge_keys, rotation=90)\n            ax.tick_params(axis=\"x\", labelsize=8)\n            ax.set_xlabel(\"Edges\")\n            ax.set_ylabel(\"Frequency\")\n            ax.set_title(\n                \"Edge counts for cluster centered at {:.3f}\".format(\n                    self.cluster_data[i][0]\n                )\n            )\n        fig.tight_layout()\n\n        for cluster_id, outlier in self.outliers.items():\n            ate = self.cluster_data[cluster_id][0]\n            df = pd.DataFrame.from_dict(\n                {\n                    \"Edge\": [\n                        (\n                            TagUtils.get_tag(self.vars, edge[0], \"prepared\"),\n                            TagUtils.get_tag(self.vars, edge[1], \"prepared\"),\n                        )\n                        for edge in outlier.keys()\n                    ],\n                    \"% Expectancy\": list(outlier.values()),\n                }\n            )\n            df[\"Status\"] = np.where(df[\"% Expectancy\"] &gt; 0, \"EXISTS\", \"DOES NOT EXIST\")\n            if self.cp.top_n:\n                df_top = (\n                    df[df[\"% Expectancy\"] &gt; 0]\n                    .nlargest(self.cp.top_n, \"% Expectancy\")\n                    .sort_values(by=\"% Expectancy\", ascending=False)\n                )\n                df_bottom = (\n                    df[df[\"% Expectancy\"] &lt; 0]\n                    .nsmallest(self.cp.top_n, \"% Expectancy\")\n                    .sort_values(by=\"% Expectancy\")\n                )\n                df = pd.concat([df_top, df_bottom])\n            else:\n                df_top = df[df[\"% Expectancy\"] &gt; 0].sort_values(\n                    by=\"% Expectancy\", ascending=False\n                )\n                df_bottom = df[df[\"% Expectancy\"] &lt; 0].sort_values(by=\"% Expectancy\")\n                df = pd.concat([df_top, df_bottom])\n            print(\n                f\"For ate = {ate}, the following edges are key assumptions made of the causal graph.\"\n            )\n            display(df)\n\n    def _score_edges(self, graph, display_df=True, top_n=None):\n        \"\"\"\n        TODO how do we want this for the paper?! Score the top_n edges we found\n        according to the true graph by ___\n        \"\"\"\n\n        # convert everything\n        sawmill_graph = nx.DiGraph(\n            [\n                (\n                    TagUtils.name_of(self.vars, edge[0], \"prepared\"),\n                    TagUtils.name_of(self.vars, edge[1], \"prepared\"),\n                )\n                for edge in graph.edges\n            ]\n        )\n        treatment = TagUtils.name_of(self.vars, self.treatment, \"prepared\")\n        outcome = TagUtils.name_of(self.vars, self.outcome, \"prepared\")\n\n        # identify backdoor nodes\n        influential_nodes = set()\n        ate_and_confidence = ATECalculator.get_ate_and_confidence(\n            self.data,\n            self.vars,\n            treatment=treatment,\n            outcome=outcome,\n            graph=sawmill_graph,\n            calculate_std_error=False,\n            get_estimand=True,\n        )\n        ate = ate_and_confidence[\"ATE\"]\n        identified_estimand = ate_and_confidence[\"Estimand\"]\n        for _, vars in identified_estimand.backdoor_variables.items():\n            influential_nodes = influential_nodes.union(vars)\n        # identify backdoor edge\n        scored_edges = []\n        for edge in sawmill_graph.edges:\n            if edge[0] in influential_nodes and (\n                nx.has_path(sawmill_graph, edge[1], treatment)\n                or nx.has_path(sawmill_graph, edge[1], outcome)\n            ):\n                scored_edges.append(edge)\n\n        if top_n is None:\n            top_n = len(scored_edges)\n\n        # build up user graph according to outlier suggestions\n        user_graph = nx.DiGraph([(treatment, outcome)])\n        max_outliers = {}  # maps outliers to the max abs percent difference\n        for _, outliers in self.outliers.items():\n            for outlier in outliers:\n                max_outliers[outlier] = max(\n                    max_outliers.get(outlier, 0), np.abs(outliers[outlier])\n                )\n        top_outliers = list(outliers.keys())\n        top_outliers.sort(key=lambda x: max_outliers[x], reverse=True)\n        top_outliers = top_outliers[:top_n]\n        user_graph.add_edges_from(\n            [edge for edge in top_outliers if edge in sawmill_graph.edges]\n        )\n\n        found_edges = [\n            1 if scored_edges[i] in user_graph.edges else 0\n            for i in range(len(scored_edges))\n        ]\n        if display_df:\n            df = pd.DataFrame.from_dict(\n                {\n                    \"Edge\": [\n                        (\n                            TagUtils.tag_of(self.data, edge[0], \"prepared\"),\n                            TagUtils.tag_of(self.data, edge[1], \"prepared\"),\n                        )\n                        for edge in scored_edges\n                    ],\n                    \"Score\": found_edges,\n                }\n            ).sort_values(by=\"Score\", ascending=False)\n            display(df)\n        edge_score = sum(found_edges) / len(found_edges)\n\n        # ate score\n        user_ate = ATECalculator.get_ate_and_confidence(\n            self.data,\n            self.vars,\n            treatment=treatment,\n            outcome=outcome,\n            graph=user_graph,\n            calculate_std_error=False,\n        )[\"ATE\"]\n        ate_score = np.abs(ate - user_ate)\n        return edge_score, ate_score\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger.__init__","title":"<code>__init__(data, vars, true_graph, treatment, outcome, work_dir, num_outputs=10, cp=None)</code>","text":"<p>Initializes a ClusteringATEChallenger.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataframe containing the data.</p> required <code>vars</code> <code>DataFrame</code> <p>The dataframe containing information about the variables.</p> required <code>true_graph</code> <code>Optional[DiGraph]</code> <p>The starting graph to be used for causal analysis.</p> required <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>work_dir</code> <code>str</code> <p>The directory to store intermediate files in.</p> required <code>num_outputs</code> <code>int</code> <p>The number of candidate changes to output.</p> <code>10</code> <code>cp</code> <code>Optional[ClusteringParams]</code> <p>The parameters to use for clustering.</p> <code>None</code> Source code in <code>src/sawmill/ate.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    vars: pd.DataFrame,\n    true_graph: Optional[nx.DiGraph],\n    treatment: str,\n    outcome: str,\n    work_dir: str,\n    num_outputs: int = 10,\n    cp: Optional[ClusteringParams] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a ClusteringATEChallenger.\n\n    Parameters:\n        data: The dataframe containing the data.\n        vars: The dataframe containing information about the variables.\n        true_graph: The starting graph to be used for causal analysis.\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        work_dir: The directory to store intermediate files in.\n        num_outputs: The number of candidate changes to output.\n        cp: The parameters to use for clustering.\n    \"\"\"\n\n    self.data = data\n    self.vars = vars\n    self.true_graph = true_graph\n    self.treatment = TagUtils.name_of(vars, treatment, \"prepared\")\n    self.outcome = TagUtils.name_of(vars, outcome, \"prepared\")\n    self.work_dir = work_dir\n    self.num_outputs = num_outputs\n    self.cp = cp\n    if self.cp is None:\n        self.cp = ClusteringParams()\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger.challenge","title":"<code>challenge()</code>","text":"<p>Use clustering to identify classes of ATEs based on presence/absence of specific important edges, and return the most impactful edge changes to transition between these classes.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the edge changes that would most impact the ATE.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def challenge(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Use clustering to identify classes of ATEs based on presence/absence of specific\n    important edges, and return the most impactful edge changes to transition between\n    these classes.\n\n    Returns:\n        A dataframe containing the edge changes that would most impact the ATE.\n    \"\"\"\n\n    filename = os.path.join(\n        self.work_dir,\n        f\"pickles/effects/{self.treatment}_{self.outcome}_{self.cp.var_pruning_method}_{self.cp.triangle_n}.pkl\",\n    )\n\n    # Check whether we can use pre-calculated results\n    if os.path.isfile(filename) and not self.cp.force:\n        self.effects = Pickler.load(filename)\n    else:\n        nodes_to_consider = self._find_nodes_to_consider()\n        edges_to_consider = self._find_edges_to_consider(nodes_to_consider)\n        dags_to_consider = self._find_dags_to_consider(edges_to_consider)\n        self.effects = self._calculate_ates(dags_to_consider)\n        Pickler.dump(self.effects, filename)\n\n    # Cluster graphs by ATE\n    self.dendrogram = self._cluster()\n    Printer.printv(\n        f\"Successfully clustered ATEs into {self.num_clusters} with means {[val[0] for val in self.cluster_data.values()]}\\n\"\n    )\n\n    # Determine outliers\n    self.tree.count_edge_occurrences(self.treatment, self.outcome, self.true_graph)\n    self.tree.calculate_edge_expectancy()\n    self.tree.find_outliers_in_tree(self.cp.threshold)\n    self.edge_counts, self.outliers = self.tree.find_outliers_per_cluster(\n        self.true_graph\n    )\n\n    for cluster, outlier_dict in self.outliers.items():\n        Printer.printv(\n            f\"\"\"For cluster centered around {self.cluster_data[cluster][0]:.3f} with {self.cluster_data[cluster][1]} points, \"\"\"\n            \"\"\"the following edges were outliers with the corresponding frequency over/under expectation:\"\"\"\n        )\n        for outlier, percentage in outlier_dict.items():\n            Printer.printv(f\"{outlier}: {percentage * 100:.2f}%\")\n\n    # self.scored_edges = self._score_edges()\n    self._display_important_edges()\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._find_nodes_to_consider","title":"<code>_find_nodes_to_consider()</code>","text":"<p>Find the nodes to consider when enumerating DAGs under the clustering method.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of nodes to consider.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _find_nodes_to_consider(self) -&gt; list[str]:\n    \"\"\"\n    Find the nodes to consider when enumerating DAGs under the clustering method.\n\n    Returns:\n        A list of nodes to consider.\n    \"\"\"\n\n    if self.cp.var_pruning_method is None:\n        return self.vars[\"Name\"].values.tolist()\n\n    # Remove variables related to treatment and outcome\n    nodes_to_consider = [\n        var\n        for var in vars[\"Name\"].values.tolist()\n        if self.treatment not in var and self.outcome not in var\n    ]\n\n    # Remove variables that are already in the DAG or that are timestamp variables\n    if self.true_graph is not None:\n        nodes_to_consider = [\n            var for var in nodes_to_consider if var not in self.true_graph.nodes\n        ]\n    if self.cp.ignore_ts:\n        nodes_to_consider = [\n            var for var in nodes_to_consider if \"Timestamp\" not in var\n        ]\n\n    # Prune aggregates\n    nodes_to_consider = self._prune_aggregates(nodes_to_consider)\n\n    # Prune further using the given method\n    if self.cp.var_pruning_method_type == \"lasso\":\n        nodes_to_consider = Regression.prune_with_lasso(\n            self.data, [self.outcome], top_n=self.cp.n\n        )\n    elif self.cp.var_pruning_method_type == \"triangle\":\n        nodes_to_consider = Regression.prune_with_triangle(\n            self.data,\n            self.vars,\n            self.treatment,\n            self.outcome,\n            self.work_dir,\n            top_n=self.cp.n,\n            force=self.cp.force_triangle,\n        )\n    else:\n        raise Exception(f\"Invalid prune type: {self.cp.var_pruning_method_type}\")\n    return list(set(nodes_to_consider + [self.treatment, self.outcome]))\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._find_edges_to_consider","title":"<code>_find_edges_to_consider(nodes_to_consider)</code>","text":"<p>Find the edges to consider when enumerating DAGs under the clustering method.</p> <p>Parameters:</p> Name Type Description Default <code>nodes_to_consider</code> <code>list[str]</code> <p>The nodes to consider.</p> required <p>Returns:</p> Type Description <code>list[Edge]</code> <p>A list of edges to consider.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _find_edges_to_consider(\n    self,\n    nodes_to_consider: list[str],\n) -&gt; list[Types.Edge]:\n    \"\"\"\n    Find the edges to consider when enumerating DAGs under the clustering method.\n\n    Parameters:\n        nodes_to_consider: The nodes to consider.\n\n    Returns:\n        A list of edges to consider.\n    \"\"\"\n\n    # Enumerate all possible edges between nodes\n    edges_to_consider = list(combinations(nodes_to_consider, 2))\n    edges_to_consider = [\n        edge\n        for edge in edges_to_consider\n        if not PreparedVariableName.same_base_var(edge[0], edge[1])\n        and not (  # remove treatment-outcome edges of any kind\n            PreparedVariableName.same_base_var(edge[0], self.treatment)\n            and PreparedVariableName.same_base_var(edge[1], self.outcome)\n        )\n        and not (\n            PreparedVariableName.same_base_var(edge[0], self.outcome)\n            and PreparedVariableName.same_base_var(edge[1], self.treatment)\n        )\n    ]\n\n    # If partial dag is given, remove its edges (and their reverses) from consideration.\n    if self.true_graph is not None:\n        edges_to_consider = [\n            edge\n            for edge in edges_to_consider\n            if (edge not in self.true_graph.edges)\n            and (edge[::-1] not in self.true_graph.edges)\n        ]\n\n    return edges_to_consider\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._find_dags_to_consider","title":"<code>_find_dags_to_consider(edges_to_consider)</code>","text":"<p>Find the DAGs to consider when enumerating DAGs under the clustering method.</p> <p>Parameters:</p> Name Type Description Default <code>edges_to_consider</code> <code>list[Edge]</code> <p>The edges to consider.</p> required <p>Returns:</p> Type Description <code>list[DiGraph]</code> <p>A list of DAGs to consider.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _find_dags_to_consider(\n    self,\n    edges_to_consider: list[Types.Edge],\n) -&gt; list[nx.DiGraph]:\n    \"\"\"\n    Find the DAGs to consider when enumerating DAGs under the clustering method.\n\n    Parameters:\n        edges_to_consider: The edges to consider.\n\n    Returns:\n        A list of DAGs to consider.\n    \"\"\"\n\n    dags = []\n\n    # Enumerate all possible DAGs based on edge presence.\n    n = len(edges_to_consider)\n    k = self.cp.num_edges\n\n    # Initialize a list to store valid sequences\n    valid_sequences = []\n    stack = [([], 0, 0)]  # (sequence, non_none_count, index)\n\n    while stack:\n        sequence, count, index = stack.pop()\n        # If the sequence is complete, add it to valid_sequences\n        if index == n:\n            valid_sequences.append(tuple(sequence))\n            continue\n        stack.append((sequence + [None], count, index + 1))\n        if count &lt; k:\n            stack.append((sequence + [-1], count + 1, index + 1))\n        if count &lt; k:\n            stack.append((sequence + [1], count + 1, index + 1))\n\n    for sequence in valid_sequences:\n        G = nx.DiGraph()\n        edges = [\n            edge[::edge_dir]\n            for edge, edge_dir in zip(edges_to_consider, sequence)\n            if edge_dir\n        ]\n        # Add partial dag in\n        if self.true_graph is not None:\n            edges.extend(self.true_graph.edges)\n\n        edges.append((self.treatment, self.outcome))\n        G.add_edges_from(edges)\n\n        if nx.is_directed_acyclic_graph(G):\n            dags.append(G)\n\n    Printer.printv(f\"Found {len(dags)} potential DAGs\")\n    return dags\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._calculate_ates","title":"<code>_calculate_ates(dags)</code>","text":"<p>Calculate the ATEs of <code>treatment</code> on <code>outcome</code> for all DAGs in <code>dags</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dags</code> <code>list[DiGraph]</code> <p>The DAGs to consider.</p> required <p>Returns:</p> Type Description <code>dict[DiGraph, tuple[float, float]]</code> <p>A dictionary mapping DAGs to their ATEs and P-values.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _calculate_ates(\n    self,\n    dags: list[nx.DiGraph],\n) -&gt; dict[nx.DiGraph, tuple[float, float]]:\n    \"\"\"\n    Calculate the ATEs of `treatment` on `outcome` for all DAGs in `dags`.\n\n    Parameters:\n        dags: The DAGs to consider.\n\n    Returns:\n        A dictionary mapping DAGs to their ATEs and P-values.\n    \"\"\"\n\n    ates = {}\n    for dag in tqdm(dags, \"Processing DAGs\"):\n        results = ATECalculator.get_ate_and_confidence(\n            self.data,\n            self.vars,\n            self.treatment,\n            self.outcome,\n            graph=dag,\n            calculate_std_error=False,\n        )\n        ates[dag] = (results[\"ATE\"], results[\"P-value\"])\n    return ates\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._prune_aggregates","title":"<code>_prune_aggregates(vars)</code>","text":"<p>Prune aggregates by comparing average max abs difference between aggregates, and allowing for those in the bottom 10% compared to other variables to be represented by a single aggregate.</p> <p>Parameters:</p> Name Type Description Default <code>vars</code> <code>list[str]</code> <p>The list of variables to prune.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The pruned list of variables.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _prune_aggregates(self, vars: list[str]) -&gt; list[str]:\n    \"\"\"\n    Prune aggregates by comparing average max abs difference between aggregates,\n    and allowing for those in the bottom 10% compared to other variables\n    to be represented by a single aggregate.\n\n    Parameters:\n        vars: The list of variables to prune.\n\n    Returns:\n        The pruned list of variables.\n    \"\"\"\n\n    print(\"Starting aggregate pruning\")\n    Printer.set_warnings_to(\"ignore\")\n    base_vars = [PreparedVariableName(var).base_var() for var in vars]\n\n    # Calculate the mean of the max abs difference for each base variable\n    mean_diffs = {\n        base_var: np.mean(\n            self.data[\n                [\n                    column\n                    for column in self.sawmill.prepared_variable_names_with_base_x_and_no_pre_post_agg(\n                        x=base_var\n                    )\n                ]\n            ].apply(lambda row: abs(row.max() - row.min()), axis=1)\n        )\n        for base_var in base_vars\n    }\n    mean_diffs = {key: val for key, val in mean_diffs.items() if not np.isnan(val)}\n\n    # For each base variable, calculate the mean of the absolute values of the variables\n    # with that base variable\n    means = {\n        base_var: np.mean(\n            np.abs(\n                self.sawmill._prepared_log[\n                    [\n                        column\n                        for column in self.sawmill.prepared_variable_names_with_base_x_and_no_pre_post_agg(\n                            x=base_var\n                        )\n                    ]\n                ].values\n            )\n        )\n        for base_var in base_vars\n    }\n\n    # normalize\n    mean_diffs = {key: val / means[key] for key, val in mean_diffs.items()}\n    mean_list = [mean for mean in mean_diffs.values() if mean != 0]\n    cutoff = np.percentile(mean_list, 10) if mean_list else 0\n    Printer.set_warnings_to(\"default\")\n\n    # Identify nodes based on the cutoff\n    to_keep = []\n    seen_base_vars = set()\n    for var in vars:\n        base_var = PreparedVariableName(var).base_var()\n\n        if base_var != \"Timestamp\":\n            if base_var not in mean_diffs or mean_diffs[base_var] &gt; cutoff:\n                to_keep.append(var)\n            elif base_var not in seen_base_vars:\n                print(f\"- Using {var} as only aggregate for {base_var}\")\n                to_keep.append(var)\n                seen_base_vars.add(base_var)\n\n    print(f\"Done pruning aggregates\")\n    return to_keep\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._cluster","title":"<code>_cluster()</code>","text":"<p>Hierarchical Clustering using \"ward\" linkage method, measuring the distance between clusters based on the sum of squares within each cluster. As a result, it forms a hierarchy of clusters where data points are initially treated as individual clusters and then merged based on their similarity.</p> <p>Determine Optimal Clusters by calculate inconsistencies, which represent how dissimilar the merged clusters are. By comparing inconsistencies, you determine a threshold that helps you identify the optimal number of clusters. This threshold is based on a threshold multiplier (e.g., 1.0 times the maximum inconsistency)</p> <p>Returns:</p> Type Description <code>DendrogramRenderer</code> <p>A DendrogramRenderer containing the dendrogram.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _cluster(\n    self,\n) -&gt; DendrogramRenderer:\n    \"\"\"\n    Hierarchical Clustering using \"ward\" linkage method, measuring the distance\n    between clusters based on the sum of squares within each cluster. As a result,\n    it forms a hierarchy of clusters where data points are initially treated as individual\n    clusters and then merged based on their similarity.\n\n    Determine Optimal Clusters by calculate inconsistencies, which represent how dissimilar\n    the merged clusters are. By comparing inconsistencies, you determine a threshold that helps\n    you identify the optimal number of clusters. This threshold is based on a threshold multiplier\n    (e.g., 1.0 times the maximum inconsistency)\n\n    Returns:\n        A DendrogramRenderer containing the dendrogram.\n    \"\"\"\n\n    # Extract ATEs from effects and create linkage matrix\n    ates = [val[0] for val in self.effects.values()]\n    data = np.array(ates).reshape(-1, 1)\n    linked: np.ndarray = linkage(data, method=\"ward\")\n\n    # Perform clustering\n    if not self.cp.num_clusters:\n        # If no number of clusters is specified, find the optimal number\n        distances = linked[:, 2]\n        threshold_multiplier = 0.5\n        optimal_clusters = fcluster(\n            linked, t=threshold_multiplier * np.max(distances), criterion=\"distance\"\n        )\n    else:\n        optimal_clusters = fcluster(\n            linked, t=self.cp.num_clusters, criterion=\"maxclust\"\n        )\n\n    self.num_clusters = int(np.max(optimal_clusters))\n    print(f\"Found {self.num_clusters} clusters\")\n\n    # Save cluster mappings\n    optimal_clusters = np.array(optimal_clusters - 1)  # make it 0-indexed\n    cluster_mapping = {\n        dag: cluster for dag, cluster in zip(self.effects.keys(), optimal_clusters)\n    }\n    self.cluster_data = {\n        i: (np.mean(data[optimal_clusters == i]), len(data[optimal_clusters == i]))\n        for i in range(self.num_clusters)\n    }\n\n    # Create the dendrogram with the number of clusters we found, using 'lastp' truncation\n    R = dendrogram(\n        linked,\n        orientation=\"top\",\n        labels=None,\n        show_leaf_counts=False,\n        truncate_mode=\"lastp\",\n        p=self.num_clusters,\n        leaf_rotation=90,\n        no_plot=True,\n    )\n\n    label_mapping = {\n        R[\"leaves\"][i]: self.cluster_data[i] for i in range(len(R[\"leaves\"]))\n    }\n    self.tree, _ = EdgeOccurrenceTree.build_tree(linked, R[\"leaves\"])\n    self.tree.assign_dags_to_nodes(cluster_mapping)\n\n    return DendrogramRenderer(\n        linked,\n        self.num_clusters,\n        lambda x: \"Mean: {:.3f}\\n Count: {}\".format(\n            label_mapping[x][0], label_mapping[x][1], no_plot=True\n        ),\n    )\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._display_important_edges","title":"<code>_display_important_edges()</code>","text":"<p>Displays the results of the edge analysis.</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _display_important_edges(self):\n    \"\"\"\n    Displays the results of the edge analysis.\n\n    \"\"\"\n\n    # display ATE histogram and dendrogram\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ates = [val[0] for val in self.effects.values()]\n    ax1.hist(ates)\n    ax1.set_xlabel(\"ATE\")\n    ax1.set_ylabel(\"Frequency\")\n    ax1.set_title(\"Distribution of ATE Values\")\n    dendrogram(\n        self.dendrogram.linkage,\n        orientation=\"top\",\n        labels=None,\n        distance_sort=\"ascending\",\n        show_leaf_counts=True,\n        truncate_mode=\"lastp\",\n        p=self.dendrogram.num_cluster,\n        leaf_label_func=self.dendrogram.llf,\n        ax=ax2,\n        leaf_font_size=6,\n    )\n    ax2.set_title(\"Hierarchical clustering of ATE\")\n\n    # display edge frequency for each cluster\n    fig, axes = plt.subplots(1, len(self.edge_counts.keys()), figsize=(12, 5))\n    sorted_edge_keys = sorted(\n        self.edge_counts[0].keys(), key=lambda k: self.edge_counts[0][k]\n    )\n    named_edge_keys = [\n        (\n            TagUtils.get_tag(self.vars, edge1, \"prepared\"),\n            TagUtils.get_tag(self.vars, edge2, \"prepared\"),\n        )\n        for (edge1, edge2) in sorted_edge_keys\n    ]\n\n    for i, ax in enumerate(axes):\n        edge_counts = {key: self.edge_counts[i][key] for key in sorted_edge_keys}\n        ax.bar(range(len(sorted_edge_keys)), edge_counts.values())\n        ax.set_xticks(range(len(sorted_edge_keys)))\n        ax.set_xticklabels(named_edge_keys, rotation=90)\n        ax.tick_params(axis=\"x\", labelsize=8)\n        ax.set_xlabel(\"Edges\")\n        ax.set_ylabel(\"Frequency\")\n        ax.set_title(\n            \"Edge counts for cluster centered at {:.3f}\".format(\n                self.cluster_data[i][0]\n            )\n        )\n    fig.tight_layout()\n\n    for cluster_id, outlier in self.outliers.items():\n        ate = self.cluster_data[cluster_id][0]\n        df = pd.DataFrame.from_dict(\n            {\n                \"Edge\": [\n                    (\n                        TagUtils.get_tag(self.vars, edge[0], \"prepared\"),\n                        TagUtils.get_tag(self.vars, edge[1], \"prepared\"),\n                    )\n                    for edge in outlier.keys()\n                ],\n                \"% Expectancy\": list(outlier.values()),\n            }\n        )\n        df[\"Status\"] = np.where(df[\"% Expectancy\"] &gt; 0, \"EXISTS\", \"DOES NOT EXIST\")\n        if self.cp.top_n:\n            df_top = (\n                df[df[\"% Expectancy\"] &gt; 0]\n                .nlargest(self.cp.top_n, \"% Expectancy\")\n                .sort_values(by=\"% Expectancy\", ascending=False)\n            )\n            df_bottom = (\n                df[df[\"% Expectancy\"] &lt; 0]\n                .nsmallest(self.cp.top_n, \"% Expectancy\")\n                .sort_values(by=\"% Expectancy\")\n            )\n            df = pd.concat([df_top, df_bottom])\n        else:\n            df_top = df[df[\"% Expectancy\"] &gt; 0].sort_values(\n                by=\"% Expectancy\", ascending=False\n            )\n            df_bottom = df[df[\"% Expectancy\"] &lt; 0].sort_values(by=\"% Expectancy\")\n            df = pd.concat([df_top, df_bottom])\n        print(\n            f\"For ate = {ate}, the following edges are key assumptions made of the causal graph.\"\n        )\n        display(df)\n</code></pre>"},{"location":"reference/src/sawmill/ate/#src.sawmill.ate.ClusteringATEChallenger._score_edges","title":"<code>_score_edges(graph, display_df=True, top_n=None)</code>","text":"<p>TODO how do we want this for the paper?! Score the top_n edges we found according to the true graph by ___</p> Source code in <code>src/sawmill/ate.py</code> <pre><code>def _score_edges(self, graph, display_df=True, top_n=None):\n    \"\"\"\n    TODO how do we want this for the paper?! Score the top_n edges we found\n    according to the true graph by ___\n    \"\"\"\n\n    # convert everything\n    sawmill_graph = nx.DiGraph(\n        [\n            (\n                TagUtils.name_of(self.vars, edge[0], \"prepared\"),\n                TagUtils.name_of(self.vars, edge[1], \"prepared\"),\n            )\n            for edge in graph.edges\n        ]\n    )\n    treatment = TagUtils.name_of(self.vars, self.treatment, \"prepared\")\n    outcome = TagUtils.name_of(self.vars, self.outcome, \"prepared\")\n\n    # identify backdoor nodes\n    influential_nodes = set()\n    ate_and_confidence = ATECalculator.get_ate_and_confidence(\n        self.data,\n        self.vars,\n        treatment=treatment,\n        outcome=outcome,\n        graph=sawmill_graph,\n        calculate_std_error=False,\n        get_estimand=True,\n    )\n    ate = ate_and_confidence[\"ATE\"]\n    identified_estimand = ate_and_confidence[\"Estimand\"]\n    for _, vars in identified_estimand.backdoor_variables.items():\n        influential_nodes = influential_nodes.union(vars)\n    # identify backdoor edge\n    scored_edges = []\n    for edge in sawmill_graph.edges:\n        if edge[0] in influential_nodes and (\n            nx.has_path(sawmill_graph, edge[1], treatment)\n            or nx.has_path(sawmill_graph, edge[1], outcome)\n        ):\n            scored_edges.append(edge)\n\n    if top_n is None:\n        top_n = len(scored_edges)\n\n    # build up user graph according to outlier suggestions\n    user_graph = nx.DiGraph([(treatment, outcome)])\n    max_outliers = {}  # maps outliers to the max abs percent difference\n    for _, outliers in self.outliers.items():\n        for outlier in outliers:\n            max_outliers[outlier] = max(\n                max_outliers.get(outlier, 0), np.abs(outliers[outlier])\n            )\n    top_outliers = list(outliers.keys())\n    top_outliers.sort(key=lambda x: max_outliers[x], reverse=True)\n    top_outliers = top_outliers[:top_n]\n    user_graph.add_edges_from(\n        [edge for edge in top_outliers if edge in sawmill_graph.edges]\n    )\n\n    found_edges = [\n        1 if scored_edges[i] in user_graph.edges else 0\n        for i in range(len(scored_edges))\n    ]\n    if display_df:\n        df = pd.DataFrame.from_dict(\n            {\n                \"Edge\": [\n                    (\n                        TagUtils.tag_of(self.data, edge[0], \"prepared\"),\n                        TagUtils.tag_of(self.data, edge[1], \"prepared\"),\n                    )\n                    for edge in scored_edges\n                ],\n                \"Score\": found_edges,\n            }\n        ).sort_values(by=\"Score\", ascending=False)\n        display(df)\n    edge_score = sum(found_edges) / len(found_edges)\n\n    # ate score\n    user_ate = ATECalculator.get_ate_and_confidence(\n        self.data,\n        self.vars,\n        treatment=treatment,\n        outcome=outcome,\n        graph=user_graph,\n        calculate_std_error=False,\n    )[\"ATE\"]\n    ate_score = np.abs(ate - user_ate)\n    return edge_score, ate_score\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/","title":"CausalDiscoverer","text":""},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer","title":"<code>CausalDiscoverer</code>","text":"<p>Provides various methods for automatic causal discovery based on a dataframe.</p> <p>Within Sawmill, the expectation is that the passed dataframe will contain the prepared variables.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>class CausalDiscoverer:\n    \"\"\"\n    Provides various methods for automatic causal discovery based on a dataframe.\n\n    Within Sawmill, the expectation is that the passed dataframe will contain the prepared variables.\n    \"\"\"\n\n    @staticmethod\n    def _pgmpy_dag_to_digraph(dag: DAG) -&gt; nx.DiGraph:\n        \"\"\"\n        Converts a pgmpy DAG to a networkx DiGraph.\n\n        Parameters:\n            dag: The pgmpy DAG.\n\n        Returns:\n            The networkx DiGraph.\n        \"\"\"\n\n        return nx.DiGraph(dag.edges())\n\n    @staticmethod\n    def pc(df: pd.DataFrame, max_cond_vars: int = 3) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the PC algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the PC algorithm.\n            max_cond_vars: The maximum number of conditioning variables to use.\n\n        Returns:\n            The causal graph learned by the PC algorithm.\n        \"\"\"\n\n        pc = PC(data=df)\n        model = pc.estimate(variant=\"parallel\", max_cond_vars=max_cond_vars)\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def hill_climb(df: pd.DataFrame) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the hill climb algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the hill climb algorithm.\n\n        Returns:\n            The causal graph learned by the hill climb algorithm.\n        \"\"\"\n\n        scoring_method = K2Score(data=df)\n        hcs = HillClimbSearch(data=df)\n        model = hcs.estimate(\n            scoring_method=scoring_method, max_indegree=4, max_iter=int(1e4)\n        )\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def exhaustive(df: pd.DataFrame) -&gt; nx.DiGraph:\n        \"\"\"\n        Runs the exhaustive search algorithm on a dataframe.\n\n        Parameters:\n            df: The dataframe on which to run the exhaustive search algorithm.\n\n        Returns:\n            The causal graph learned by the exhaustive search algorithm.\n        \"\"\"\n\n        scoring_method = K2Score(data=df)\n        exh = ExhaustiveSearch(data=df, complete_samples_only=False)\n        model = exh.estimate()\n        return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n\n    @staticmethod\n    def gpt(df: pd.DataFrame, model: str = \"gpt-3.5-turbo\") -&gt; nx.DiGraph:\n        \"\"\"\n        Consults GPT to determine the causal graph of the variables in the dataframe.\n\n        Parameters:\n            df: The dataframe based on which to construct a causal graph.\n            model: The GPT model to use.\n\n        Returns:\n            The causal graph learned by consulting GPT.\n        \"\"\"\n\n        # Open a file for logging, with the model and the timestamp in the name\n        log_file = open(\n            f\"~/causal-log/gpt-logs/{model}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.txt\",\n            \"w\",\n        )\n\n        client = OpenAI()\n        graph = nx.DiGraph()\n\n        for i in tqdm(\n            range(len(df.columns)), desc=\"Outer edge-finding loop using GPT...\"\n        ):\n            for j in range(i + 1, len(df.columns)):\n                var_a = df.columns[i]\n                var_b = df.columns[j]\n\n                example_rows = df[[var_a, var_b]].dropna().sample(3)\n                examples_a = \", \".join(str(x) for x in example_rows[var_a].tolist())\n                examples_b = \", \".join(str(x) for x in example_rows[var_b].tolist())\n\n                # Define the messages to send to the model\n                messages = [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a helpful assistant for causal reasoning.\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"\"\"Which cause-and-effect relationship is more likely? \"\"\"\n                        f\"\"\"A. changing {var_a} causes a change in {var_b}. \"\"\"\n                        f\"\"\"B. changing {var_b} causes a change in {var_a}. \"\"\"\n                        f\"\"\"C. Neither of the two. \"\"\"\n                        f\"\"\" Here are some example values of {var_a} : [{examples_a}]\"\"\"\n                        f\"\"\" Here are the corresponding values of {var_b} : [{examples_b}]\"\"\"\n                        \"\"\"Let's work this out in a step by step way to be sure that we have the right answer. \"\"\"\n                        \"\"\"Then provide your \ufb01nal answer within the tags &lt;Answer&gt;A/B/C&lt;/Answer&gt;.\"\"\",\n                    },\n                ]\n\n                reply = (\n                    client.chat.completions.create(model=model, messages=messages)\n                    .choices[0]\n                    .message.content\n                )\n\n                # Log the messages and the reply\n                log_file.write(f\"{datetime.now()}\\n\")\n                log_file.write(\"Messages:\\n\")\n                for message in messages:\n                    log_file.write(f\"{message['role']}: {message['content']}\\n\")\n                log_file.write(\"----------------\\n\")\n                log_file.write(f\"Reply: {reply}\\n\\n\")\n                log_file.write(\"================\\n\")\n\n                # Find the part of the reply that contains the answer\n                start_idx = reply.find(\"&lt;Answer&gt;\") + len(\"&lt;Answer&gt;\")\n                end_idx = reply.find(\"&lt;/Answer&gt;\")\n                answer = reply[start_idx:end_idx]\n\n                # Add the edge to the graph\n                if answer == \"A\":\n                    graph.add_edge(var_a, var_b)\n                elif answer == \"B\":\n                    graph.add_edge(var_b, var_a)\n        log_file.close()\n        return graph\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer._pgmpy_dag_to_digraph","title":"<code>_pgmpy_dag_to_digraph(dag)</code>  <code>staticmethod</code>","text":"<p>Converts a pgmpy DAG to a networkx DiGraph.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DAG</code> <p>The pgmpy DAG.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The networkx DiGraph.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>@staticmethod\ndef _pgmpy_dag_to_digraph(dag: DAG) -&gt; nx.DiGraph:\n    \"\"\"\n    Converts a pgmpy DAG to a networkx DiGraph.\n\n    Parameters:\n        dag: The pgmpy DAG.\n\n    Returns:\n        The networkx DiGraph.\n    \"\"\"\n\n    return nx.DiGraph(dag.edges())\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer.pc","title":"<code>pc(df, max_cond_vars=3)</code>  <code>staticmethod</code>","text":"<p>Runs the PC algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the PC algorithm.</p> required <code>max_cond_vars</code> <code>int</code> <p>The maximum number of conditioning variables to use.</p> <code>3</code> <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the PC algorithm.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>@staticmethod\ndef pc(df: pd.DataFrame, max_cond_vars: int = 3) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the PC algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the PC algorithm.\n        max_cond_vars: The maximum number of conditioning variables to use.\n\n    Returns:\n        The causal graph learned by the PC algorithm.\n    \"\"\"\n\n    pc = PC(data=df)\n    model = pc.estimate(variant=\"parallel\", max_cond_vars=max_cond_vars)\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer.hill_climb","title":"<code>hill_climb(df)</code>  <code>staticmethod</code>","text":"<p>Runs the hill climb algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the hill climb algorithm.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the hill climb algorithm.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>@staticmethod\ndef hill_climb(df: pd.DataFrame) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the hill climb algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the hill climb algorithm.\n\n    Returns:\n        The causal graph learned by the hill climb algorithm.\n    \"\"\"\n\n    scoring_method = K2Score(data=df)\n    hcs = HillClimbSearch(data=df)\n    model = hcs.estimate(\n        scoring_method=scoring_method, max_indegree=4, max_iter=int(1e4)\n    )\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer.exhaustive","title":"<code>exhaustive(df)</code>  <code>staticmethod</code>","text":"<p>Runs the exhaustive search algorithm on a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe on which to run the exhaustive search algorithm.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by the exhaustive search algorithm.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>@staticmethod\ndef exhaustive(df: pd.DataFrame) -&gt; nx.DiGraph:\n    \"\"\"\n    Runs the exhaustive search algorithm on a dataframe.\n\n    Parameters:\n        df: The dataframe on which to run the exhaustive search algorithm.\n\n    Returns:\n        The causal graph learned by the exhaustive search algorithm.\n    \"\"\"\n\n    scoring_method = K2Score(data=df)\n    exh = ExhaustiveSearch(data=df, complete_samples_only=False)\n    model = exh.estimate()\n    return CausalDiscoverer._pgmpy_dag_to_digraph(model)\n</code></pre>"},{"location":"reference/src/sawmill/causal_discoverer/#src.sawmill.causal_discoverer.CausalDiscoverer.gpt","title":"<code>gpt(df, model='gpt-3.5-turbo')</code>  <code>staticmethod</code>","text":"<p>Consults GPT to determine the causal graph of the variables in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe based on which to construct a causal graph.</p> required <code>model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> <p>Returns:</p> Type Description <code>DiGraph</code> <p>The causal graph learned by consulting GPT.</p> Source code in <code>src/sawmill/causal_discoverer.py</code> <pre><code>@staticmethod\ndef gpt(df: pd.DataFrame, model: str = \"gpt-3.5-turbo\") -&gt; nx.DiGraph:\n    \"\"\"\n    Consults GPT to determine the causal graph of the variables in the dataframe.\n\n    Parameters:\n        df: The dataframe based on which to construct a causal graph.\n        model: The GPT model to use.\n\n    Returns:\n        The causal graph learned by consulting GPT.\n    \"\"\"\n\n    # Open a file for logging, with the model and the timestamp in the name\n    log_file = open(\n        f\"~/causal-log/gpt-logs/{model}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.txt\",\n        \"w\",\n    )\n\n    client = OpenAI()\n    graph = nx.DiGraph()\n\n    for i in tqdm(\n        range(len(df.columns)), desc=\"Outer edge-finding loop using GPT...\"\n    ):\n        for j in range(i + 1, len(df.columns)):\n            var_a = df.columns[i]\n            var_b = df.columns[j]\n\n            example_rows = df[[var_a, var_b]].dropna().sample(3)\n            examples_a = \", \".join(str(x) for x in example_rows[var_a].tolist())\n            examples_b = \", \".join(str(x) for x in example_rows[var_b].tolist())\n\n            # Define the messages to send to the model\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant for causal reasoning.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Which cause-and-effect relationship is more likely? \"\"\"\n                    f\"\"\"A. changing {var_a} causes a change in {var_b}. \"\"\"\n                    f\"\"\"B. changing {var_b} causes a change in {var_a}. \"\"\"\n                    f\"\"\"C. Neither of the two. \"\"\"\n                    f\"\"\" Here are some example values of {var_a} : [{examples_a}]\"\"\"\n                    f\"\"\" Here are the corresponding values of {var_b} : [{examples_b}]\"\"\"\n                    \"\"\"Let's work this out in a step by step way to be sure that we have the right answer. \"\"\"\n                    \"\"\"Then provide your \ufb01nal answer within the tags &lt;Answer&gt;A/B/C&lt;/Answer&gt;.\"\"\",\n                },\n            ]\n\n            reply = (\n                client.chat.completions.create(model=model, messages=messages)\n                .choices[0]\n                .message.content\n            )\n\n            # Log the messages and the reply\n            log_file.write(f\"{datetime.now()}\\n\")\n            log_file.write(\"Messages:\\n\")\n            for message in messages:\n                log_file.write(f\"{message['role']}: {message['content']}\\n\")\n            log_file.write(\"----------------\\n\")\n            log_file.write(f\"Reply: {reply}\\n\\n\")\n            log_file.write(\"================\\n\")\n\n            # Find the part of the reply that contains the answer\n            start_idx = reply.find(\"&lt;Answer&gt;\") + len(\"&lt;Answer&gt;\")\n            end_idx = reply.find(\"&lt;/Answer&gt;\")\n            answer = reply[start_idx:end_idx]\n\n            # Add the edge to the graph\n            if answer == \"A\":\n                graph.add_edge(var_a, var_b)\n            elif answer == \"B\":\n                graph.add_edge(var_b, var_a)\n    log_file.close()\n    return graph\n</code></pre>"},{"location":"reference/src/sawmill/causal_unit_suggester/","title":"CausalUnitSuggester","text":""},{"location":"reference/src/sawmill/causal_unit_suggester/#src.sawmill.causal_unit_suggester.CausalUnitSuggester","title":"<code>CausalUnitSuggester</code>","text":"<p>This class is responsible for suggesting causal units to the user.</p> Source code in <code>src/sawmill/causal_unit_suggester.py</code> <pre><code>class CausalUnitSuggester:\n    \"\"\"\n    This class is responsible for suggesting causal units to the user.\n    \"\"\"\n\n    @staticmethod\n    def _discretize(col: pd.Series, col_type: str, bins: int = 0) -&gt; pd.Series:\n        \"\"\"\n        Discretize an unsorted `col` based on its type. If `col_type` is 'num', then\n        return labels for each of `bins` equi-depth bins. If `col_type` is 'str,\n        then return a unique label for each unique value. Nulls in `col` are assigned\n        to bin -1.\n\n        Parameters:\n            col: The column to discretize.\n            col_type: The type of the column.\n            bins: The number of bins to use when discretizing the column.\n\n        Returns:\n            A vector of length len(`col`) with the labels of each value in `col`.\n        \"\"\"\n        if col_type == \"num\":\n            return (\n                pd.qcut(col, bins, labels=False, duplicates=\"drop\")\n                .fillna(-1)\n                .astype(int)\n            )\n        elif col_type == \"str\":\n            return pd.factorize(col, use_na_sentinel=True)[0]\n        else:\n            raise ValueError(f\"Unknown column type: {col_type}\")\n\n    @staticmethod\n    def _get_all_discretizations(\n        col: pd.Series, col_type: str, k: int\n    ) -&gt; list[pd.Series]:\n        \"\"\"\n        Return a list of all possible discretizations of `col` based on its type.\n        If `col_type` is 'num', then return discretizations with `k`, `2k` and `10k` bins.\n        If `col_type` is 'str', then return a discretization with a unique label for\n        each unique value in `col`.\n\n        Parameters:\n            col: The column to discretize.\n            col_type: The type of the column.\n            k: A parameter indirectly controlling the number of bins to use when discretizing\n                a numeric column (see above).\n\n        Returns:\n            A list of all desired discretizations of `col`.\n        \"\"\"\n\n        if col_type == \"num\":\n            l = []\n            if len(col) &gt;= k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, k))\n            if len(col) &gt;= 2 * k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, 2 * k))\n            if len(col) &gt;= 10 * k:\n                l.append(CausalUnitSuggester._discretize(col, col_type, 10 * k))\n            return l\n        elif col_type == \"str\":\n            return [CausalUnitSuggester._discretize(col, col_type)]\n        else:\n            raise ValueError(f\"Unknown column type: {col_type}\")\n\n    @staticmethod\n    def _calculate_IUS(df: pd.DataFrame, discretization: pd.Series) -&gt; float:\n        \"\"\"\n        Calculate the Information Utilization Score of `df` if each row belongs\n        to the causal unit specified by `discretization`. The unit labelled -1\n        contails rows with null value for the causal unit column, so the corresponding\n        rows in `df` are ignored.\n\n        Parameters:\n            df: The DataFrame to calculate the Information Utilization Score of.\n            discretization: The causal unit of each row.\n\n        Returns:\n            The Information Utilization Score of `df`.\n        \"\"\"\n\n        grouped = df.groupby(discretization)  # TODO: handle nulls\n        ius = 0\n\n        for group_id, group_data in grouped:\n            if group_id == -1:\n                continue\n            columns_with_non_nulls = group_data.notna().any(axis=0).sum()\n            ius += columns_with_non_nulls * len(group_data)\n\n        return ius / (len(df.columns) * len(df))\n\n    @staticmethod\n    def suggest_causal_unit_defs(\n        data_df: pd.DataFrame,\n        var_df: pd.DataFrame,\n        min_causal_units: int = 4,\n        num_suggestions: int = 10,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Suggest at most `num_suggestions` causal unit definitions for `data_df` based on ius\n        maximization, while returning at least `min_causal_units` causal units. `var_df` provides\n        information on the type of each variable.\n\n        Parameters:\n            data_df: The DataFrame to suggest causal unit definitions for.\n            var_df: A DataFrame with one row for each variable in `data_df` that includes variable type information.\n            min_causal_units: The minimum number of causal units that a suggested definition should create.\n            num_suggestions: The maximum number of causal unit definitions to suggest.\n\n        Returns:\n            A DataFrame with one row for each suggested causal unit definition, or `None` if no suggestions were made.\n        \"\"\"\n\n        list_of_suggestions = []\n\n        for col in data_df.columns:\n            discretizations = CausalUnitSuggester._get_all_discretizations(\n                data_df[col],\n                var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                k=min_causal_units,\n            )\n            for disc in discretizations:\n                # Ensure that the unique values in disc, excluding -1 if it exists, are at least min_causal_units\n                if disc.max() &gt;= (min_causal_units - 1):\n                    list_of_suggestions.append(\n                        {\n                            \"Variable\": col,\n                            \"Type\": var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                            \"Num Units\": disc.max() + 1,\n                            \"IUS\": CausalUnitSuggester._calculate_IUS(data_df, disc),\n                        }\n                    )\n\n        df_of_suggestions = pd.DataFrame(list_of_suggestions)\n        if len(df_of_suggestions) == 0:\n            return None\n        return df_of_suggestions.sort_values(by=[\"IUS\"], ascending=False).head(\n            num_suggestions\n        )\n</code></pre>"},{"location":"reference/src/sawmill/causal_unit_suggester/#src.sawmill.causal_unit_suggester.CausalUnitSuggester._discretize","title":"<code>_discretize(col, col_type, bins=0)</code>  <code>staticmethod</code>","text":"<p>Discretize an unsorted <code>col</code> based on its type. If <code>col_type</code> is 'num', then return labels for each of <code>bins</code> equi-depth bins. If <code>col_type</code> is 'str, then return a unique label for each unique value. Nulls in <code>col</code> are assigned to bin -1.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column to discretize.</p> required <code>col_type</code> <code>str</code> <p>The type of the column.</p> required <code>bins</code> <code>int</code> <p>The number of bins to use when discretizing the column.</p> <code>0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A vector of length len(<code>col</code>) with the labels of each value in <code>col</code>.</p> Source code in <code>src/sawmill/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _discretize(col: pd.Series, col_type: str, bins: int = 0) -&gt; pd.Series:\n    \"\"\"\n    Discretize an unsorted `col` based on its type. If `col_type` is 'num', then\n    return labels for each of `bins` equi-depth bins. If `col_type` is 'str,\n    then return a unique label for each unique value. Nulls in `col` are assigned\n    to bin -1.\n\n    Parameters:\n        col: The column to discretize.\n        col_type: The type of the column.\n        bins: The number of bins to use when discretizing the column.\n\n    Returns:\n        A vector of length len(`col`) with the labels of each value in `col`.\n    \"\"\"\n    if col_type == \"num\":\n        return (\n            pd.qcut(col, bins, labels=False, duplicates=\"drop\")\n            .fillna(-1)\n            .astype(int)\n        )\n    elif col_type == \"str\":\n        return pd.factorize(col, use_na_sentinel=True)[0]\n    else:\n        raise ValueError(f\"Unknown column type: {col_type}\")\n</code></pre>"},{"location":"reference/src/sawmill/causal_unit_suggester/#src.sawmill.causal_unit_suggester.CausalUnitSuggester._get_all_discretizations","title":"<code>_get_all_discretizations(col, col_type, k)</code>  <code>staticmethod</code>","text":"<p>Return a list of all possible discretizations of <code>col</code> based on its type. If <code>col_type</code> is 'num', then return discretizations with <code>k</code>, <code>2k</code> and <code>10k</code> bins. If <code>col_type</code> is 'str', then return a discretization with a unique label for each unique value in <code>col</code>.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>Series</code> <p>The column to discretize.</p> required <code>col_type</code> <code>str</code> <p>The type of the column.</p> required <code>k</code> <code>int</code> <p>A parameter indirectly controlling the number of bins to use when discretizing a numeric column (see above).</p> required <p>Returns:</p> Type Description <code>list[Series]</code> <p>A list of all desired discretizations of <code>col</code>.</p> Source code in <code>src/sawmill/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _get_all_discretizations(\n    col: pd.Series, col_type: str, k: int\n) -&gt; list[pd.Series]:\n    \"\"\"\n    Return a list of all possible discretizations of `col` based on its type.\n    If `col_type` is 'num', then return discretizations with `k`, `2k` and `10k` bins.\n    If `col_type` is 'str', then return a discretization with a unique label for\n    each unique value in `col`.\n\n    Parameters:\n        col: The column to discretize.\n        col_type: The type of the column.\n        k: A parameter indirectly controlling the number of bins to use when discretizing\n            a numeric column (see above).\n\n    Returns:\n        A list of all desired discretizations of `col`.\n    \"\"\"\n\n    if col_type == \"num\":\n        l = []\n        if len(col) &gt;= k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, k))\n        if len(col) &gt;= 2 * k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, 2 * k))\n        if len(col) &gt;= 10 * k:\n            l.append(CausalUnitSuggester._discretize(col, col_type, 10 * k))\n        return l\n    elif col_type == \"str\":\n        return [CausalUnitSuggester._discretize(col, col_type)]\n    else:\n        raise ValueError(f\"Unknown column type: {col_type}\")\n</code></pre>"},{"location":"reference/src/sawmill/causal_unit_suggester/#src.sawmill.causal_unit_suggester.CausalUnitSuggester._calculate_IUS","title":"<code>_calculate_IUS(df, discretization)</code>  <code>staticmethod</code>","text":"<p>Calculate the Information Utilization Score of <code>df</code> if each row belongs to the causal unit specified by <code>discretization</code>. The unit labelled -1 contails rows with null value for the causal unit column, so the corresponding rows in <code>df</code> are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to calculate the Information Utilization Score of.</p> required <code>discretization</code> <code>Series</code> <p>The causal unit of each row.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Information Utilization Score of <code>df</code>.</p> Source code in <code>src/sawmill/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef _calculate_IUS(df: pd.DataFrame, discretization: pd.Series) -&gt; float:\n    \"\"\"\n    Calculate the Information Utilization Score of `df` if each row belongs\n    to the causal unit specified by `discretization`. The unit labelled -1\n    contails rows with null value for the causal unit column, so the corresponding\n    rows in `df` are ignored.\n\n    Parameters:\n        df: The DataFrame to calculate the Information Utilization Score of.\n        discretization: The causal unit of each row.\n\n    Returns:\n        The Information Utilization Score of `df`.\n    \"\"\"\n\n    grouped = df.groupby(discretization)  # TODO: handle nulls\n    ius = 0\n\n    for group_id, group_data in grouped:\n        if group_id == -1:\n            continue\n        columns_with_non_nulls = group_data.notna().any(axis=0).sum()\n        ius += columns_with_non_nulls * len(group_data)\n\n    return ius / (len(df.columns) * len(df))\n</code></pre>"},{"location":"reference/src/sawmill/causal_unit_suggester/#src.sawmill.causal_unit_suggester.CausalUnitSuggester.suggest_causal_unit_defs","title":"<code>suggest_causal_unit_defs(data_df, var_df, min_causal_units=4, num_suggestions=10)</code>  <code>staticmethod</code>","text":"<p>Suggest at most <code>num_suggestions</code> causal unit definitions for <code>data_df</code> based on ius maximization, while returning at least <code>min_causal_units</code> causal units. <code>var_df</code> provides information on the type of each variable.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The DataFrame to suggest causal unit definitions for.</p> required <code>var_df</code> <code>DataFrame</code> <p>A DataFrame with one row for each variable in <code>data_df</code> that includes variable type information.</p> required <code>min_causal_units</code> <code>int</code> <p>The minimum number of causal units that a suggested definition should create.</p> <code>4</code> <code>num_suggestions</code> <code>int</code> <p>The maximum number of causal unit definitions to suggest.</p> <code>10</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame with one row for each suggested causal unit definition, or <code>None</code> if no suggestions were made.</p> Source code in <code>src/sawmill/causal_unit_suggester.py</code> <pre><code>@staticmethod\ndef suggest_causal_unit_defs(\n    data_df: pd.DataFrame,\n    var_df: pd.DataFrame,\n    min_causal_units: int = 4,\n    num_suggestions: int = 10,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Suggest at most `num_suggestions` causal unit definitions for `data_df` based on ius\n    maximization, while returning at least `min_causal_units` causal units. `var_df` provides\n    information on the type of each variable.\n\n    Parameters:\n        data_df: The DataFrame to suggest causal unit definitions for.\n        var_df: A DataFrame with one row for each variable in `data_df` that includes variable type information.\n        min_causal_units: The minimum number of causal units that a suggested definition should create.\n        num_suggestions: The maximum number of causal unit definitions to suggest.\n\n    Returns:\n        A DataFrame with one row for each suggested causal unit definition, or `None` if no suggestions were made.\n    \"\"\"\n\n    list_of_suggestions = []\n\n    for col in data_df.columns:\n        discretizations = CausalUnitSuggester._get_all_discretizations(\n            data_df[col],\n            var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n            k=min_causal_units,\n        )\n        for disc in discretizations:\n            # Ensure that the unique values in disc, excluding -1 if it exists, are at least min_causal_units\n            if disc.max() &gt;= (min_causal_units - 1):\n                list_of_suggestions.append(\n                    {\n                        \"Variable\": col,\n                        \"Type\": var_df[var_df[\"Name\"] == col][\"Type\"].values[0],\n                        \"Num Units\": disc.max() + 1,\n                        \"IUS\": CausalUnitSuggester._calculate_IUS(data_df, disc),\n                    }\n                )\n\n    df_of_suggestions = pd.DataFrame(list_of_suggestions)\n    if len(df_of_suggestions) == 0:\n        return None\n    return df_of_suggestions.sort_values(by=[\"IUS\"], ascending=False).head(\n        num_suggestions\n    )\n</code></pre>"},{"location":"reference/src/sawmill/clustering_params/","title":"ClusteringParams","text":""},{"location":"reference/src/sawmill/clustering_params/#src.sawmill.clustering_params.ClusteringParams","title":"<code>ClusteringParams</code>","text":"<p>A class to conveniently hold all the parameters required by the clustering approach to challenging the ATE.</p> Source code in <code>src/sawmill/clustering_params.py</code> <pre><code>class ClusteringParams:\n    \"\"\"\n    A class to conveniently hold all the parameters required by the clustering\n    approach to challenging the ATE.\n    \"\"\"\n\n    def __init__(\n        self,\n        top_n: int = 10,\n        num_edges: int = 3,\n        ignore_ts: bool = True,\n        var_pruning_method: Optional[str] = None,\n        triangle_n: int = 6,\n        force: bool = False,\n        force_triangle: bool = False,\n        num_clusters: Optional[int] = None,\n        threshold: float = 0,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a ClusteringParams object.\n\n        Parameters:\n            top_n: The number of top edges to identify.\n            num_edges: The maximum number of edges to use when enumerating DAGs.\n            ignore_ts: Whether to ignore timestamp variables.\n            var_pruning_method: The pruning method to use. Can be either \"lasso\" or \"triangle\".\n            triangle_n: The number of variables to use for the triangle method.\n            force: Whether to force recalculation.\n            force_triangle: Whether to force the triangle method to be recalculated, if selected.\n            num_clusters: The number of clusters to use. If None, will try to find the optimal number.\n            threshold: The threshold to use when finding outlier edges.\n\n        \"\"\"\n        self.top_n = top_n\n        self.num_edges = num_edges\n        self.ignore_ts = ignore_ts\n        self.var_pruning_method = var_pruning_method\n        self.triangle_n = triangle_n\n        self.force = force\n        self.force_triangle = force_triangle\n        self.num_clusters = num_clusters\n        self.threshold = threshold\n</code></pre>"},{"location":"reference/src/sawmill/clustering_params/#src.sawmill.clustering_params.ClusteringParams.__init__","title":"<code>__init__(top_n=10, num_edges=3, ignore_ts=True, var_pruning_method=None, triangle_n=6, force=False, force_triangle=False, num_clusters=None, threshold=0)</code>","text":"<p>Initializes a ClusteringParams object.</p> <p>Parameters:</p> Name Type Description Default <code>top_n</code> <code>int</code> <p>The number of top edges to identify.</p> <code>10</code> <code>num_edges</code> <code>int</code> <p>The maximum number of edges to use when enumerating DAGs.</p> <code>3</code> <code>ignore_ts</code> <code>bool</code> <p>Whether to ignore timestamp variables.</p> <code>True</code> <code>var_pruning_method</code> <code>Optional[str]</code> <p>The pruning method to use. Can be either \"lasso\" or \"triangle\".</p> <code>None</code> <code>triangle_n</code> <code>int</code> <p>The number of variables to use for the triangle method.</p> <code>6</code> <code>force</code> <code>bool</code> <p>Whether to force recalculation.</p> <code>False</code> <code>force_triangle</code> <code>bool</code> <p>Whether to force the triangle method to be recalculated, if selected.</p> <code>False</code> <code>num_clusters</code> <code>Optional[int]</code> <p>The number of clusters to use. If None, will try to find the optimal number.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The threshold to use when finding outlier edges.</p> <code>0</code> Source code in <code>src/sawmill/clustering_params.py</code> <pre><code>def __init__(\n    self,\n    top_n: int = 10,\n    num_edges: int = 3,\n    ignore_ts: bool = True,\n    var_pruning_method: Optional[str] = None,\n    triangle_n: int = 6,\n    force: bool = False,\n    force_triangle: bool = False,\n    num_clusters: Optional[int] = None,\n    threshold: float = 0,\n) -&gt; None:\n    \"\"\"\n    Initializes a ClusteringParams object.\n\n    Parameters:\n        top_n: The number of top edges to identify.\n        num_edges: The maximum number of edges to use when enumerating DAGs.\n        ignore_ts: Whether to ignore timestamp variables.\n        var_pruning_method: The pruning method to use. Can be either \"lasso\" or \"triangle\".\n        triangle_n: The number of variables to use for the triangle method.\n        force: Whether to force recalculation.\n        force_triangle: Whether to force the triangle method to be recalculated, if selected.\n        num_clusters: The number of clusters to use. If None, will try to find the optimal number.\n        threshold: The threshold to use when finding outlier edges.\n\n    \"\"\"\n    self.top_n = top_n\n    self.num_edges = num_edges\n    self.ignore_ts = ignore_ts\n    self.var_pruning_method = var_pruning_method\n    self.triangle_n = triangle_n\n    self.force = force\n    self.force_triangle = force_triangle\n    self.num_clusters = num_clusters\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/src/sawmill/drain/","title":"Drain","text":"<p>Inspired by the LogPAI implementation of the Drain algorithm for log parsing,  available under the MIT license here: https://github.com/HelenGuohx/logbert/blob/main/logparser/Drain.py</p>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Cluster","title":"<code>Cluster</code>","text":"<p>A cluster in the Drain parse tree.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>class Cluster:\n    \"\"\"\n    A cluster in the Drain parse tree.\n    \"\"\"\n\n    def __init__(self, template: str = \"\", message_ids: list[int] = []):\n        \"\"\"\n        Parameters:\n            template : the template of log messages in this cluster\n            message_ids : the list of log message IDs in this cluster\n        \"\"\"\n\n        self.template = template\n        self.message_ids = message_ids\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Cluster.__init__","title":"<code>__init__(template='', message_ids=[])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>template</code> <p>the template of log messages in this cluster</p> <code>''</code> <code>message_ids</code> <p>the list of log message IDs in this cluster</p> <code>[]</code> Source code in <code>src/sawmill/drain.py</code> <pre><code>def __init__(self, template: str = \"\", message_ids: list[int] = []):\n    \"\"\"\n    Parameters:\n        template : the template of log messages in this cluster\n        message_ids : the list of log message IDs in this cluster\n    \"\"\"\n\n    self.template = template\n    self.message_ids = message_ids\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Node","title":"<code>Node</code>","text":"<p>A node in the Drain parse tree.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>class Node:\n    \"\"\"\n    A node in the Drain parse tree.\n    \"\"\"\n\n    def __init__(self, children=None, depth=0, id=None):\n        \"\"\"\n        Parameters:\n            children : the dictionary of children nodes\n            depth : the depth of this node in the tree\n            id : the digit or token that this node represents\n        \"\"\"\n        if children is None:\n            children = dict()\n        self.children = children\n        self.depth = depth\n        self.id = id\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Node.__init__","title":"<code>__init__(children=None, depth=0, id=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>children</code> <p>the dictionary of children nodes</p> <code>None</code> <code>depth</code> <p>the depth of this node in the tree</p> <code>0</code> <code>id</code> <p>the digit or token that this node represents</p> <code>None</code> Source code in <code>src/sawmill/drain.py</code> <pre><code>def __init__(self, children=None, depth=0, id=None):\n    \"\"\"\n    Parameters:\n        children : the dictionary of children nodes\n        depth : the depth of this node in the tree\n        id : the digit or token that this node represents\n    \"\"\"\n    if children is None:\n        children = dict()\n    self.children = children\n    self.depth = depth\n    self.id = id\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain","title":"<code>Drain</code>","text":"<p>A class implementing the Drain log parsing algorithm.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>class Drain:\n    \"\"\"\n    A class implementing the Drain log parsing algorithm.\n    \"\"\"\n\n    def __init__(\n        self,\n        indir: str = \".\",\n        depth: int = 4,\n        st: float = 0.4,\n        max_children: int = 100,\n        rex: dict = {},\n        skip_writeout: bool = False,\n        message_prefix: str = r\".*\",\n    ):\n        \"\"\"\n        Initialize a Drain-based parser.\n\n        Parameters:\n            indir: the input directory stores the input log file name\n            depth: depth of all leaf nodes\n            st: similarity threshold\n            max_children: max number of children of an internal node\n            rex: regular expressions used in preprocessing, provided as a dictionary from field name to field regex\n            skip_writeout: whether to skip writing out the parsed log file, templates and variables.\n            message_prefix: prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.\n        \"\"\"\n        self.indir = indir\n        self.depth = depth - 2\n        self.st = st\n        self.max_children = max_children\n        self.rex = rex\n        self.skip_writeout = skip_writeout\n        self.message_prefix = message_prefix\n\n    def parse(self, filename: str) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Parse a log file.\n\n        Parameters:\n            filename: The name of the log file to parse (without path).\n\n        Returns:\n            A tuple of three dataframes, containing the parsed log file, the parsed log templates,\n            and the parsed variables respectively.\n        \"\"\"\n\n        full_path = os.path.join(self.indir, filename)\n        print(f\"Parsing file: {full_path}\")\n        self.filename = filename\n        self.root = Node()\n        self.cluster_list = []\n        self.logdf = self._to_df(full_path)\n\n        tqdm.pandas(desc=\"Determining template for each line...\")\n        self.logdf.progress_apply(self._parse_message, axis=1)\n\n        return self._postprocess()\n\n    def _to_df(self, log_file: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform a log file into a dataframe.\n\n        Parameters:\n            log_file: The path to the log file.\n\n        Returns:\n            A dataframe containing the log file's lines, tokenized and with regexes replaced.\n        \"\"\"\n\n        log_messages = []\n        linecount = 0\n\n        with open(log_file, \"r\") as f:\n            log_message = \"\"\n\n            for line in tqdm(f.readlines(), desc=\"Reading and tokenizing log lines...\"):\n                line = line.strip()\n\n                if re.match(self.message_prefix, line):\n                    if log_message:\n                        try:\n                            log_messages.append(self._preprocess(log_message))\n                            linecount += 1\n                        except Exception as e:\n                            raise ValueError\n                    log_message = line\n                else:\n                    log_message += \" \" + line\n\n            if log_message:\n                try:\n                    log_messages.append(self._preprocess(log_message))\n                    linecount += 1\n                except Exception as e:\n                    raise ValueError\n\n        logdf = pd.DataFrame(\n            log_messages, columns=[\"Message\", \"Tokenized\", \"Replaced by regex\"]\n        )\n        logdf[\"MsgId\"] = range(len(logdf))\n        return logdf\n\n    def _preprocess(self, msg: str) -&gt; tuple[str, list[str], list[str]]:\n        \"\"\"\n        Preprocess a message of a log file.\n\n        Parameters:\n            msg: The message to preprocess.\n\n        Returns:\n            A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.\n        \"\"\"\n\n        msg = msg.strip()\n\n        regex_matches = []\n        for i, rex in enumerate(self.rex.values()):\n            matches = re.findall(rex, msg)  ##### ASSUMPTION: only 1 match of interest\n            regex_matches.append(matches[0] if matches else \"\")\n            msg = re.sub(rex, \"&lt;*\" + str(i) + \"&gt;\", msg, count=1)\n\n        pattern = r'([=,\\{\\}\\[\\]\\(\\);\"\\'])'  # Add spaces around punctuation\n        msg = re.sub(pattern, r\" \\1 \", msg)\n        pattern = r\"(?&lt;=\\D):|:(?=\\D)\"  # Colons not in timestamps\n        msg = re.sub(pattern, \" : \", msg)\n\n        return (msg, msg.strip().split(), regex_matches)\n\n    def _parse_message(self, msg: pd.Series) -&gt; None:\n        \"\"\"\n        Parse a single log message and add it to the Drain parse tree in the appropriate cluster.\n\n        Parameters:\n            msg: The log message to parse.\n        \"\"\"\n\n        line_id = msg[\"MsgId\"]\n        tokenized = msg[\"Tokenized\"]\n        cluster = self._tree_search(self.root, tokenized)\n\n        if cluster is None:\n            new_cluster = Cluster(template=tokenized, message_ids=[line_id])\n            self.cluster_list.append(new_cluster)\n            self._add_cluster_to_tree(self.root, new_cluster)\n        else:\n            new_template = self._get_updated_template(tokenized, cluster.template)\n            cluster.message_ids.append(line_id)\n            if \" \".join(new_template) != \" \".join(cluster.template):\n                cluster.template = new_template\n\n    def _tree_search(self, root: Node, tokenized: list[str]) -&gt; Optional[Cluster]:\n        \"\"\"\n        Search the Drain parse tree for a cluster matching `tokenized`.\n\n        Parameters:\n            root: The root of the Drain parse tree.\n            tokenized: The tokenized log message to search for.\n\n        Returns:\n            The cluster in the Drain parse tree that matches `tokenized`,\n            or None if no such cluster exists.\n        \"\"\"\n\n        num_toks = len(tokenized)\n        if num_toks not in root.children:\n            return None\n\n        node = root.children[num_toks]\n\n        depth = 1\n        for token in tokenized:\n            if depth &gt;= self.depth or depth &gt; num_toks:\n                break\n            if token in node.children:\n                node = node.children[token]\n            elif \"&lt;*&gt;\" in node.children:\n                node = node.children[\"&lt;*&gt;\"]\n            else:\n                return None\n            depth += 1\n\n        cluster_list = node.children\n        returned_cluster = self._find_cluster(cluster_list, tokenized)\n\n        return returned_cluster\n\n    def _add_cluster_to_tree(self, root: Node, cluster: Cluster) -&gt; None:\n        \"\"\"\n        Add a cluster to the Drain parse tree.\n\n        Parameters:\n            root: The root of the Drain parse tree.\n            cluster: The cluster to add.\n        \"\"\"\n\n        # Add a node to the first layer of the tree representing the length of the log message.\n        length = len(cluster.template)\n        first_layer_node = None\n        if length not in root.children:\n            first_layer_node = Node(depth=1, id=length)\n            root.children[length] = first_layer_node\n        else:\n            first_layer_node = root.children[length]\n\n        # Traverse the tree to add the new cluster.\n        node = first_layer_node\n        depth = 1\n        for token in cluster.template:\n            # If out of depth, add current log cluster to the leaf node\n            if depth &gt;= self.depth or depth &gt; length:\n                if len(node.children) == 0:\n                    node.children = [cluster]\n                else:\n                    node.children.append(cluster)\n                break\n\n            # If token not matched in this layer of existing tree.\n            if token not in node.children:\n                if not any(char.isdigit() for char in token):\n                    if \"&lt;*&gt;\" in node.children:\n                        if len(node.children) &lt; self.max_children:\n                            new_node = Node(depth=depth + 1, id=token)\n                            node.children[token] = new_node\n                            node = new_node\n                        else:\n                            node = node.children[\"&lt;*&gt;\"]\n                    else:\n                        if len(node.children) + 1 &lt; self.max_children:\n                            new_node = Node(depth=depth + 1, id=token)\n                            node.children[token] = new_node\n                            node = new_node\n                        elif len(node.children) + 1 == self.max_children:\n                            new_node = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                            node.children[\"&lt;*&gt;\"] = new_node\n                            node = new_node\n                        else:\n                            node = node.children[\"&lt;*&gt;\"]\n                else:\n                    if \"&lt;*&gt;\" not in node.children:\n                        node.children[\"&lt;*&gt;\"] = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                    node = node.children[\"&lt;*&gt;\"]\n\n            # If the token is matched\n            else:\n                node = node.children[token]\n\n            depth += 1\n\n    def _similarity(self, seq1: list[str], seq2: list[str]) -&gt; tuple[float, int]:\n        \"\"\"\n        Determine the fraction of tokens in `seq1` that are identical to the corresponding token in `seq2`.\n        Also return the number of parameters in `seq1`.\n\n        Parameters:\n            seq1: The first sequence.\n            seq2: The second sequence.\n\n        Returns:\n            A tuple containing the fraction of identical tokens and the number of parameters in `seq1`.\n        \"\"\"\n        assert len(seq1) == len(seq2)\n        matches = 0\n        num_params = 0\n\n        for token1, token2 in zip(seq1, seq2):\n            if token1 == \"&lt;*&gt;\":\n                num_params += 1\n            if token1 == token2:\n                matches += 1\n\n        similarity = float(matches) / len(seq1)\n\n        return similarity, num_params\n\n    def _find_cluster(\n        self, cluster_list: list[Cluster], seq: list[str]\n    ) -&gt; Optional[Cluster]:\n        \"\"\"\n        Find the cluster in `cluster_list` that is most similar to `seq`.\n\n        Parameters:\n            cluster_list: The list of clusters to search.\n            seq: The sequence of tokens to compare to.\n\n        Returns:\n            The cluster in `cluster_list` that is most similar to `seq`,\n            or None if no cluster is sufficiently similar.\n        \"\"\"\n\n        max_similarity = -1\n        max_num_params = -1\n        max_cluster = None\n\n        for cluster in cluster_list:\n            similarity, num_params = self._similarity(cluster.template, seq)\n            if similarity &gt; max_similarity or (\n                similarity == max_similarity and num_params &gt; max_num_params\n            ):\n                max_similarity = similarity\n                max_num_params = num_params\n                max_cluster = cluster\n\n        if max_similarity &gt;= self.st:\n            return max_cluster\n        else:\n            return None\n\n    def _get_updated_template(self, template: list[str], msg: list[str]) -&gt; list[str]:\n        \"\"\"\n        Get the updated template from matching `msg` to `template`.\n\n        Parameters:\n            template: The template to match to.\n            msg: The message to match.\n\n        Returns:\n            The updated template.\n        \"\"\"\n\n        assert len(template) == len(msg)\n        updated_template = []\n\n        for i, word in enumerate(template):\n            if word == msg[i]:\n                updated_template.append(word)\n            else:\n                updated_template.append(\"&lt;*&gt;\")\n\n        return updated_template\n\n    @staticmethod\n    def _preceding_3(parsed_templates: pd.DataFrame, x: str) -&gt; list[str]:\n        \"\"\"\n        Get the 3 tokens preceding the variable `x` in the template.\n\n        Parameters:\n            parsed_templates: The dataframe containing information about the parsed templates.\n            x: The name of the variable.\n\n        Returns:\n            The 3 tokens preceding the variable `x` in the template.\n        \"\"\"\n\n        splitx = x.split(\"_\")\n        if len(splitx) != 2:\n            return []\n        id = splitx[0]\n        position = int(splitx[1])\n        start_position = max(0, position - 3)\n        return (\n            parsed_templates[parsed_templates[\"TemplateId\"] == id][\"TemplateText\"]\n            .values[0]\n            .split()[start_position:position]\n        )\n\n    def _postprocess(\n        self,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        template_id_per_msg = [0] * self.logdf.shape[0]\n        parsed_templates_list = []\n\n        regex_tokens = [\"&lt;*\" + str(i) + \"&gt;\" for i in range(len(self.rex))]\n\n        # Process each cluster to determine template information.\n        for cluster in self.cluster_list:\n            d = {}\n\n            # Determine the template ID\n            d[\"TemplateText\"] = \" \".join(cluster.template)\n            d[\"TemplateId\"] = hashlib.md5(\n                d[\"TemplateText\"].encode(\"utf-8\")\n            ).hexdigest()[0:8]\n\n            # Determine the indices of the variables and regexes in the template.\n            d[\"VariableIndices\"] = [\n                i for i, x in enumerate(cluster.template) if x == \"&lt;*&gt;\"\n            ]\n            d[\"RegexIndices\"] = []\n            for i in regex_tokens:\n                try:\n                    d[\"RegexIndices\"].append(cluster.template.index(str(i)))\n                except:\n                    pass\n\n            # Update the template ID for each log message in the cluster.\n            for i, line_id in enumerate(cluster.message_ids):\n                self.logdf.loc[line_id, \"TemplateId\"] = d[\"TemplateId\"]\n\n                if i == 0:\n                    d[\"TemplateExample\"] = self.logdf.loc[line_id, \"Message\"]\n\n            parsed_templates_list.append(d.copy())\n\n        # Create a dataframe of the parsed templates.\n        self.parsed_templates = pd.DataFrame(parsed_templates_list)\n        template_occurences = dict(self.logdf[\"TemplateId\"].value_counts())\n        self.parsed_templates[\"Occurrences\"] = self.parsed_templates[\"TemplateId\"].map(\n            template_occurences\n        )\n\n        # Create columns for each variable (parsed or regex-derived) and extract them from each log message.\n        variable_columns = list(self.rex.keys())\n        variable_columns.extend(\n            [\n                str(i) + \"_\" + str(j)\n                for i in self.parsed_templates[\"TemplateId\"].values\n                for j in self.parsed_templates.loc[\n                    self.parsed_templates[\"TemplateId\"] == i, \"VariableIndices\"\n                ].values[0]\n            ]\n        )\n        par_df = pd.DataFrame(\n            columns=variable_columns, index=range(self.logdf.shape[0])\n        )\n        self.logdf = pd.concat((self.logdf, par_df), axis=1)\n        self._extract_variables()\n\n        # Create a dataframe of the parsed variables.\n        parsed_variables = pd.DataFrame()\n        parsed_variables[\"Name\"] = variable_columns\n        parsed_variables[\"Occurrences\"] = parsed_variables[\"Name\"].map(\n            lambda x: self.logdf[x].notna().sum()\n        )\n        parsed_variables[\"Preceding 3 tokens\"] = parsed_variables[\"Name\"].map(\n            lambda x: Drain._preceding_3(self.parsed_templates, x)\n        )\n        parsed_variables[\"Examples\"] = parsed_variables[\"Name\"].map(\n            lambda x: self.logdf[x].loc[self.logdf[x].notna()].unique()[:5].tolist()\n        )\n        parsed_variables[\"From regex\"] = parsed_variables[\"Name\"].map(\n            lambda x: True if x in self.rex.keys() else False\n        )\n\n        # Drop unnecessary columns from the parsed log.\n        to_drop = [\"MsgId\", \"Message\", \"Tokenized\", \"Replaced by regex\"]\n        to_drop.extend(\n            parsed_variables[parsed_variables[\"Occurrences\"] == 0][\"Name\"].tolist()\n        )\n        parsed_log = self.logdf.drop(columns=to_drop)\n        parsed_variables = (\n            parsed_variables[~parsed_variables.isin(to_drop)[\"Name\"]]\n            .reset_index()\n            .drop(columns=\"index\")\n        )\n\n        return parsed_log, self.parsed_templates, parsed_variables\n\n    def _extract_variables(self) -&gt; None:\n        \"\"\"\n        Extract the variables from the log messages.\n        \"\"\"\n\n        for row in tqdm(\n            self.parsed_templates.itertuples(),\n            desc=\"Extracting variables from each log message...\",\n            total=len(self.parsed_templates),\n        ):\n            template_id = row.TemplateId\n            variable_indices = row.VariableIndices\n\n            mask = self.logdf[\"TemplateId\"] == template_id\n            for i in variable_indices:\n                col_name = f\"{template_id}_{str(i)}\"\n                self.logdf.loc[mask, col_name] = self.logdf.loc[mask, \"Tokenized\"].str[\n                    i\n                ]\n\n            for i, col_name in enumerate(self.rex.keys()):\n                self.logdf.loc[mask, col_name] = self.logdf.loc[\n                    mask, \"Replaced by regex\"\n                ].str[i]\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain.__init__","title":"<code>__init__(indir='.', depth=4, st=0.4, max_children=100, rex={}, skip_writeout=False, message_prefix='.*')</code>","text":"<p>Initialize a Drain-based parser.</p> <p>Parameters:</p> Name Type Description Default <code>indir</code> <code>str</code> <p>the input directory stores the input log file name</p> <code>'.'</code> <code>depth</code> <code>int</code> <p>depth of all leaf nodes</p> <code>4</code> <code>st</code> <code>float</code> <p>similarity threshold</p> <code>0.4</code> <code>max_children</code> <code>int</code> <p>max number of children of an internal node</p> <code>100</code> <code>rex</code> <code>dict</code> <p>regular expressions used in preprocessing, provided as a dictionary from field name to field regex</p> <code>{}</code> <code>skip_writeout</code> <code>bool</code> <p>whether to skip writing out the parsed log file, templates and variables.</p> <code>False</code> <code>message_prefix</code> <code>str</code> <p>prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.</p> <code>'.*'</code> Source code in <code>src/sawmill/drain.py</code> <pre><code>def __init__(\n    self,\n    indir: str = \".\",\n    depth: int = 4,\n    st: float = 0.4,\n    max_children: int = 100,\n    rex: dict = {},\n    skip_writeout: bool = False,\n    message_prefix: str = r\".*\",\n):\n    \"\"\"\n    Initialize a Drain-based parser.\n\n    Parameters:\n        indir: the input directory stores the input log file name\n        depth: depth of all leaf nodes\n        st: similarity threshold\n        max_children: max number of children of an internal node\n        rex: regular expressions used in preprocessing, provided as a dictionary from field name to field regex\n        skip_writeout: whether to skip writing out the parsed log file, templates and variables.\n        message_prefix: prefix that starts each message of the log file - lines are merged to their preceding line if they do not start with this prefix.\n    \"\"\"\n    self.indir = indir\n    self.depth = depth - 2\n    self.st = st\n    self.max_children = max_children\n    self.rex = rex\n    self.skip_writeout = skip_writeout\n    self.message_prefix = message_prefix\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain.parse","title":"<code>parse(filename)</code>","text":"<p>Parse a log file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the log file to parse (without path).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A tuple of three dataframes, containing the parsed log file, the parsed log templates,</p> <code>DataFrame</code> <p>and the parsed variables respectively.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def parse(self, filename: str) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Parse a log file.\n\n    Parameters:\n        filename: The name of the log file to parse (without path).\n\n    Returns:\n        A tuple of three dataframes, containing the parsed log file, the parsed log templates,\n        and the parsed variables respectively.\n    \"\"\"\n\n    full_path = os.path.join(self.indir, filename)\n    print(f\"Parsing file: {full_path}\")\n    self.filename = filename\n    self.root = Node()\n    self.cluster_list = []\n    self.logdf = self._to_df(full_path)\n\n    tqdm.pandas(desc=\"Determining template for each line...\")\n    self.logdf.progress_apply(self._parse_message, axis=1)\n\n    return self._postprocess()\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._to_df","title":"<code>_to_df(log_file)</code>","text":"<p>Transform a log file into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>log_file</code> <code>str</code> <p>The path to the log file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the log file's lines, tokenized and with regexes replaced.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _to_df(self, log_file: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform a log file into a dataframe.\n\n    Parameters:\n        log_file: The path to the log file.\n\n    Returns:\n        A dataframe containing the log file's lines, tokenized and with regexes replaced.\n    \"\"\"\n\n    log_messages = []\n    linecount = 0\n\n    with open(log_file, \"r\") as f:\n        log_message = \"\"\n\n        for line in tqdm(f.readlines(), desc=\"Reading and tokenizing log lines...\"):\n            line = line.strip()\n\n            if re.match(self.message_prefix, line):\n                if log_message:\n                    try:\n                        log_messages.append(self._preprocess(log_message))\n                        linecount += 1\n                    except Exception as e:\n                        raise ValueError\n                log_message = line\n            else:\n                log_message += \" \" + line\n\n        if log_message:\n            try:\n                log_messages.append(self._preprocess(log_message))\n                linecount += 1\n            except Exception as e:\n                raise ValueError\n\n    logdf = pd.DataFrame(\n        log_messages, columns=[\"Message\", \"Tokenized\", \"Replaced by regex\"]\n    )\n    logdf[\"MsgId\"] = range(len(logdf))\n    return logdf\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._preprocess","title":"<code>_preprocess(msg)</code>","text":"<p>Preprocess a message of a log file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to preprocess.</p> required <p>Returns:</p> Type Description <code>tuple[str, list[str], list[str]]</code> <p>A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _preprocess(self, msg: str) -&gt; tuple[str, list[str], list[str]]:\n    \"\"\"\n    Preprocess a message of a log file.\n\n    Parameters:\n        msg: The message to preprocess.\n\n    Returns:\n        A tuple containing the original message, the tokenized message, and a list of the values replaced by regexes.\n    \"\"\"\n\n    msg = msg.strip()\n\n    regex_matches = []\n    for i, rex in enumerate(self.rex.values()):\n        matches = re.findall(rex, msg)  ##### ASSUMPTION: only 1 match of interest\n        regex_matches.append(matches[0] if matches else \"\")\n        msg = re.sub(rex, \"&lt;*\" + str(i) + \"&gt;\", msg, count=1)\n\n    pattern = r'([=,\\{\\}\\[\\]\\(\\);\"\\'])'  # Add spaces around punctuation\n    msg = re.sub(pattern, r\" \\1 \", msg)\n    pattern = r\"(?&lt;=\\D):|:(?=\\D)\"  # Colons not in timestamps\n    msg = re.sub(pattern, \" : \", msg)\n\n    return (msg, msg.strip().split(), regex_matches)\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._parse_message","title":"<code>_parse_message(msg)</code>","text":"<p>Parse a single log message and add it to the Drain parse tree in the appropriate cluster.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Series</code> <p>The log message to parse.</p> required Source code in <code>src/sawmill/drain.py</code> <pre><code>def _parse_message(self, msg: pd.Series) -&gt; None:\n    \"\"\"\n    Parse a single log message and add it to the Drain parse tree in the appropriate cluster.\n\n    Parameters:\n        msg: The log message to parse.\n    \"\"\"\n\n    line_id = msg[\"MsgId\"]\n    tokenized = msg[\"Tokenized\"]\n    cluster = self._tree_search(self.root, tokenized)\n\n    if cluster is None:\n        new_cluster = Cluster(template=tokenized, message_ids=[line_id])\n        self.cluster_list.append(new_cluster)\n        self._add_cluster_to_tree(self.root, new_cluster)\n    else:\n        new_template = self._get_updated_template(tokenized, cluster.template)\n        cluster.message_ids.append(line_id)\n        if \" \".join(new_template) != \" \".join(cluster.template):\n            cluster.template = new_template\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._tree_search","title":"<code>_tree_search(root, tokenized)</code>","text":"<p>Search the Drain parse tree for a cluster matching <code>tokenized</code>.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Node</code> <p>The root of the Drain parse tree.</p> required <code>tokenized</code> <code>list[str]</code> <p>The tokenized log message to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Cluster]</code> <p>The cluster in the Drain parse tree that matches <code>tokenized</code>,</p> <code>Optional[Cluster]</code> <p>or None if no such cluster exists.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _tree_search(self, root: Node, tokenized: list[str]) -&gt; Optional[Cluster]:\n    \"\"\"\n    Search the Drain parse tree for a cluster matching `tokenized`.\n\n    Parameters:\n        root: The root of the Drain parse tree.\n        tokenized: The tokenized log message to search for.\n\n    Returns:\n        The cluster in the Drain parse tree that matches `tokenized`,\n        or None if no such cluster exists.\n    \"\"\"\n\n    num_toks = len(tokenized)\n    if num_toks not in root.children:\n        return None\n\n    node = root.children[num_toks]\n\n    depth = 1\n    for token in tokenized:\n        if depth &gt;= self.depth or depth &gt; num_toks:\n            break\n        if token in node.children:\n            node = node.children[token]\n        elif \"&lt;*&gt;\" in node.children:\n            node = node.children[\"&lt;*&gt;\"]\n        else:\n            return None\n        depth += 1\n\n    cluster_list = node.children\n    returned_cluster = self._find_cluster(cluster_list, tokenized)\n\n    return returned_cluster\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._add_cluster_to_tree","title":"<code>_add_cluster_to_tree(root, cluster)</code>","text":"<p>Add a cluster to the Drain parse tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Node</code> <p>The root of the Drain parse tree.</p> required <code>cluster</code> <code>Cluster</code> <p>The cluster to add.</p> required Source code in <code>src/sawmill/drain.py</code> <pre><code>def _add_cluster_to_tree(self, root: Node, cluster: Cluster) -&gt; None:\n    \"\"\"\n    Add a cluster to the Drain parse tree.\n\n    Parameters:\n        root: The root of the Drain parse tree.\n        cluster: The cluster to add.\n    \"\"\"\n\n    # Add a node to the first layer of the tree representing the length of the log message.\n    length = len(cluster.template)\n    first_layer_node = None\n    if length not in root.children:\n        first_layer_node = Node(depth=1, id=length)\n        root.children[length] = first_layer_node\n    else:\n        first_layer_node = root.children[length]\n\n    # Traverse the tree to add the new cluster.\n    node = first_layer_node\n    depth = 1\n    for token in cluster.template:\n        # If out of depth, add current log cluster to the leaf node\n        if depth &gt;= self.depth or depth &gt; length:\n            if len(node.children) == 0:\n                node.children = [cluster]\n            else:\n                node.children.append(cluster)\n            break\n\n        # If token not matched in this layer of existing tree.\n        if token not in node.children:\n            if not any(char.isdigit() for char in token):\n                if \"&lt;*&gt;\" in node.children:\n                    if len(node.children) &lt; self.max_children:\n                        new_node = Node(depth=depth + 1, id=token)\n                        node.children[token] = new_node\n                        node = new_node\n                    else:\n                        node = node.children[\"&lt;*&gt;\"]\n                else:\n                    if len(node.children) + 1 &lt; self.max_children:\n                        new_node = Node(depth=depth + 1, id=token)\n                        node.children[token] = new_node\n                        node = new_node\n                    elif len(node.children) + 1 == self.max_children:\n                        new_node = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                        node.children[\"&lt;*&gt;\"] = new_node\n                        node = new_node\n                    else:\n                        node = node.children[\"&lt;*&gt;\"]\n            else:\n                if \"&lt;*&gt;\" not in node.children:\n                    node.children[\"&lt;*&gt;\"] = Node(depth=depth + 1, id=\"&lt;*&gt;\")\n                node = node.children[\"&lt;*&gt;\"]\n\n        # If the token is matched\n        else:\n            node = node.children[token]\n\n        depth += 1\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._similarity","title":"<code>_similarity(seq1, seq2)</code>","text":"<p>Determine the fraction of tokens in <code>seq1</code> that are identical to the corresponding token in <code>seq2</code>. Also return the number of parameters in <code>seq1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>seq1</code> <code>list[str]</code> <p>The first sequence.</p> required <code>seq2</code> <code>list[str]</code> <p>The second sequence.</p> required <p>Returns:</p> Type Description <code>tuple[float, int]</code> <p>A tuple containing the fraction of identical tokens and the number of parameters in <code>seq1</code>.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _similarity(self, seq1: list[str], seq2: list[str]) -&gt; tuple[float, int]:\n    \"\"\"\n    Determine the fraction of tokens in `seq1` that are identical to the corresponding token in `seq2`.\n    Also return the number of parameters in `seq1`.\n\n    Parameters:\n        seq1: The first sequence.\n        seq2: The second sequence.\n\n    Returns:\n        A tuple containing the fraction of identical tokens and the number of parameters in `seq1`.\n    \"\"\"\n    assert len(seq1) == len(seq2)\n    matches = 0\n    num_params = 0\n\n    for token1, token2 in zip(seq1, seq2):\n        if token1 == \"&lt;*&gt;\":\n            num_params += 1\n        if token1 == token2:\n            matches += 1\n\n    similarity = float(matches) / len(seq1)\n\n    return similarity, num_params\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._find_cluster","title":"<code>_find_cluster(cluster_list, seq)</code>","text":"<p>Find the cluster in <code>cluster_list</code> that is most similar to <code>seq</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_list</code> <code>list[Cluster]</code> <p>The list of clusters to search.</p> required <code>seq</code> <code>list[str]</code> <p>The sequence of tokens to compare to.</p> required <p>Returns:</p> Type Description <code>Optional[Cluster]</code> <p>The cluster in <code>cluster_list</code> that is most similar to <code>seq</code>,</p> <code>Optional[Cluster]</code> <p>or None if no cluster is sufficiently similar.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _find_cluster(\n    self, cluster_list: list[Cluster], seq: list[str]\n) -&gt; Optional[Cluster]:\n    \"\"\"\n    Find the cluster in `cluster_list` that is most similar to `seq`.\n\n    Parameters:\n        cluster_list: The list of clusters to search.\n        seq: The sequence of tokens to compare to.\n\n    Returns:\n        The cluster in `cluster_list` that is most similar to `seq`,\n        or None if no cluster is sufficiently similar.\n    \"\"\"\n\n    max_similarity = -1\n    max_num_params = -1\n    max_cluster = None\n\n    for cluster in cluster_list:\n        similarity, num_params = self._similarity(cluster.template, seq)\n        if similarity &gt; max_similarity or (\n            similarity == max_similarity and num_params &gt; max_num_params\n        ):\n            max_similarity = similarity\n            max_num_params = num_params\n            max_cluster = cluster\n\n    if max_similarity &gt;= self.st:\n        return max_cluster\n    else:\n        return None\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._get_updated_template","title":"<code>_get_updated_template(template, msg)</code>","text":"<p>Get the updated template from matching <code>msg</code> to <code>template</code>.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>list[str]</code> <p>The template to match to.</p> required <code>msg</code> <code>list[str]</code> <p>The message to match.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The updated template.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _get_updated_template(self, template: list[str], msg: list[str]) -&gt; list[str]:\n    \"\"\"\n    Get the updated template from matching `msg` to `template`.\n\n    Parameters:\n        template: The template to match to.\n        msg: The message to match.\n\n    Returns:\n        The updated template.\n    \"\"\"\n\n    assert len(template) == len(msg)\n    updated_template = []\n\n    for i, word in enumerate(template):\n        if word == msg[i]:\n            updated_template.append(word)\n        else:\n            updated_template.append(\"&lt;*&gt;\")\n\n    return updated_template\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._preceding_3","title":"<code>_preceding_3(parsed_templates, x)</code>  <code>staticmethod</code>","text":"<p>Get the 3 tokens preceding the variable <code>x</code> in the template.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_templates</code> <code>DataFrame</code> <p>The dataframe containing information about the parsed templates.</p> required <code>x</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The 3 tokens preceding the variable <code>x</code> in the template.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>@staticmethod\ndef _preceding_3(parsed_templates: pd.DataFrame, x: str) -&gt; list[str]:\n    \"\"\"\n    Get the 3 tokens preceding the variable `x` in the template.\n\n    Parameters:\n        parsed_templates: The dataframe containing information about the parsed templates.\n        x: The name of the variable.\n\n    Returns:\n        The 3 tokens preceding the variable `x` in the template.\n    \"\"\"\n\n    splitx = x.split(\"_\")\n    if len(splitx) != 2:\n        return []\n    id = splitx[0]\n    position = int(splitx[1])\n    start_position = max(0, position - 3)\n    return (\n        parsed_templates[parsed_templates[\"TemplateId\"] == id][\"TemplateText\"]\n        .values[0]\n        .split()[start_position:position]\n    )\n</code></pre>"},{"location":"reference/src/sawmill/drain/#src.sawmill.drain.Drain._extract_variables","title":"<code>_extract_variables()</code>","text":"<p>Extract the variables from the log messages.</p> Source code in <code>src/sawmill/drain.py</code> <pre><code>def _extract_variables(self) -&gt; None:\n    \"\"\"\n    Extract the variables from the log messages.\n    \"\"\"\n\n    for row in tqdm(\n        self.parsed_templates.itertuples(),\n        desc=\"Extracting variables from each log message...\",\n        total=len(self.parsed_templates),\n    ):\n        template_id = row.TemplateId\n        variable_indices = row.VariableIndices\n\n        mask = self.logdf[\"TemplateId\"] == template_id\n        for i in variable_indices:\n            col_name = f\"{template_id}_{str(i)}\"\n            self.logdf.loc[mask, col_name] = self.logdf.loc[mask, \"Tokenized\"].str[\n                i\n            ]\n\n        for i, col_name in enumerate(self.rex.keys()):\n            self.logdf.loc[mask, col_name] = self.logdf.loc[\n                mask, \"Replaced by regex\"\n            ].str[i]\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/","title":"EdgeOccurrenceTree","text":""},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree","title":"<code>EdgeOccurrenceTree</code>","text":"<p>A tree of DAGs based on the ATE cluster they belong to.</p> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>class EdgeOccurrenceTree:\n    \"\"\"\n    A tree of DAGs based on the ATE cluster they belong to.\n    \"\"\"\n\n    def __init__(self, cluster_id: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialize a tree node with a specific cluster id.\n\n        Parameters:\n            cluster_id: The cluster id of the DAGs that belong to this node.\n        \"\"\"\n\n        self.cluster_id = cluster_id\n        self.left = None\n        self.right = None\n\n    @staticmethod\n    def build_tree(linked: np.ndarray, leaves: list[int]) -&gt; tuple[Self, int]:\n        \"\"\"\n        Build a tree from a linkage matrix.\n\n        Parameters:\n            linked: The linkage matrix.\n            leaves: The list of leaf nodes.\n\n        Returns:\n            A tuple containing the root of the tree, and the index of the next cluster to be merged.\n        \"\"\"\n\n        # Base case: if there is only one cluster, return it as a leaf.\n        if len(leaves) == 1:\n            return EdgeOccurrenceTree(cluster_id=leaves[0]), -1\n\n        # Otherwise, build the tree recursively.\n        root = EdgeOccurrenceTree()\n        curr = root\n        i = len(linked) - 1\n        while i &gt; -1:\n            # Linked contains 4 elements: cluster1, cluster2, distance, num_observations\n            # They represent the clusters that were merged, the distance between them, and\n            # the number of observations in the new cluster.\n            c1, c2, _, _ = linked[i]\n\n            if c1 not in leaves and c2 not in leaves:\n                curr.left, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n                curr.right, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n                break\n            if c1 in leaves:\n                curr.left = EdgeOccurrenceTree(leaves.index(c1))\n                curr.right = EdgeOccurrenceTree()\n                curr = curr.right\n            if c2 in leaves:\n                curr.right = EdgeOccurrenceTree(leaves.index(c2))\n                break\n            i -= 1\n        root = EdgeOccurrenceTree._cleanup_tree(root)\n        return root, i\n\n    @staticmethod\n    def _cleanup_tree(root: Optional[Self]) -&gt; Self:\n        \"\"\"\n        Clean up the tree by removing nodes that have only one child, and nodes that have no\n        children and are not leaves.\n\n        Parameters:\n            root: The root of the tree.\n\n        Returns:\n            The root of the cleaned up tree.\n        \"\"\"\n\n        if root is None:\n            return None\n\n        # Recursively clean up left and right subtrees\n        root.left = EdgeOccurrenceTree._cleanup_tree(root.left)\n        root.right = EdgeOccurrenceTree._cleanup_tree(root.right)\n\n        # If the current node has only one child, replace the node with its child\n        if root.left is None and root.right is not None:\n            return root.right\n        elif root.left is not None and root.right is None:\n            return root.left\n\n        # If the current node has no left and right child and is not a leaf, remove the node\n        if root.left is None and root.right is None and root.cluster_id is None:\n            return None\n\n        return root\n\n    def print_tree(self, depth: int = 0) -&gt; None:\n        \"\"\"\n        Print the tree in a readable format.\n\n        Parameters:\n            depth: The depth of the current node in the tree.\n        \"\"\"\n\n        prefix = \"\"\n        for _ in range(depth):\n            prefix += \"-\"\n        if self.cluster_id is not None:\n            print(prefix + str(self.cluster_id))\n        else:\n            print(prefix + \"node\")\n        if self.left:\n            self.left.print_tree(depth + 1)\n        if self.right:\n            self.right.print_tree(depth + 1)\n\n    def assign_dags_to_nodes(self, cluster_mapping: dict[nx.DiGraph, int]) -&gt; None:\n        \"\"\"\n        Assign each DAG to the node it belongs to, based on `cluster_mapping`.\n\n        Parameters:\n            cluster_mapping: A dictionary mapping DAGs to cluster id's.\n        \"\"\"\n        self.num_dags = 0\n\n        # If leaf, assign DAGs and set count.\n        if self.cluster_id is not None:\n            self.dags = [\n                key\n                for key in cluster_mapping.keys()\n                if cluster_mapping[key] == self.cluster_id\n            ]\n            self.num_dags = len(self.dags)\n\n        # Otherwise, recurse for children and retireve counts.\n        if self.left:\n            self.left.assign_dags_to_nodes(cluster_mapping)\n            self.num_dags += self.left.num_dags\n        if self.right:\n            self.right.assign_dags_to_nodes(cluster_mapping)\n            self.num_dags += self.right.num_dags\n\n    def count_edge_occurrences(\n        self, treatment: str, outcome: str, dag: nx.DiGraph\n    ) -&gt; None:\n        \"\"\"\n        Recursively count the number of times each edge occurs amongst the DAGs\n        assigned to all the children of this node, omitting the edge from treatment -&gt; outcome,\n        since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.\n\n        Parameters:\n            treatment: The treatment variable.\n            outcome: The outcome variable.\n            dag: The optional dag structure to ignore.\n        \"\"\"\n        self.edge_counts: Types.EdgeCountDict = defaultdict(int)\n\n        # If leaf, actually compute count.\n        if self.cluster_id is not None:\n            edges_to_ignore = [(treatment, outcome)]\n            if dag:\n                edges_to_ignore.extend(dag.edges)\n            for graph in self.dags:\n                for edge in graph.edges:\n                    if edge not in edges_to_ignore:\n                        self.edge_counts[edge] += 1\n\n        # Otherwise, derive counts from children.\n        if self.left:\n            self.left.count_edge_occurrences(treatment, outcome, dag)\n            for key in self.left.edge_counts.keys():\n                self.edge_counts[key] += self.left.edge_counts[key]\n        if self.right:\n            self.right.count_edge_occurrences(treatment, outcome, dag)\n            for key in self.right.edge_counts.keys():\n                self.edge_counts[key] += self.right.edge_counts[key]\n\n        # Compute statistics.\n        freq_counts = list(self.edge_counts.values())\n        if len(freq_counts) == 0:\n            self.mean = None\n            self.std_dev = None\n        else:\n            self.mean = np.mean(freq_counts)\n            self.std_dev = np.std(freq_counts)\n\n    def calculate_edge_expectancy(\n        self, totals: tuple[int, Types.EdgeCountDict] = None\n    ) -&gt; None:\n        \"\"\"\n        For each edge at each node, calculate what percent over or under\n        expectancy the edge is at in relationship to its parent.\n\n        Parameters:\n            totals: A tuple containing the total number of DAGs and the mapping from\n                edges to their counts for the parent of this node.\n        \"\"\"\n        # At root node, calculate expectancy\n        if totals is None:\n            totals = (self.num_dags, self.edge_counts)\n\n        # Otherwise, calculate expectancy based on parent.\n        total_dags, total_edges = totals\n        self.percent_expectancy = defaultdict(float)\n\n        for edge in self.edge_counts.keys():\n            expected = self.num_dags / total_dags * total_edges[edge]\n            self.percent_expectancy[edge] = (\n                self.edge_counts[edge] - expected\n            ) / expected\n\n        # Recurse for children.\n        if self.left:\n            self.left.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n        if self.right:\n            self.right.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n\n    def find_outliers_in_tree(self, threshold: float = 0) -&gt; None:\n        \"\"\"\n        Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an\n        edge that is below expectancy on one side of the tree, and above on the other side, and\n        optionally, over some threshold on both sides.\n\n        Parameters:\n            threshold: The threshold for an edge to be considered an outlier.\n        \"\"\"\n\n        # If able to compare, find outliers.\n        if self.left and self.right:\n            self.left.outliers = {}\n            self.right.outliers = {}\n            edges = set(self.left.edge_counts.keys()).union(\n                set(self.right.edge_counts.keys())\n            )\n            for edge in edges:\n                if (\n                    np.sign(self.left.percent_expectancy[edge])\n                    != np.sign(self.right.percent_expectancy[edge])\n                    and abs(self.left.percent_expectancy[edge]) &gt; threshold\n                    and abs(self.right.percent_expectancy[edge]) &gt; threshold\n                ):\n                    self.left.outliers[edge] = self.left.percent_expectancy[edge]\n                    self.right.outliers[edge] = self.right.percent_expectancy[edge]\n\n        # Recurse for children.\n        if self.left:\n            self.left.find_outliers_in_tree(threshold)\n        if self.right:\n            self.right.find_outliers_in_tree(threshold)\n\n    def find_outliers_per_cluster(\n        self,\n        dag: nx.DiGraph,\n    ) -&gt; tuple[Types.EdgeCountDict, dict[Types.Edge, float]]:\n        \"\"\"\n        Collect the edge counts and outliers found earlier into appropriate dictionaries\n        per cluster.\n\n        Parameters:\n            dag: The DAG to ignore when collecting outliers.\n\n        Returns:\n            A tuple containing the following: a dictionary mapping cluster id's to edge counts,\n            and a dictionary mapping cluster id's to outlier edges.\n        \"\"\"\n\n        cluster_edge_count = {}\n        cluster_outliers = {}\n\n        # If leaf, add to cluster counts.\n        if self.cluster_id is not None:\n            cluster_edge_count[self.cluster_id] = self.edge_counts\n            edges_to_ignore = dag.edges if dag is not None else []\n            cluster_outliers[self.cluster_id] = {\n                edge: self.outliers[edge]\n                for edge in self.outliers\n                if edge not in edges_to_ignore\n            }\n\n        # Otherwise, recurse for children.\n        if self.left:\n            lec, lo = self.left.find_outliers_per_cluster(dag)\n            cluster_edge_count.update(lec)\n            cluster_outliers.update(lo)\n        if self.right:\n            rec, ro = self.right.find_outliers_per_cluster(dag)\n            cluster_edge_count.update(rec)\n            cluster_outliers.update(ro)\n\n        return cluster_edge_count, cluster_outliers\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.__init__","title":"<code>__init__(cluster_id=None)</code>","text":"<p>Initialize a tree node with a specific cluster id.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>Optional[str]</code> <p>The cluster id of the DAGs that belong to this node.</p> <code>None</code> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def __init__(self, cluster_id: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialize a tree node with a specific cluster id.\n\n    Parameters:\n        cluster_id: The cluster id of the DAGs that belong to this node.\n    \"\"\"\n\n    self.cluster_id = cluster_id\n    self.left = None\n    self.right = None\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.build_tree","title":"<code>build_tree(linked, leaves)</code>  <code>staticmethod</code>","text":"<p>Build a tree from a linkage matrix.</p> <p>Parameters:</p> Name Type Description Default <code>linked</code> <code>ndarray</code> <p>The linkage matrix.</p> required <code>leaves</code> <code>list[int]</code> <p>The list of leaf nodes.</p> required <p>Returns:</p> Type Description <code>tuple[Self, int]</code> <p>A tuple containing the root of the tree, and the index of the next cluster to be merged.</p> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>@staticmethod\ndef build_tree(linked: np.ndarray, leaves: list[int]) -&gt; tuple[Self, int]:\n    \"\"\"\n    Build a tree from a linkage matrix.\n\n    Parameters:\n        linked: The linkage matrix.\n        leaves: The list of leaf nodes.\n\n    Returns:\n        A tuple containing the root of the tree, and the index of the next cluster to be merged.\n    \"\"\"\n\n    # Base case: if there is only one cluster, return it as a leaf.\n    if len(leaves) == 1:\n        return EdgeOccurrenceTree(cluster_id=leaves[0]), -1\n\n    # Otherwise, build the tree recursively.\n    root = EdgeOccurrenceTree()\n    curr = root\n    i = len(linked) - 1\n    while i &gt; -1:\n        # Linked contains 4 elements: cluster1, cluster2, distance, num_observations\n        # They represent the clusters that were merged, the distance between them, and\n        # the number of observations in the new cluster.\n        c1, c2, _, _ = linked[i]\n\n        if c1 not in leaves and c2 not in leaves:\n            curr.left, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n            curr.right, i = EdgeOccurrenceTree.build_tree(linked[:i], leaves)\n            break\n        if c1 in leaves:\n            curr.left = EdgeOccurrenceTree(leaves.index(c1))\n            curr.right = EdgeOccurrenceTree()\n            curr = curr.right\n        if c2 in leaves:\n            curr.right = EdgeOccurrenceTree(leaves.index(c2))\n            break\n        i -= 1\n    root = EdgeOccurrenceTree._cleanup_tree(root)\n    return root, i\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree._cleanup_tree","title":"<code>_cleanup_tree(root)</code>  <code>staticmethod</code>","text":"<p>Clean up the tree by removing nodes that have only one child, and nodes that have no children and are not leaves.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Optional[Self]</code> <p>The root of the tree.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The root of the cleaned up tree.</p> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>@staticmethod\ndef _cleanup_tree(root: Optional[Self]) -&gt; Self:\n    \"\"\"\n    Clean up the tree by removing nodes that have only one child, and nodes that have no\n    children and are not leaves.\n\n    Parameters:\n        root: The root of the tree.\n\n    Returns:\n        The root of the cleaned up tree.\n    \"\"\"\n\n    if root is None:\n        return None\n\n    # Recursively clean up left and right subtrees\n    root.left = EdgeOccurrenceTree._cleanup_tree(root.left)\n    root.right = EdgeOccurrenceTree._cleanup_tree(root.right)\n\n    # If the current node has only one child, replace the node with its child\n    if root.left is None and root.right is not None:\n        return root.right\n    elif root.left is not None and root.right is None:\n        return root.left\n\n    # If the current node has no left and right child and is not a leaf, remove the node\n    if root.left is None and root.right is None and root.cluster_id is None:\n        return None\n\n    return root\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.print_tree","title":"<code>print_tree(depth=0)</code>","text":"<p>Print the tree in a readable format.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth of the current node in the tree.</p> <code>0</code> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def print_tree(self, depth: int = 0) -&gt; None:\n    \"\"\"\n    Print the tree in a readable format.\n\n    Parameters:\n        depth: The depth of the current node in the tree.\n    \"\"\"\n\n    prefix = \"\"\n    for _ in range(depth):\n        prefix += \"-\"\n    if self.cluster_id is not None:\n        print(prefix + str(self.cluster_id))\n    else:\n        print(prefix + \"node\")\n    if self.left:\n        self.left.print_tree(depth + 1)\n    if self.right:\n        self.right.print_tree(depth + 1)\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.assign_dags_to_nodes","title":"<code>assign_dags_to_nodes(cluster_mapping)</code>","text":"<p>Assign each DAG to the node it belongs to, based on <code>cluster_mapping</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_mapping</code> <code>dict[DiGraph, int]</code> <p>A dictionary mapping DAGs to cluster id's.</p> required Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def assign_dags_to_nodes(self, cluster_mapping: dict[nx.DiGraph, int]) -&gt; None:\n    \"\"\"\n    Assign each DAG to the node it belongs to, based on `cluster_mapping`.\n\n    Parameters:\n        cluster_mapping: A dictionary mapping DAGs to cluster id's.\n    \"\"\"\n    self.num_dags = 0\n\n    # If leaf, assign DAGs and set count.\n    if self.cluster_id is not None:\n        self.dags = [\n            key\n            for key in cluster_mapping.keys()\n            if cluster_mapping[key] == self.cluster_id\n        ]\n        self.num_dags = len(self.dags)\n\n    # Otherwise, recurse for children and retireve counts.\n    if self.left:\n        self.left.assign_dags_to_nodes(cluster_mapping)\n        self.num_dags += self.left.num_dags\n    if self.right:\n        self.right.assign_dags_to_nodes(cluster_mapping)\n        self.num_dags += self.right.num_dags\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.count_edge_occurrences","title":"<code>count_edge_occurrences(treatment, outcome, dag)</code>","text":"<p>Recursively count the number of times each edge occurs amongst the DAGs assigned to all the children of this node, omitting the edge from treatment -&gt; outcome, since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The outcome variable.</p> required <code>dag</code> <code>DiGraph</code> <p>The optional dag structure to ignore.</p> required Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def count_edge_occurrences(\n    self, treatment: str, outcome: str, dag: nx.DiGraph\n) -&gt; None:\n    \"\"\"\n    Recursively count the number of times each edge occurs amongst the DAGs\n    assigned to all the children of this node, omitting the edge from treatment -&gt; outcome,\n    since this always exists. If a DAG is passed in, ignore the edges in that DAG as well.\n\n    Parameters:\n        treatment: The treatment variable.\n        outcome: The outcome variable.\n        dag: The optional dag structure to ignore.\n    \"\"\"\n    self.edge_counts: Types.EdgeCountDict = defaultdict(int)\n\n    # If leaf, actually compute count.\n    if self.cluster_id is not None:\n        edges_to_ignore = [(treatment, outcome)]\n        if dag:\n            edges_to_ignore.extend(dag.edges)\n        for graph in self.dags:\n            for edge in graph.edges:\n                if edge not in edges_to_ignore:\n                    self.edge_counts[edge] += 1\n\n    # Otherwise, derive counts from children.\n    if self.left:\n        self.left.count_edge_occurrences(treatment, outcome, dag)\n        for key in self.left.edge_counts.keys():\n            self.edge_counts[key] += self.left.edge_counts[key]\n    if self.right:\n        self.right.count_edge_occurrences(treatment, outcome, dag)\n        for key in self.right.edge_counts.keys():\n            self.edge_counts[key] += self.right.edge_counts[key]\n\n    # Compute statistics.\n    freq_counts = list(self.edge_counts.values())\n    if len(freq_counts) == 0:\n        self.mean = None\n        self.std_dev = None\n    else:\n        self.mean = np.mean(freq_counts)\n        self.std_dev = np.std(freq_counts)\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.calculate_edge_expectancy","title":"<code>calculate_edge_expectancy(totals=None)</code>","text":"<p>For each edge at each node, calculate what percent over or under expectancy the edge is at in relationship to its parent.</p> <p>Parameters:</p> Name Type Description Default <code>totals</code> <code>tuple[int, EdgeCountDict]</code> <p>A tuple containing the total number of DAGs and the mapping from edges to their counts for the parent of this node.</p> <code>None</code> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def calculate_edge_expectancy(\n    self, totals: tuple[int, Types.EdgeCountDict] = None\n) -&gt; None:\n    \"\"\"\n    For each edge at each node, calculate what percent over or under\n    expectancy the edge is at in relationship to its parent.\n\n    Parameters:\n        totals: A tuple containing the total number of DAGs and the mapping from\n            edges to their counts for the parent of this node.\n    \"\"\"\n    # At root node, calculate expectancy\n    if totals is None:\n        totals = (self.num_dags, self.edge_counts)\n\n    # Otherwise, calculate expectancy based on parent.\n    total_dags, total_edges = totals\n    self.percent_expectancy = defaultdict(float)\n\n    for edge in self.edge_counts.keys():\n        expected = self.num_dags / total_dags * total_edges[edge]\n        self.percent_expectancy[edge] = (\n            self.edge_counts[edge] - expected\n        ) / expected\n\n    # Recurse for children.\n    if self.left:\n        self.left.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n    if self.right:\n        self.right.calculate_edge_expectancy((self.num_dags, self.edge_counts))\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.find_outliers_in_tree","title":"<code>find_outliers_in_tree(threshold=0)</code>","text":"<p>Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an edge that is below expectancy on one side of the tree, and above on the other side, and optionally, over some threshold on both sides.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for an edge to be considered an outlier.</p> <code>0</code> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def find_outliers_in_tree(self, threshold: float = 0) -&gt; None:\n    \"\"\"\n    Find outlier edges, based on the percent expectancy of each edge. Define an outlier as an\n    edge that is below expectancy on one side of the tree, and above on the other side, and\n    optionally, over some threshold on both sides.\n\n    Parameters:\n        threshold: The threshold for an edge to be considered an outlier.\n    \"\"\"\n\n    # If able to compare, find outliers.\n    if self.left and self.right:\n        self.left.outliers = {}\n        self.right.outliers = {}\n        edges = set(self.left.edge_counts.keys()).union(\n            set(self.right.edge_counts.keys())\n        )\n        for edge in edges:\n            if (\n                np.sign(self.left.percent_expectancy[edge])\n                != np.sign(self.right.percent_expectancy[edge])\n                and abs(self.left.percent_expectancy[edge]) &gt; threshold\n                and abs(self.right.percent_expectancy[edge]) &gt; threshold\n            ):\n                self.left.outliers[edge] = self.left.percent_expectancy[edge]\n                self.right.outliers[edge] = self.right.percent_expectancy[edge]\n\n    # Recurse for children.\n    if self.left:\n        self.left.find_outliers_in_tree(threshold)\n    if self.right:\n        self.right.find_outliers_in_tree(threshold)\n</code></pre>"},{"location":"reference/src/sawmill/edge_occurrence_tree/#src.sawmill.edge_occurrence_tree.EdgeOccurrenceTree.find_outliers_per_cluster","title":"<code>find_outliers_per_cluster(dag)</code>","text":"<p>Collect the edge counts and outliers found earlier into appropriate dictionaries per cluster.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DiGraph</code> <p>The DAG to ignore when collecting outliers.</p> required <p>Returns:</p> Type Description <code>EdgeCountDict</code> <p>A tuple containing the following: a dictionary mapping cluster id's to edge counts,</p> <code>dict[Edge, float]</code> <p>and a dictionary mapping cluster id's to outlier edges.</p> Source code in <code>src/sawmill/edge_occurrence_tree.py</code> <pre><code>def find_outliers_per_cluster(\n    self,\n    dag: nx.DiGraph,\n) -&gt; tuple[Types.EdgeCountDict, dict[Types.Edge, float]]:\n    \"\"\"\n    Collect the edge counts and outliers found earlier into appropriate dictionaries\n    per cluster.\n\n    Parameters:\n        dag: The DAG to ignore when collecting outliers.\n\n    Returns:\n        A tuple containing the following: a dictionary mapping cluster id's to edge counts,\n        and a dictionary mapping cluster id's to outlier edges.\n    \"\"\"\n\n    cluster_edge_count = {}\n    cluster_outliers = {}\n\n    # If leaf, add to cluster counts.\n    if self.cluster_id is not None:\n        cluster_edge_count[self.cluster_id] = self.edge_counts\n        edges_to_ignore = dag.edges if dag is not None else []\n        cluster_outliers[self.cluster_id] = {\n            edge: self.outliers[edge]\n            for edge in self.outliers\n            if edge not in edges_to_ignore\n        }\n\n    # Otherwise, recurse for children.\n    if self.left:\n        lec, lo = self.left.find_outliers_per_cluster(dag)\n        cluster_edge_count.update(lec)\n        cluster_outliers.update(lo)\n    if self.right:\n        rec, ro = self.right.find_outliers_per_cluster(dag)\n        cluster_edge_count.update(rec)\n        cluster_outliers.update(ro)\n\n    return cluster_edge_count, cluster_outliers\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/","title":"EdgeStateMatrix","text":""},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix","title":"<code>EdgeStateMatrix</code>","text":"<p>A class for managing an edge state matrix.</p> <p>An edge state matrix is square, with the entry (i,j) representing the state of the directed edge between nodes i and j. The state of an edge is one of:      0: The existence of the state is undecided.     -1: The edge does not exist.      1: The edge exists.</p> <p>Self-edges are not allowed. The presence of an edge implies the absence of its inverse.</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>class EdgeStateMatrix:\n    \"\"\"\n    A class for managing an edge state matrix.\n\n    An edge state matrix is square, with the entry (i,j) representing the state\n    of the directed edge between nodes i and j. The state of an edge is one of:\n         0: The existence of the state is undecided.\n        -1: The edge does not exist.\n         1: The edge exists.\n\n    Self-edges are not allowed. The presence of an edge implies the absence of\n    its inverse.\n    \"\"\"\n\n    def __init__(self, variables: list[str]) -&gt; None:\n        \"\"\"\n        Initialize the edge state matrix to the right dimensions and mark self-edges\n        as rejected and all other edges as undecided.\n\n        Parameters:\n            variables: The variables to initialize the edge state matrix based on. This\n                list must include variable NAMES, not tags.\n        \"\"\"\n\n        n = len(variables)\n        self._variables = variables\n        self._m = np.zeros((n, n))\n        for i in range(n):\n            self._m[i, i] = -1\n\n    @property\n    def m(self) -&gt; np.ndarray:\n        \"\"\"\n        Returns the edge state matrix.\n        \"\"\"\n        return self._m\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"\n        Returns the number of nodes.\n        \"\"\"\n        return self._m.shape[0]\n\n    def clear_and_set_from_graph(self, graph: nx.DiGraph) -&gt; None:\n        \"\"\"\n        Clear the edge state matrix and then set it based on the provided graph.\n        In particular, mark all edges in the graph as accepted and all others as rejected.\n\n        Parameters:\n            graph: The graph to use to set the edge states.\n        \"\"\"\n\n        self._m = np.zeros((self.n, self.n))\n        for edge in graph.edges:\n            print(\"Marking edge as accepted: \", edge)\n            self._m[self.idx(edge[0]), self.idx(edge[1])] = 1\n\n        self._m[self._m == 0] = -1\n\n    def clear_and_set_from_matrix(self, m: np.ndarray) -&gt; None:\n        \"\"\"\n        Clear the edge state matrix and then set it based on the provided matrix.\n\n        Parameters:\n            m: The matrix to use to set the edge states.\n        \"\"\"\n\n        self._m = m\n\n    def idx(self, var: str) -&gt; int:\n        \"\"\"\n        Retrieve the index of a variable in the edge state matrix.\n\n        Parameters:\n            var: The name or tag of the variable.\n\n        Returns:\n            The index of the variable in the edge state matrix.\n        \"\"\"\n        return self._variables.index(var)\n\n    def get_edge_state(self, src: str, dst: str) -&gt; str:\n        \"\"\"\n        Get the state of a specific edge.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n\n        Returns:\n            The state of the edge (Accepted, Rejected, or Undecided).\n        \"\"\"\n        src_idx = self.idx(src)\n        dst_idx = self.idx(dst)\n        return self.edge_state_to_str(self._m[src_idx][dst_idx])\n\n    def edge_state_to_str(self, state: int) -&gt; str:\n        \"\"\"\n        Translate between edge value and its interpretation.\n\n        Parameters:\n            state: The state of the edge represented as an integer.\n\n        Returns:\n            The state of the edge (Accepted, Rejected, or Undecided).\n        \"\"\"\n        if state == 0:\n            return \"Undecided\"\n        elif state == -1:\n            return \"Rejected\"\n        elif state == 1:\n            return \"Accepted\"\n        else:\n            raise ValueError(f\"Invalid edge state {state}\")\n\n    def mark_edge(self, src: str, dst: str, state: str) -&gt; list[str]:\n        \"\"\"\n        Mark an edge as being in a specified state.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            state: The state to mark the edge with (Accepted, Rejected, or Undecided).\n\n        Returns:\n            A list of variables that were removed from the partial causal graph as a result\n            of this edge being marked as Accepted.\n\n        Throws:\n            ValueError: If `state` is not one of \"Accepted\", \"Rejected\", or \"Undecided\".\n        \"\"\"\n\n        src_idx = self.idx(src)\n        dst_idx = self.idx(dst)\n\n        if state == \"Accepted\":\n            self._m[src_idx][dst_idx] = 1\n            self._m[dst_idx][src_idx] = -1\n            return self._reject_other_variants(src, dst)\n        elif state == \"Rejected\":\n            self._m[src_idx][dst_idx] = -1\n            return []\n        elif state == \"Undecided\":\n            self._m[src_idx][dst_idx] = 0\n            return []\n        else:\n            raise ValueError(f\"Invalid edge state {state}\")\n\n    def _reject_other_variants(self, src: str, dst: str) -&gt; list[str]:\n        \"\"\"\n        Mark any edges that touch a variable different from `src` and `dst`, but sharing\n        the same base variable as `src` or `dst`, as rejected. Also remove any such variables\n        from the partial causal graph.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n\n        Returns:\n            A list of variables that were removed from the partial causal graph as a result\n            of this edge being marked as Accepted.\n        \"\"\"\n\n        src_base = PreparedVariableName(src).base_var()\n        dst_base = PreparedVariableName(dst).base_var()\n\n        l = []\n        for var in self._variables:\n            var_base = PreparedVariableName(var).base_var()\n            if (var_base == src_base and var != src) or (\n                var_base == dst_base and var != dst\n            ):\n                self._m[self.idx(var), :] = -1\n                self._m[:, self.idx(var)] = -1\n                l.append(var)\n\n        return l \n\n    @staticmethod\n    def enumerate_with_max_edges(n: int, max_edges: int) -&gt; list[np.ndarray]:\n        \"\"\"\n        Enumerate all edge state matrices of dimension `n` with at most `max_edges` accepted edges.\n\n        Parameters:\n            n: The dimension of the edge state matrices.\n            max_edges: The maximum number of edges to allow in the edge state matrices.\n\n        Returns:\n            A list of edge state matrices.\n        \"\"\"\n        valid_matrices = {0: [np.full(shape=(n, n), fill_value=-1)]}\n\n        # Enumerate all valid matrices with k edges\n        for k in range(1, max_edges + 1):\n            valid_matrices[k] = []\n\n            # For each valid matrix with k-1 edges...\n            for m in valid_matrices[k - 1]:\n                # ...add a new edge in every possible way\n                for i in range(n):\n                    for j in range(i + 1, n):\n                        if m[i, j] &lt; 0 and m[j, i] &lt; 0:\n                            forward = m.copy()\n                            forward[i, j] = 1\n                            valid_matrices[k].append(forward)\n                            backward = m.copy()\n                            backward[j, i] = 1\n                            valid_matrices[k].append(backward)\n\n        # Flatten the collection of matrices into a single list\n        returned_matrices = []\n        for k in range(1, max_edges + 1):\n            returned_matrices.extend(valid_matrices[k])\n\n        return returned_matrices\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.m","title":"<code>m: np.ndarray</code>  <code>property</code>","text":"<p>Returns the edge state matrix.</p>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>Returns the number of nodes.</p>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.__init__","title":"<code>__init__(variables)</code>","text":"<p>Initialize the edge state matrix to the right dimensions and mark self-edges as rejected and all other edges as undecided.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>The variables to initialize the edge state matrix based on. This list must include variable NAMES, not tags.</p> required Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def __init__(self, variables: list[str]) -&gt; None:\n    \"\"\"\n    Initialize the edge state matrix to the right dimensions and mark self-edges\n    as rejected and all other edges as undecided.\n\n    Parameters:\n        variables: The variables to initialize the edge state matrix based on. This\n            list must include variable NAMES, not tags.\n    \"\"\"\n\n    n = len(variables)\n    self._variables = variables\n    self._m = np.zeros((n, n))\n    for i in range(n):\n        self._m[i, i] = -1\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.clear_and_set_from_graph","title":"<code>clear_and_set_from_graph(graph)</code>","text":"<p>Clear the edge state matrix and then set it based on the provided graph. In particular, mark all edges in the graph as accepted and all others as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to use to set the edge states.</p> required Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def clear_and_set_from_graph(self, graph: nx.DiGraph) -&gt; None:\n    \"\"\"\n    Clear the edge state matrix and then set it based on the provided graph.\n    In particular, mark all edges in the graph as accepted and all others as rejected.\n\n    Parameters:\n        graph: The graph to use to set the edge states.\n    \"\"\"\n\n    self._m = np.zeros((self.n, self.n))\n    for edge in graph.edges:\n        print(\"Marking edge as accepted: \", edge)\n        self._m[self.idx(edge[0]), self.idx(edge[1])] = 1\n\n    self._m[self._m == 0] = -1\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.clear_and_set_from_matrix","title":"<code>clear_and_set_from_matrix(m)</code>","text":"<p>Clear the edge state matrix and then set it based on the provided matrix.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray</code> <p>The matrix to use to set the edge states.</p> required Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def clear_and_set_from_matrix(self, m: np.ndarray) -&gt; None:\n    \"\"\"\n    Clear the edge state matrix and then set it based on the provided matrix.\n\n    Parameters:\n        m: The matrix to use to set the edge states.\n    \"\"\"\n\n    self._m = m\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.idx","title":"<code>idx(var)</code>","text":"<p>Retrieve the index of a variable in the edge state matrix.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the variable in the edge state matrix.</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def idx(self, var: str) -&gt; int:\n    \"\"\"\n    Retrieve the index of a variable in the edge state matrix.\n\n    Parameters:\n        var: The name or tag of the variable.\n\n    Returns:\n        The index of the variable in the edge state matrix.\n    \"\"\"\n    return self._variables.index(var)\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.get_edge_state","title":"<code>get_edge_state(src, dst)</code>","text":"<p>Get the state of a specific edge.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The state of the edge (Accepted, Rejected, or Undecided).</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def get_edge_state(self, src: str, dst: str) -&gt; str:\n    \"\"\"\n    Get the state of a specific edge.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n\n    Returns:\n        The state of the edge (Accepted, Rejected, or Undecided).\n    \"\"\"\n    src_idx = self.idx(src)\n    dst_idx = self.idx(dst)\n    return self.edge_state_to_str(self._m[src_idx][dst_idx])\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.edge_state_to_str","title":"<code>edge_state_to_str(state)</code>","text":"<p>Translate between edge value and its interpretation.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>int</code> <p>The state of the edge represented as an integer.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The state of the edge (Accepted, Rejected, or Undecided).</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def edge_state_to_str(self, state: int) -&gt; str:\n    \"\"\"\n    Translate between edge value and its interpretation.\n\n    Parameters:\n        state: The state of the edge represented as an integer.\n\n    Returns:\n        The state of the edge (Accepted, Rejected, or Undecided).\n    \"\"\"\n    if state == 0:\n        return \"Undecided\"\n    elif state == -1:\n        return \"Rejected\"\n    elif state == 1:\n        return \"Accepted\"\n    else:\n        raise ValueError(f\"Invalid edge state {state}\")\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.mark_edge","title":"<code>mark_edge(src, dst, state)</code>","text":"<p>Mark an edge as being in a specified state.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>state</code> <code>str</code> <p>The state to mark the edge with (Accepted, Rejected, or Undecided).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables that were removed from the partial causal graph as a result</p> <code>list[str]</code> <p>of this edge being marked as Accepted.</p> Throws <p>ValueError: If <code>state</code> is not one of \"Accepted\", \"Rejected\", or \"Undecided\".</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def mark_edge(self, src: str, dst: str, state: str) -&gt; list[str]:\n    \"\"\"\n    Mark an edge as being in a specified state.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        state: The state to mark the edge with (Accepted, Rejected, or Undecided).\n\n    Returns:\n        A list of variables that were removed from the partial causal graph as a result\n        of this edge being marked as Accepted.\n\n    Throws:\n        ValueError: If `state` is not one of \"Accepted\", \"Rejected\", or \"Undecided\".\n    \"\"\"\n\n    src_idx = self.idx(src)\n    dst_idx = self.idx(dst)\n\n    if state == \"Accepted\":\n        self._m[src_idx][dst_idx] = 1\n        self._m[dst_idx][src_idx] = -1\n        return self._reject_other_variants(src, dst)\n    elif state == \"Rejected\":\n        self._m[src_idx][dst_idx] = -1\n        return []\n    elif state == \"Undecided\":\n        self._m[src_idx][dst_idx] = 0\n        return []\n    else:\n        raise ValueError(f\"Invalid edge state {state}\")\n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix._reject_other_variants","title":"<code>_reject_other_variants(src, dst)</code>","text":"<p>Mark any edges that touch a variable different from <code>src</code> and <code>dst</code>, but sharing the same base variable as <code>src</code> or <code>dst</code>, as rejected. Also remove any such variables from the partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables that were removed from the partial causal graph as a result</p> <code>list[str]</code> <p>of this edge being marked as Accepted.</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>def _reject_other_variants(self, src: str, dst: str) -&gt; list[str]:\n    \"\"\"\n    Mark any edges that touch a variable different from `src` and `dst`, but sharing\n    the same base variable as `src` or `dst`, as rejected. Also remove any such variables\n    from the partial causal graph.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n\n    Returns:\n        A list of variables that were removed from the partial causal graph as a result\n        of this edge being marked as Accepted.\n    \"\"\"\n\n    src_base = PreparedVariableName(src).base_var()\n    dst_base = PreparedVariableName(dst).base_var()\n\n    l = []\n    for var in self._variables:\n        var_base = PreparedVariableName(var).base_var()\n        if (var_base == src_base and var != src) or (\n            var_base == dst_base and var != dst\n        ):\n            self._m[self.idx(var), :] = -1\n            self._m[:, self.idx(var)] = -1\n            l.append(var)\n\n    return l \n</code></pre>"},{"location":"reference/src/sawmill/edge_state_matrix/#src.sawmill.edge_state_matrix.EdgeStateMatrix.enumerate_with_max_edges","title":"<code>enumerate_with_max_edges(n, max_edges)</code>  <code>staticmethod</code>","text":"<p>Enumerate all edge state matrices of dimension <code>n</code> with at most <code>max_edges</code> accepted edges.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The dimension of the edge state matrices.</p> required <code>max_edges</code> <code>int</code> <p>The maximum number of edges to allow in the edge state matrices.</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>A list of edge state matrices.</p> Source code in <code>src/sawmill/edge_state_matrix.py</code> <pre><code>@staticmethod\ndef enumerate_with_max_edges(n: int, max_edges: int) -&gt; list[np.ndarray]:\n    \"\"\"\n    Enumerate all edge state matrices of dimension `n` with at most `max_edges` accepted edges.\n\n    Parameters:\n        n: The dimension of the edge state matrices.\n        max_edges: The maximum number of edges to allow in the edge state matrices.\n\n    Returns:\n        A list of edge state matrices.\n    \"\"\"\n    valid_matrices = {0: [np.full(shape=(n, n), fill_value=-1)]}\n\n    # Enumerate all valid matrices with k edges\n    for k in range(1, max_edges + 1):\n        valid_matrices[k] = []\n\n        # For each valid matrix with k-1 edges...\n        for m in valid_matrices[k - 1]:\n            # ...add a new edge in every possible way\n            for i in range(n):\n                for j in range(i + 1, n):\n                    if m[i, j] &lt; 0 and m[j, i] &lt; 0:\n                        forward = m.copy()\n                        forward[i, j] = 1\n                        valid_matrices[k].append(forward)\n                        backward = m.copy()\n                        backward[j, i] = 1\n                        valid_matrices[k].append(backward)\n\n    # Flatten the collection of matrices into a single list\n    returned_matrices = []\n    for k in range(1, max_edges + 1):\n        returned_matrices.extend(valid_matrices[k])\n\n    return returned_matrices\n</code></pre>"},{"location":"reference/src/sawmill/graph_renderer/","title":"GraphRenderer","text":""},{"location":"reference/src/sawmill/graph_renderer/#src.sawmill.graph_renderer.GraphRenderer","title":"<code>GraphRenderer</code>","text":"<p>Render a digraph with appropriate margins and node tags.</p> Source code in <code>src/sawmill/graph_renderer.py</code> <pre><code>class GraphRenderer:\n    \"\"\"\n    Render a digraph with appropriate margins and node tags.\n    \"\"\"\n\n    @staticmethod\n    def draw_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Draw a graph with appropriate margins and node tags.\n\n        Parameters:\n            graph: The graph to be drawn.\n            var_info: A dataframe containing the tags of the variables in the\n                graph.\n\n        Returns:\n            A base64-encoded string representation of the graph.\n        \"\"\"\n        if graph.number_of_nodes() == 0:\n            return \"\"\n\n        pos = nx.spring_layout(graph)\n        nx.draw(\n            graph,\n            pos,\n            edgelist=graph.edges(),\n            with_labels=False,\n            width=2.0,\n            node_color=\"orange\",\n        )\n        node_labels = {\n            n: (\n                n\n                if len(var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]) == 0\n                else var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]\n            )\n            for n in list(graph.nodes)\n        }\n        text = nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n        for _, t in text.items():\n            t.set_rotation(30)\n\n        # Fix margins\n        x_values, y_values = zip(*pos.values())\n        x_max, x_min = max(x_values), min(x_values)\n        y_max, y_min = max(y_values), min(y_values)\n        if x_max != x_min:\n            x_margin = (x_max - x_min) * 0.3\n            plt.xlim(x_min - x_margin, x_max + x_margin)\n        if y_max != y_min:\n            y_margin = (y_max - y_min) * 0.3\n            plt.ylim(y_min - y_margin, y_max + y_margin)\n\n        buffer = BytesIO()\n        plt.savefig(buffer, format=\"png\")\n        plt.clf()\n        img_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        plt.close()\n\n        return img_str\n\n    @staticmethod\n    def display_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Display the graph.\n\n        Parameters:\n            graph: The graph to be displayed.\n            var_info: A dataframe containing the tags of the variables in the\n                graph.\n        \"\"\"\n        display(\n            HTML(\n                '&lt;img src=\"data:image/png;base64,{}\"&gt;'.format(\n                    GraphRenderer.draw_graph(graph, var_info)\n                )\n            )\n        )\n</code></pre>"},{"location":"reference/src/sawmill/graph_renderer/#src.sawmill.graph_renderer.GraphRenderer.draw_graph","title":"<code>draw_graph(graph, var_info)</code>  <code>staticmethod</code>","text":"<p>Draw a graph with appropriate margins and node tags.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be drawn.</p> required <code>var_info</code> <code>DataFrame</code> <p>A dataframe containing the tags of the variables in the graph.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A base64-encoded string representation of the graph.</p> Source code in <code>src/sawmill/graph_renderer.py</code> <pre><code>@staticmethod\ndef draw_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Draw a graph with appropriate margins and node tags.\n\n    Parameters:\n        graph: The graph to be drawn.\n        var_info: A dataframe containing the tags of the variables in the\n            graph.\n\n    Returns:\n        A base64-encoded string representation of the graph.\n    \"\"\"\n    if graph.number_of_nodes() == 0:\n        return \"\"\n\n    pos = nx.spring_layout(graph)\n    nx.draw(\n        graph,\n        pos,\n        edgelist=graph.edges(),\n        with_labels=False,\n        width=2.0,\n        node_color=\"orange\",\n    )\n    node_labels = {\n        n: (\n            n\n            if len(var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]) == 0\n            else var_info.loc[var_info[\"Name\"] == n, \"Tag\"].values[0]\n        )\n        for n in list(graph.nodes)\n    }\n    text = nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n    for _, t in text.items():\n        t.set_rotation(30)\n\n    # Fix margins\n    x_values, y_values = zip(*pos.values())\n    x_max, x_min = max(x_values), min(x_values)\n    y_max, y_min = max(y_values), min(y_values)\n    if x_max != x_min:\n        x_margin = (x_max - x_min) * 0.3\n        plt.xlim(x_min - x_margin, x_max + x_margin)\n    if y_max != y_min:\n        y_margin = (y_max - y_min) * 0.3\n        plt.ylim(y_min - y_margin, y_max + y_margin)\n\n    buffer = BytesIO()\n    plt.savefig(buffer, format=\"png\")\n    plt.clf()\n    img_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n    plt.close()\n\n    return img_str\n</code></pre>"},{"location":"reference/src/sawmill/graph_renderer/#src.sawmill.graph_renderer.GraphRenderer.display_graph","title":"<code>display_graph(graph, var_info)</code>  <code>staticmethod</code>","text":"<p>Display the graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>The graph to be displayed.</p> required <code>var_info</code> <code>DataFrame</code> <p>A dataframe containing the tags of the variables in the graph.</p> required Source code in <code>src/sawmill/graph_renderer.py</code> <pre><code>@staticmethod\ndef display_graph(graph: nx.DiGraph, var_info: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Display the graph.\n\n    Parameters:\n        graph: The graph to be displayed.\n        var_info: A dataframe containing the tags of the variables in the\n            graph.\n    \"\"\"\n    display(\n        HTML(\n            '&lt;img src=\"data:image/png;base64,{}\"&gt;'.format(\n                GraphRenderer.draw_graph(graph, var_info)\n            )\n        )\n    )\n</code></pre>"},{"location":"reference/src/sawmill/pickler/","title":"Pickler","text":""},{"location":"reference/src/sawmill/pickler/#src.sawmill.pickler.Pickler","title":"<code>Pickler</code>","text":"<p>A class for loading and dumping dataframes to and from pkl files.</p> Source code in <code>src/sawmill/pickler.py</code> <pre><code>class Pickler:\n    \"\"\"\n    A class for loading and dumping dataframes to and from pkl files.\n    \"\"\"\n\n    @staticmethod\n    def load(filename: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Loads a dataframe from a pkl file.\n\n        Parameters:\n            filename: The name of the pkl file.\n\n        Returns:\n            The dataframe loaded from the pkl file.\n        \"\"\"\n        df = pd.DataFrame()\n        with tqdm(total=1, desc=f\"Loading existing pkl file {filename}...\") as pbar:\n            with open(filename, \"rb\") as f:\n                df = pickle.load(f)\n                pbar.update(1)\n        return df\n\n    @staticmethod\n    def dump(df: pd.DataFrame, filename: str) -&gt; None:\n        \"\"\"\n        Dumps a dataframe to a pkl file.\n\n        Parameters:\n            df: The dataframe to be dumped.\n            filename: The name of the pkl file.\n        \"\"\"\n\n        if \"/\" in filename:\n            path = filename[: filename.rindex(\"/\")]\n            os.makedirs(path, exist_ok=True)\n\n        with tqdm(total=1, desc=f\"Dumping pkl file to {filename}...\") as pbar:\n            with open(filename, \"wb+\") as f:\n                pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n                pbar.update(1)\n</code></pre>"},{"location":"reference/src/sawmill/pickler/#src.sawmill.pickler.Pickler.load","title":"<code>load(filename)</code>  <code>staticmethod</code>","text":"<p>Loads a dataframe from a pkl file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the pkl file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe loaded from the pkl file.</p> Source code in <code>src/sawmill/pickler.py</code> <pre><code>@staticmethod\ndef load(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads a dataframe from a pkl file.\n\n    Parameters:\n        filename: The name of the pkl file.\n\n    Returns:\n        The dataframe loaded from the pkl file.\n    \"\"\"\n    df = pd.DataFrame()\n    with tqdm(total=1, desc=f\"Loading existing pkl file {filename}...\") as pbar:\n        with open(filename, \"rb\") as f:\n            df = pickle.load(f)\n            pbar.update(1)\n    return df\n</code></pre>"},{"location":"reference/src/sawmill/pickler/#src.sawmill.pickler.Pickler.dump","title":"<code>dump(df, filename)</code>  <code>staticmethod</code>","text":"<p>Dumps a dataframe to a pkl file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be dumped.</p> required <code>filename</code> <code>str</code> <p>The name of the pkl file.</p> required Source code in <code>src/sawmill/pickler.py</code> <pre><code>@staticmethod\ndef dump(df: pd.DataFrame, filename: str) -&gt; None:\n    \"\"\"\n    Dumps a dataframe to a pkl file.\n\n    Parameters:\n        df: The dataframe to be dumped.\n        filename: The name of the pkl file.\n    \"\"\"\n\n    if \"/\" in filename:\n        path = filename[: filename.rindex(\"/\")]\n        os.makedirs(path, exist_ok=True)\n\n    with tqdm(total=1, desc=f\"Dumping pkl file to {filename}...\") as pbar:\n        with open(filename, \"wb+\") as f:\n            pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n            pbar.update(1)\n</code></pre>"},{"location":"reference/src/sawmill/printer/","title":"Printer","text":""},{"location":"reference/src/sawmill/printer/#src.sawmill.printer.Printer","title":"<code>Printer</code>","text":"<p>A class for controlling message printing.</p> Source code in <code>src/sawmill/printer.py</code> <pre><code>class Printer:\n    \"\"\"\n    A class for controlling message printing.\n    \"\"\"\n\n    \"\"\"\n    A flag indicating whether or not to print messages to the console.\n    \"\"\"\n    SAWMILL_VERBOSE = False\n\n    @classmethod\n    def printv(self, msg: Any) -&gt; None:\n        \"\"\"\n        Prints a message to the console if in verbose mode.\n\n        Parameters:\n            msg: The message to be printed.\n        \"\"\"\n        if Printer.SAWMILL_VERBOSE:\n            print(msg)\n\n    @classmethod\n    def set_verbose(self, val: bool) -&gt; None:\n        \"\"\"\n        Sets the verbosity of the printer.\n\n        Parameters:\n            val: The new verbosity value.\n        \"\"\"\n        Printer.SAWMILL_VERBOSE = val\n\n    @staticmethod\n    def set_warnings_to(self, value: str):\n        \"\"\"\n        Set selected warnings to `value`.\n\n        Parameters:\n            value: The value to set the warnings to.\n        \"\"\"\n        warnings.filterwarnings(\n            value, category=RuntimeWarning, message=\"mean of empty slice\"\n        )\n        warnings.filterwarnings(\n            value,\n            category=RuntimeWarning,\n            message=\"invalid value encountered in scalar divide\",\n        )\n</code></pre>"},{"location":"reference/src/sawmill/printer/#src.sawmill.printer.Printer.printv","title":"<code>printv(msg)</code>  <code>classmethod</code>","text":"<p>Prints a message to the console if in verbose mode.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>The message to be printed.</p> required Source code in <code>src/sawmill/printer.py</code> <pre><code>@classmethod\ndef printv(self, msg: Any) -&gt; None:\n    \"\"\"\n    Prints a message to the console if in verbose mode.\n\n    Parameters:\n        msg: The message to be printed.\n    \"\"\"\n    if Printer.SAWMILL_VERBOSE:\n        print(msg)\n</code></pre>"},{"location":"reference/src/sawmill/printer/#src.sawmill.printer.Printer.set_verbose","title":"<code>set_verbose(val)</code>  <code>classmethod</code>","text":"<p>Sets the verbosity of the printer.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>bool</code> <p>The new verbosity value.</p> required Source code in <code>src/sawmill/printer.py</code> <pre><code>@classmethod\ndef set_verbose(self, val: bool) -&gt; None:\n    \"\"\"\n    Sets the verbosity of the printer.\n\n    Parameters:\n        val: The new verbosity value.\n    \"\"\"\n    Printer.SAWMILL_VERBOSE = val\n</code></pre>"},{"location":"reference/src/sawmill/printer/#src.sawmill.printer.Printer.set_warnings_to","title":"<code>set_warnings_to(value)</code>  <code>staticmethod</code>","text":"<p>Set selected warnings to <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to set the warnings to.</p> required Source code in <code>src/sawmill/printer.py</code> <pre><code>@staticmethod\ndef set_warnings_to(self, value: str):\n    \"\"\"\n    Set selected warnings to `value`.\n\n    Parameters:\n        value: The value to set the warnings to.\n    \"\"\"\n    warnings.filterwarnings(\n        value, category=RuntimeWarning, message=\"mean of empty slice\"\n    )\n    warnings.filterwarnings(\n        value,\n        category=RuntimeWarning,\n        message=\"invalid value encountered in scalar divide\",\n    )\n</code></pre>"},{"location":"reference/src/sawmill/regression/","title":"Regression","text":""},{"location":"reference/src/sawmill/regression/#src.sawmill.regression.Regression","title":"<code>Regression</code>","text":"<p>A collection of regression-related functions.</p> Source code in <code>src/sawmill/regression.py</code> <pre><code>class Regression:\n    \"\"\"\n    A collection of regression-related functions.\n    \"\"\"\n\n    @staticmethod\n    def ols(X_name: str, X_data: pd.Series, Y_data: pd.Series) -&gt; dict:\n        \"\"\"\n        Calculate the slope and p-value of a linear regression of `X` on `Y`.\n\n        Parameters:\n            X_name: The name of the predictor variable.\n            X_data: The data for the predictor variable.\n            Y_data: The data for the target variable.\n\n        Returns:\n            A dictionary containing the slope and p-value of the regression.\n        \"\"\"\n        X_data = sm.add_constant(X_data)\n        model = sm.OLS(Y_data, X_data).fit()\n        slope = model.params.iloc[1]\n        p_value = model.pvalues.iloc[1]\n        return {\n            \"Candidate\": X_name,\n            \"Slope\": slope,\n            \"P-value\": p_value,\n        }\n</code></pre>"},{"location":"reference/src/sawmill/regression/#src.sawmill.regression.Regression.ols","title":"<code>ols(X_name, X_data, Y_data)</code>  <code>staticmethod</code>","text":"<p>Calculate the slope and p-value of a linear regression of <code>X</code> on <code>Y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_name</code> <code>str</code> <p>The name of the predictor variable.</p> required <code>X_data</code> <code>Series</code> <p>The data for the predictor variable.</p> required <code>Y_data</code> <code>Series</code> <p>The data for the target variable.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the slope and p-value of the regression.</p> Source code in <code>src/sawmill/regression.py</code> <pre><code>@staticmethod\ndef ols(X_name: str, X_data: pd.Series, Y_data: pd.Series) -&gt; dict:\n    \"\"\"\n    Calculate the slope and p-value of a linear regression of `X` on `Y`.\n\n    Parameters:\n        X_name: The name of the predictor variable.\n        X_data: The data for the predictor variable.\n        Y_data: The data for the target variable.\n\n    Returns:\n        A dictionary containing the slope and p-value of the regression.\n    \"\"\"\n    X_data = sm.add_constant(X_data)\n    model = sm.OLS(Y_data, X_data).fit()\n    slope = model.params.iloc[1]\n    p_value = model.pvalues.iloc[1]\n    return {\n        \"Candidate\": X_name,\n        \"Slope\": slope,\n        \"P-value\": p_value,\n    }\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/","title":"Sawmill","text":""},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill","title":"<code>Sawmill</code>","text":"<p>Sawmill provides a high-level interface for causal analysis of event logs.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>class Sawmill:\n    \"\"\"\n    Sawmill provides a high-level interface for causal analysis of event logs.\n    \"\"\"\n\n    def _set_vars_to_defaults(self) -&gt; None:\n        \"\"\"\n        Set some of the variables to their default values.\n        \"\"\"\n        # The parsed log as a dataframe, and metadata about the parsed variables.\n        self._parsed_log: pd.DataFrame = pd.DataFrame()\n        self._parsed_variables: pd.DataFrame = pd.DataFrame()\n        self._parsed_templates: pd.DataFrame = pd.DataFrame()\n\n        # The variable used to define causal units and the number of causal units.\n        self._causal_unit_var: Optional[str] = None\n        self._num_causal_units: Optional[int] = None\n\n        # The prepared log as a dataframe, and metadata about the prepared variables.\n        self._prepared_log: pd.DataFrame = pd.DataFrame()\n        self._prepared_variables: pd.DataFrame = pd.DataFrame()\n\n        # The available aggregation and imputation functions.\n        agg_module = importlib.import_module(\"src.sawmill.aggimp.agg_funcs\")\n        self._agg_funcs: dict[str, Callable] = {\n            n: f for n, f in inspect.getmembers(agg_module, inspect.isfunction)\n        }\n\n        imp_module = importlib.import_module(\"src.sawmill.aggimp.imp_funcs\")\n        self._imp_funcs: dict[str, Callable] = {\n            n: f for n, f in inspect.getmembers(imp_module, inspect.isfunction)\n        }\n\n        # The graph of causal relationships.\n        self._graph: nx.DiGraph = nx.DiGraph()\n\n        # The exploration progress matrix, indicating which edges have been explored.\n        self._edge_states: Optional[EdgeStateMatrix] = None\n\n        # The most recent next exploration suggestion.\n        self._next_exploration: Optional[str] = None\n\n    @property\n    def parsed_log(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed log as a dataframe.\n        \"\"\"\n        return self._parsed_log\n\n    @property\n    def parsed_variables(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed variables as a dataframe.\n        \"\"\"\n        return self._parsed_variables\n\n    @property\n    def parsed_templates(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the parsed templates as a dataframe.\n        \"\"\"\n        return self._parsed_templates\n\n    @property\n    def prepared_log(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the prepared log as a dataframe.\n        \"\"\"\n        return self._prepared_log\n\n    @property\n    def prepared_variables(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the prepared variables as a dataframe.\n        \"\"\"\n        return self._prepared_variables\n\n    @property\n    def prepared_variable_names(self) -&gt; list[str]:\n        \"\"\"\n        Get the names of the prepared variables.\n        \"\"\"\n        return self._prepared_variables[\"Name\"].values.tolist()\n\n    def prepared_variable_names_with_base_x_and_no_pre_post_agg(\n        self, x: Union[str, PreparedVariableName]\n    ) -&gt; list[str]:\n        \"\"\"\n        Get all prepared variables with the given base variable and no pre-\n        or post-aggregate values.\n\n        Parameters:\n            x: The base variable to check.\n\n        Returns:\n            A list of variables with the given base variable and no pre-\n            or post-aggregate values.\n        \"\"\"\n        return [\n            var\n            for var in self.prepared_variable_names\n            if PreparedVariableName(var).has_base_var(x)\n            and PreparedVariableName(var).no_pre_post_aggs()\n        ]\n\n    @property\n    def num_prepared_variables(self) -&gt; int:\n        \"\"\"\n        Get the number of prepared variables.\n        \"\"\"\n        return len(self.prepared_variables)\n\n    def __init__(\n        self, filename: str, workdir: str = None, skip_writeout: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Initialize a Sawmill instance, giving it the full path to the log file that will be analyzed.\n\n        Parameters:\n            filename: The full path to the log file that will be analyzed.\n            workdir: The directory where the parsed and prepared dataframes will be stored.\n            skip_writeout: Whether to skip writing out the parsed and prepared dataframes.\n        \"\"\"\n\n        self._set_vars_to_defaults()\n        self._filename = filename\n        print(f\"Initialized Sawmill with log file {filename}\")\n\n        # Set and create working directory\n        if workdir != None:\n            self._workdir = workdir\n        else:\n            self._workdir = os.path.join(\n                \"../../datasets\",\n                \"xyzw_logs\",\n                self._filename.split(\"/\")[-1].split(\".\")[0],\n            )\n        self._skip_writeout = skip_writeout\n\n        if not os.path.exists(self._workdir):\n            os.makedirs(self._workdir, exist_ok=True)\n        print(f\"Work directory set to {self._workdir}\")\n\n    def _get_filename(self, var_name: str) -&gt; str:\n        \"\"\"\n        Create the file name string for dumping/loading pkl files.\n\n        Parameters:\n            var_name: The name of the variable to be dumped/loaded.\n\n        Returns:\n            The file name string.\n        \"\"\"\n        return os.path.join(\n            self._workdir,\n            os.path.basename(self._filename)\n            + f\"{var_name}_{self._causal_unit_var}_{self._num_causal_units}.pkl\",\n        )\n\n    def _find_type(self, row: pd.Series) -&gt; str:\n        \"\"\"\n        Identify the type of a parsed variable.\n\n        Parameters:\n            row: A row of the parsed variables dataframe.\n\n        Returns:\n            The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".\n        \"\"\"\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"error\", category=UserWarning)\n\n            try:\n                y = pd.to_numeric(row[\"Examples\"], errors=\"raise\")\n                return \"num\"\n            except Exception as e:\n                try:\n                    y = pd.to_timedelta(row[\"Examples\"], errors=\"raise\")\n                    return \"time\"\n                except Exception as e:\n                    try:\n                        y = pd.to_datetime(row[\"Examples\"], errors=\"raise\")\n                        return \"date\"\n                    except Exception as e:\n                        return \"str\"\n\n    def _find_uninteresting(self, row: pd.Series) -&gt; bool:\n        \"\"\"\n        Identify whether a parsed variable is likely to be uninteresting.\n\n        Parameters:\n            row: A row of the parsed variables dataframe.\n\n        Returns:\n            True if the variable is likely to be uninteresting, False otherwise.\n        \"\"\"\n        return (\n            row[\"Type\"] != \"num\"\n            and (self._parsed_log[row[\"Name\"]].nunique() &gt;= 0.15 * row[\"Occurrences\"])\n        ) or (self._parsed_log[row[\"Name\"]].nunique() == 1)\n\n    \"\"\"\n    A default dictionary of regular expressions to be used for parsing the log.\n    \"\"\"\n    DEFAULT_REGEX_DICT = {\n        \"Timestamp\": r\"\\d{4}\\-\\d{2}\\-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}Z\",\n    }\n\n    def parse(\n        self,\n        regex_dict: dict[str, str] = DEFAULT_REGEX_DICT,\n        sim_thresh: float = 0.65,\n        depth: int = 5,\n        force: bool = False,\n        message_prefix: str = r\".*\",\n        enable_gpt_tagging: bool = False,\n        gpt_model: str = \"gpt-3.5-turbo\",\n    ) -&gt; str:\n        \"\"\"\n        Parse the log file into a dataframe.\n\n        Parameters:\n            regex_dict: (for Drain) A dictionary of regular expressions to be used for parsing.\n            sim_thresh: (for Drain) The similarity threshold to be used for parsing.\n            depth: (for Drain) The parse tree depth to be used for parsing.\n            force: Whether to force re-parsing of the log file.\n            message_prefix: A prefix used to identify the beginning of each log message.\n                Can be used to collapse multiple lines into a single message. Each line that doesn't start with this\n                prefix will be concatenated to the previous log message.\n            enable_gpt_tagging: A boolean indicating whether GPT tagging should be enabled.\n            gpt_model: The GPT model to use.\n\n        Returns:\n            The time elapsed for parsing, as a string.\n        \"\"\"\n        start_time = datetime.now()\n        parser = Drain(\n            indir=os.path.dirname(self._filename),\n            depth=depth,\n            st=sim_thresh,\n            rex=regex_dict,\n            skip_writeout=self._skip_writeout,\n            message_prefix=message_prefix,\n        )\n\n        # Check if the parsed files already exist.\n        files_exist = not force\n        parsed_df_names = [\n            nameof(self._parsed_log),\n            nameof(self._parsed_templates),\n            nameof(self._parsed_variables),\n        ]\n        for var_name in parsed_df_names:\n            if not os.path.isfile(self._get_filename(var_name)):\n                files_exist = False\n                break\n\n        if files_exist:\n            self._parsed_log = Pickler.load(self._get_filename(parsed_df_names[0]))\n            self._parsed_templates = Pickler.load(\n                self._get_filename(parsed_df_names[1])\n            )\n            self._parsed_variables = Pickler.load(\n                self._get_filename(parsed_df_names[2])\n            )\n        else:\n            (\n                self._parsed_log,\n                self._parsed_templates,\n                self._parsed_variables,\n            ) = parser.parse(self._filename.split(\"/\")[-1])\n            tqdm.pandas(desc=\"Determining variable types...\")\n            self._parsed_variables[\"Type\"] = self._parsed_variables.progress_apply(\n                self._find_type, axis=1\n            )\n\n            # Cast and convert date columns\n            is_date = self._parsed_variables[\"Type\"] == \"date\"\n            date_cols = self._parsed_variables.loc[is_date, \"Name\"]\n            tqdm.pandas(desc=\"Casting date variables...\")\n            self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_apply(\n                pd.to_datetime, errors=\"coerce\"\n            )\n            tqdm.pandas(desc=\"Casting date variables round 2...\")\n            self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_applymap(\n                lambda x: x.timestamp() if not pd.isnull(x) else None\n            )\n            self._parsed_variables.loc[is_date, \"Type\"] = \"num\"\n\n            # Cast and convert time columns\n            is_time = self._parsed_variables[\"Type\"] == \"time\"\n            time_cols = self._parsed_variables.loc[is_time, \"Name\"]\n            tqdm.pandas(desc=\"Casting time variables...\")\n            self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_apply(\n                pd.to_timedelta, errors=\"coerce\"\n            )\n            tqdm.pandas(desc=\"Casting time variables round 2...\")\n            self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_applymap(\n                lambda x: x.total_seconds() if not pd.isnull(x) else None\n            )\n            self._parsed_variables.loc[is_time, \"Type\"] = \"num\"\n\n            # Cast numeric columns\n            is_num = self._parsed_variables[\"Type\"] == \"num\"\n            numeric_cols = self._parsed_variables.loc[is_num, \"Name\"]\n            tqdm.pandas(desc=\"Casting numerical variables...\")\n            self._parsed_log[numeric_cols] = self._parsed_log[\n                numeric_cols\n            ].progress_apply(pd.to_numeric, errors=\"coerce\")\n\n            # Tag variables.\n            tqdm.pandas(desc=\"Tagging variables...\")\n            tag, tag_from_gpt = zip(\n                *self._parsed_variables.progress_apply(\n                    lambda x: TagUtils.best_effort_tag(\n                        self.parsed_templates, x, enable_gpt_tagging, gpt_model\n                    ),\n                    axis=1,\n                )\n            )\n            self._parsed_variables[\"Tag\"] = tag\n            self._parsed_variables[\"TagFromGPT\"] = tag_from_gpt\n            TagUtils.deduplicate_tags(self._parsed_variables)\n\n            # Detect identifiers.\n            tqdm.pandas(desc=\"Detecting identifiers...\")\n            self._parsed_variables[\n                \"IsUninteresting\"\n            ] = self._parsed_variables.progress_apply(self._find_uninteresting, axis=1)\n\n            # Reorder columns.\n            self._parsed_variables = self._parsed_variables[\n                [\n                    \"Name\",\n                    \"Tag\",\n                    \"Type\",\n                    \"IsUninteresting\",\n                    \"Occurrences\",\n                    \"Preceding 3 tokens\",\n                    \"Examples\",\n                    \"From regex\",\n                ]\n            ]\n\n        # Write out files if appropriate.\n        if not self._skip_writeout and not files_exist:\n            Pickler.dump(self._parsed_log, self._get_filename(parsed_df_names[0]))\n            Pickler.dump(self._parsed_templates, self._get_filename(parsed_df_names[1]))\n            Pickler.dump(self._parsed_variables, self._get_filename(parsed_df_names[2]))\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(f\"Parsing complete in {elapsed} seconds!\")\n        return elapsed\n\n    def include_in_template(\n        self,\n        name: str,\n        enable_gpt_tagging: bool = False,\n        gpt_model: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Treat a certain parsed variable as part of its template and regenerate parsed dataframes.\n\n        Parameters:\n            name: The name of the variable to be included in its template.\n            enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n            gpt_model: The GPT model to use.\n        \"\"\"\n\n        old_template_id = ParsedVariableName(name).template_id()\n        idx = ParsedVariableName(name).index()\n        value_counts = self._parsed_log[name].value_counts().to_dict()\n\n        ### Modify _parsed_templates\n        old_template_row = (\n            self._parsed_templates.loc[\n                self._parsed_templates[\"TemplateId\"] == old_template_id\n            ]\n            .iloc[0]\n            .copy()\n        )\n        toks = old_template_row[\"TemplateText\"].split(\" \")\n        new_template_ids = {}\n        new_variable_indices = old_template_row[\"VariableIndices\"]\n        new_variable_indices.remove(idx)\n\n        for value, occurences in value_counts.items():\n            new_template_row = old_template_row.copy()\n            toks[idx] = value\n\n            new_template_row[\"TemplateText\"] = \" \".join(toks)\n            new_template_row[\"TemplateId\"] = hashlib.md5(\n                new_template_row[\"TemplateText\"].encode(\"utf-8\")\n            ).hexdigest()[0:8]\n            new_template_row[\"Occurrences\"] = occurences\n            new_template_row[\"VariableIndices\"] = new_variable_indices\n            new_template_row[\"RegexIndices\"] = old_template_row[\"RegexIndices\"]\n\n            self._parsed_templates.loc[len(self._parsed_templates)] = new_template_row\n            new_template_ids[value] = new_template_row[\"TemplateId\"]\n\n        self._parsed_templates = self._parsed_templates[\n            self._parsed_templates[\"TemplateId\"] != old_template_id\n        ].reset_index(drop=True)\n\n        ### Modify _parsed_log\n\n        # Update the template ids of all rows that belonged to the old template\n        self._parsed_log[\"TemplateId\"] = self._parsed_log.apply(\n            lambda x: new_template_ids[x[name]]\n            if (x[\"TemplateId\"] == old_template_id)\n            else x[\"TemplateId\"],\n            axis=1,\n        )\n\n        # Create new variables for each new template id and assign the value of the old variables to them\n        new_variables = []\n        for new_template_id in new_template_ids.values():\n            for other_idx in new_variable_indices:\n                new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n                new_variables.append(new_var_name)\n                self._parsed_log[new_var_name] = self._parsed_log.apply(\n                    lambda x: x[f\"{old_template_id}_{other_idx}\"]\n                    if (x[\"TemplateId\"] == new_template_id)\n                    else None,\n                    axis=1,\n                )\n\n        # Drop variable columns associated with old template id\n        variables_to_drop = [\n            v for v in self._parsed_log.columns if v.startswith(old_template_id)\n        ]\n        self._parsed_log.drop(columns=variables_to_drop, inplace=True)\n\n        ### Modify _parsed_variables\n\n        # Add variable rows for each new variable\n        for value, occurrences in value_counts.items():\n            for other_idx in new_variable_indices:\n                new_template_id = new_template_ids[value]\n                new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n\n                x = {}\n                x[\"Name\"] = new_var_name\n                x[\"Occurrences\"] = occurrences\n                x[\"Preceding 3 tokens\"] = (\n                    self._parsed_templates[\n                        self._parsed_templates[\"TemplateId\"] == new_template_id\n                    ][\"TemplateText\"]\n                    .values[0]\n                    .split()[max(0, other_idx - 3) : other_idx]\n                )\n                x[\"Examples\"] = (\n                    self._parsed_log[new_var_name]\n                    .loc[self._parsed_log[new_var_name].notna()]\n                    .unique()[:5]\n                    .tolist()\n                )\n                x[\"From regex\"] = False\n                x[\"Tag\"], x[\"TagFromGPT\"] = TagUtils.best_effort_tag(\n                    self.parsed_templates, pd.Series(x), enable_gpt_tagging, gpt_model\n                )\n                x[\"Type\"] = self._find_type(pd.Series(x))\n                x[\"IsUninteresting\"] = self._find_uninteresting(pd.Series(x))\n\n                self._parsed_variables.loc[len(self._parsed_variables)] = x\n\n        # Drop variable rows associated with old template id\n        self._parsed_variables = self._parsed_variables[\n            ~self._parsed_variables[\"Name\"].isin(variables_to_drop)\n        ].reset_index(drop=True)\n\n        # Deduplicate tags again\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n    def tag_parsed_variable(self, name: str, tag: str) -&gt; None:\n        \"\"\"\n        Tag a parsed variable.\n\n        Parameters:\n            name: The name of the variable to be tagged.\n            tag: The tag to be assigned to the variable.\n        \"\"\"\n        TagUtils.set_tag(self._parsed_variables, name, tag, \"parsed\")\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n    def get_tag_of_parsed(self, name: str) -&gt; str:\n        \"\"\"\n        Get the tag of a parsed variable.\n\n        Parameters:\n            name: The name of the variable.\n\n        Returns:\n            The tag of the variable.\n        \"\"\"\n        return TagUtils.get_tag(self._parsed_variables, name, \"parsed\")\n\n    def tag_prepared_variable(self, name: str, tag: str) -&gt; None:\n        \"\"\"\n        Tag a prepared variable.\n\n        Parameters:\n            name: The name of the variable to be tagged.\n            tag: The tag to be assigned to the variable.\n        \"\"\"\n        TagUtils.set_tag(self._prepared_variables, name, tag, \"prepared\")\n        TagUtils.deduplicate_tags(self._prepared_variables)\n\n    def get_tag_of_prepared(self, name: str) -&gt; str:\n        \"\"\"\n        Get the tag of a prepared variable.\n\n        Parameters:\n            name: The name of the variable.\n\n        Returns:\n            The tag of the variable.\n        \"\"\"\n        return TagUtils.get_tag(self._prepared_variables, name, \"prepared\")\n\n    def get_causal_unit_info(self) -&gt; Tuple[str, int]:\n        \"\"\"\n        Get the variable used to define causal units and the number of\n        causal units.\n\n        Returns:\n            The name of the variable used to define causal units\n            and the number of causal units.\n        \"\"\"\n        return self._causal_unit_var, self._num_causal_units\n\n    def suggest_causal_unit_defs(\n        self,\n        min_causal_units: int = 4,\n        num_suggestions: int = 10,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Suggest at most `num_suggestions` causal unit definitions based on IUS maximization,\n        while returning at least `min_causal_units` causal units.\n\n        Parameters:\n            min_causal_units: The minimum number of causal units that a suggested\n                definition should create.\n            num_suggestions: The maximum number of causal unit definitions to suggest.\n\n        Returns:\n            A DataFrame with one row for each suggested causal unit definition, or `None`\n                if no suggestions were made.\n        \"\"\"\n\n        return CausalUnitSuggester.suggest_causal_unit_defs(\n            self._parsed_log[self._parsed_variables[\"Name\"].values],\n            self._parsed_variables,\n            min_causal_units=min_causal_units,\n            num_suggestions=num_suggestions,\n        )\n\n    def set_causal_unit(\n        self,\n        var: str,\n        num_units: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set the variable used to define causal units and optionally the number of\n        causal units. The latter will be ignored if the variable is categorical, but it\n        must be specified if the variable is numerical.\n\n        Parameters:\n            var: The name or tag of the variable to be used as the causal unit.\n            num_units: The number of causal units to be created.\n\n        Raises:\n            ValueError: If the variable is numerical and `num_units` is not specified.\n        \"\"\"\n        var_name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n        var_type = self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == var_name, \"Type\"\n        ].values[0]\n\n        if var_type == \"num\" and num_units is None:\n            raise ValueError(\n                \"The number of causal units must be specified if the causal unit is numerical.\"\n            )\n\n        self._causal_unit_var = var_name\n        self._num_causal_units = num_units\n\n        print(\n            f\"Causal unit set to {var_name} (tag: {self.get_tag_of_parsed(var_name)}) \"\n            + (\n                \"\"\n                if not self._num_causal_units\n                else f\" with {self._num_causal_units} causal units.\"\n            )\n        )\n\n    def prepare(\n        self,\n        custom_agg: dict[str, list[str]] = {},\n        custom_imp: dict[str, list[str]] = {},\n        count_occurences: bool = False,\n        ignore_uninteresting: bool = True,\n        force: bool = False,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n        drop_bad_aggs: bool = True,\n        reject_prunable_edges: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.\n\n        Parameters:\n            custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n            custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n            count_occurences: Whether to include extra variables counting the occurence of each template.\n            ignore_uninteresting: Whether to ignore uninteresting variables.\n            force: Whether to force re-preparation of the log.\n            lasso_alpha: The alpha parameter to be used for LASSO regression.\n            lasso_max_iter: The maximum number of iterations to be used for LASSO regression.\n            drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n                variables based on the same base variable but using a different aggregation function.\n            reject_prunable_edges: Whether to reject edges that are prunable based on LASSO applciation.\n\n        Returns:\n            The time elapsed for preparation, as a string.\n        \"\"\"\n\n        start_time = datetime.now()\n        # Ensure causal unit is set. TODO: make IUS maximizer the default\n        if self._causal_unit_var is None:\n            print(\"Causal unit not defined. Aborting.\")\n            return None\n\n        # Check if the prepared files already exist.\n        files_exist = not force\n        prepared_df_names = [\n            nameof(self._prepared_log),\n            nameof(self._prepared_variables),\n        ]\n        for var_name in prepared_df_names:\n            if not os.path.isfile(self._get_filename(var_name)):\n                files_exist = False\n                break\n\n        if files_exist:\n            self._prepared_log = Pickler.load(self._get_filename(prepared_df_names[0]))\n            self._prepared_variables = Pickler.load(\n                self._get_filename(prepared_df_names[1])\n            )\n        else:\n            self._prepare_anew(\n                custom_agg,\n                custom_imp,\n                count_occurences=count_occurences,\n                ignore_uninteresting=ignore_uninteresting,\n                drop_bad_aggs=drop_bad_aggs,\n            )\n\n        self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n        if reject_prunable_edges:\n            print(f\"Pruning edges...\")\n            self.reject_all_prunable_edges(\n                lasso_alpha=lasso_alpha, lasso_max_iter=lasso_max_iter\n            )\n\n        end_time = datetime.now()\n        elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n        print(\n            f\"\"\"Preparation complete in {elapsed} seconds! \"\"\"\n            f\"\"\"{np.count_nonzero(self._edge_states.m == -1)} of the {self.num_prepared_variables ** 2} possible edges were auto-rejected.\"\"\"\n        )\n\n        return elapsed\n\n    def _prepare_anew(\n        self,\n        custom_agg: dict[str, list[str]] = {},\n        custom_imp: dict[str, list[str]] = {},\n        count_occurences: bool = False,\n        ignore_uninteresting: bool = True,\n        drop_bad_aggs: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Prepare the log anew.\n\n        Parameters:\n            custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n            custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n            count_occurences: Whether to include extra variables counting the occurence of each template.\n            ignore_uninteresting: Whether to ignore uninteresting variables.\n            drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n                variables based on the same base variable but using a different aggregation function.\n        \"\"\"\n\n        print(f\"Determining the causal unit assignment...\")\n        causal_unit_assignment = CausalUnitSuggester._discretize(\n            self._parsed_log[self._causal_unit_var],\n            self._parsed_variables[\n                self._parsed_variables[\"Name\"] == self._causal_unit_var\n            ][\"Type\"].values[0],\n            self._num_causal_units,\n        )\n\n        # Convert keys in custom_agg and custom_imp to the names of the variables, if they are tags.\n        custom_agg = {\n            TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n            for k, v in custom_agg.items()\n        }\n        custom_imp = {\n            TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n            for k, v in custom_imp.items()\n        }\n\n        # Start with the parsed log, optionally with extra variables counting the occurence of each template.\n        if count_occurences:\n            print(f\"Adding template occurrence count variables...\")\n            self._prepared_log = pd.concat(\n                [\n                    self._parsed_log,\n                    pd.get_dummies(\n                        self._parsed_log[\"TemplateId\"],\n                        prefix=\"TemplateId\",\n                        prefix_sep=\"=\",\n                    ),\n                ],\n                axis=1,\n            )\n        else:\n            self._prepared_log = self._parsed_log.copy(deep=True)\n\n        # No longer need the column storing the actual template IDs\n        self._prepared_log.drop(columns=\"TemplateId\", inplace=True)\n\n        # Build dictionary of aggregation functions\n        agg_dict: dict[str, str] = {\n            variable.Name: (\n                custom_agg[variable.Name]\n                if variable.Name in custom_agg\n                else AggregateSelector.DEFAULT_AGGREGATES[variable.Type]\n            )\n            for variable in self._parsed_variables.itertuples()\n        }\n\n        # Add aggregations for template counts\n        for col in self._prepared_log.columns:\n            if PreparedVariableName(col).base_var() == \"TemplateId\":\n                agg_dict[col] = [\"sum\"]\n\n        # Drop uninteresting columns if requested, except if they are the causal unit.\n        ui_cols = self._parsed_variables.loc[\n            self._parsed_variables[\"IsUninteresting\"], \"Name\"\n        ].values\n        ui_cols = [x for x in ui_cols if x != self._causal_unit_var]\n        if ignore_uninteresting:\n            self._prepared_log.drop(\n                columns=ui_cols,\n                inplace=True,\n            )\n            for col in ui_cols:\n                agg_dict.pop(col, None)\n            print(\n                f\"Dropped {len(ui_cols)} uninteresting columns, out of an original total of {len(self.parsed_variables)}.\"\n            )\n\n        # Ensure the causal unit variable only has one aggregation function\n        agg_dict[self._causal_unit_var] = agg_dict[self._causal_unit_var][:1]\n\n        # Perform the aggregation\n        print(\"Calculating aggregates for each causal unit...\")\n        agg_func_dict: dict[str, list[Callable]] = {\n            name: [self._agg_funcs[f] for f in funcs]\n            for name, funcs in agg_dict.items()\n        }\n        self._prepared_log = self._prepared_log.groupby(\n            causal_unit_assignment\n        ).aggregate(agg_func_dict)\n        self._prepared_log.columns = [\n            \"+\".join(col) for col in self._prepared_log.columns.values\n        ]\n        self._parsed_variables[\"Aggregates\"] = self._parsed_variables[\"Name\"].map(\n            lambda x: agg_dict.get(x, [])\n        )\n        self._prepared_log.set_index(\n            f\"{self._causal_unit_var}+{self._parsed_variables[self._parsed_variables['Name'] == self._causal_unit_var]['Aggregates'].values[0][0]}\",\n            inplace=True,\n        )\n        self._prepared_log.sort_index(inplace=True)\n        self._prepared_log.index = self._prepared_log.index.astype(str)\n\n        # Perform the imputation\n        for col in tqdm(self._prepared_log.columns, desc=\"Imputing missing values...\"):\n            if self._prepared_log[col].isnull().values.any():\n                base_var = PreparedVariableName(col).base_var()\n                func_name: str = (\n                    custom_imp[base_var] if base_var in custom_imp else \"no_imp\"\n                )\n                self._prepared_log[col] = (self._imp_funcs[func_name])(\n                    self._prepared_log[col]\n                )\n        self._prepared_log.dropna(inplace=True)\n\n        # Drop variables that do not add information compared to other variables based on the same base variable\n        # but using a different aggregation function.\n        if drop_bad_aggs:\n            print(f\"Dropping aggregates that do not add information...\")\n            cols_to_drop = AggregateSelector.find_uninformative_aggregates(\n                self._prepared_log, self._parsed_variables, self._causal_unit_var\n            )\n            self._prepared_log.drop(columns=cols_to_drop, inplace=True)\n\n        # Identify the categorical variables and one-hot encode them\n        categorical_vars = self._prepared_log.select_dtypes(\n            include=\"object\"\n        ).columns.tolist()\n        for col in tqdm(\n            categorical_vars, desc=\"One-hot encoding categorical variables...\"\n        ):\n            self._prepared_log = pd.concat(\n                [\n                    self._prepared_log,\n                    pd.get_dummies(\n                        self._prepared_log[col], prefix=col, prefix_sep=\"=\", dtype=int\n                    ),\n                ],\n                axis=1,\n            )\n            self._prepared_log.drop(col, axis=1, inplace=True)\n        # Deal with https://github.com/pydot/pydot/issues/258\n        self._prepared_log.columns = [\n            x.replace(\":\", \";\") for x in self._prepared_log.columns\n        ]\n\n        # Generate dataframe of prepared variables for later tagging etc.\n        self._generate_prepared_variables_df()\n\n        # Convert any date columns to Unix timestamps in milliseconds\n        date_cols = self._prepared_variables.loc[\n            self._prepared_variables[\"Type\"] == \"date\", \"Name\"\n        ].values\n        self._prepared_log[date_cols] = self._prepared_log[date_cols].map(\n            lambda x: x.timestamp() * 1000.0\n        )\n\n        # Convert any time columns to milliseconds\n        time_cols = self._prepared_variables.loc[\n            self._prepared_variables[\"Type\"] == \"time\", \"Name\"\n        ].values\n        self._prepared_log[time_cols] = self._prepared_log[time_cols].map(\n            lambda x: x.total_seconds() * 1000.0\n        )\n\n        # Write out prepared log and variables\n        if not self._skip_writeout:\n            Pickler.dump(\n                self._prepared_log, self._get_filename(nameof(self._prepared_log))\n            )\n            Pickler.dump(\n                self._prepared_variables,\n                self._get_filename(nameof(self._prepared_variables)),\n            )\n\n        print(\n            f\"\"\"Successfully prepared the log with causal unit {self._causal_unit_var} \"\"\"\n            f\"\"\"(tag: {self.get_tag_of_parsed(self._causal_unit_var)})\"\"\"\n            + (\n                \"\"\n                if not self._num_causal_units\n                else f\" with {self._num_causal_units} causal units.\"\n            )\n        )\n\n        return\n\n    def _generate_prepared_variables_df(self) -&gt; None:\n        \"\"\"\n        Generate dataframe of prepared variables for later tagging etc.\n        \"\"\"\n\n        self._prepared_variables = pd.DataFrame()\n        self._prepared_variables[\"Name\"] = self._prepared_log.columns\n\n        # Bring in varable name components leveraging PreparedVariableName\n        self._prepared_variables[\"Base\"] = self._prepared_variables[\"Name\"].apply(\n            lambda x: PreparedVariableName(x).base_var()\n        )\n        self._prepared_variables[\"Pre-agg Value\"] = self._prepared_variables[\n            \"Name\"\n        ].apply(lambda x: PreparedVariableName(x).pre_agg_value())\n        self._prepared_variables[\"Agg\"] = self._prepared_variables[\"Name\"].apply(\n            lambda x: PreparedVariableName(x).aggregate()\n        )\n        self._prepared_variables[\"Post-agg Value\"] = self._prepared_variables[\n            \"Name\"\n        ].apply(lambda x: PreparedVariableName(x).post_agg_value())\n\n        # Bring in other info from self._parsed_variables\n        self._prepared_variables[\"Tag\"] = self._prepared_variables.apply(\n            lambda x: (\n                self._parsed_variables.loc[\n                    self._parsed_variables[\"Name\"] == x[\"Base\"],\n                    \"Tag\",\n                ].values[0]\n                if x[\"Base\"] != \"TemplateId\"\n                else \"TemplateId\"\n            )\n            + (f\" {x['Pre-agg Value']}\" if x[\"Pre-agg Value\"] != \"\" else \"\")\n            + (f\" {x['Agg']}\" if x[\"Agg\"] != \"\" else \"\")\n            + (f\" {x['Post-agg Value']}\" if x[\"Post-agg Value\"] != \"\" else \"\"),\n            axis=1,\n        )\n        self._prepared_variables[\"Base Variable Occurences\"] = self._prepared_variables[\n            \"Base\"\n        ].apply(\n            lambda x: self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Occurrences\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n        self._prepared_variables[\"Type\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Type\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n        self._prepared_variables[\"Examples\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"Examples\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n        self._prepared_variables[\"From regex\"] = self._prepared_variables[\"Base\"].apply(\n            lambda x: self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x, \"From regex\"\n            ].values[0]\n            if x != \"TemplateId\"\n            else \"\"\n        )\n\n        # Bring in template text, only for appropriate base variables.\n        self._prepared_variables[\"TemplateText\"] = self._prepared_variables.apply(\n            lambda x: self._parsed_templates.loc[\n                self._parsed_templates[\"TemplateId\"]\n                == PreparedVariableName(x[\"Name\"]).template_id(),\n                \"TemplateText\",\n            ].values[0]\n            if x[\"From regex\"] == False\n            else \"\",\n            axis=1,\n        )\n\n    def inspect(\n        self,\n        var: str,\n        ref_var: Optional[str] = None,\n        row_limit: Optional[int] = 10,\n    ) -&gt; None:\n        \"\"\"\n        Print information about a specific prepared variable.\n\n        Parameters:\n            var: The name or tag of the variable.\n            ref_var: The name or tag of a reference variable.\n            row_limit: The number of rows of the prepared log to print out,\n                to illustrate example values of this variable.\n        \"\"\"\n\n        # Retrieve the name of this variable, if a tag was passed in.\n        name = TagUtils.name_of(self._prepared_variables, var, \"prepared\")\n\n        print(f\"Information about prepared variable {name}:\\n\")\n        base_var = PreparedVariableName(name).base_var()\n        from_regex = False\n\n        if base_var != \"TemplateId\":\n            print(f\"--&gt; Variable Information about {base_var}:\")\n            sample = self._parsed_variables[self._parsed_variables[\"Name\"] == base_var]\n            from_regex = sample[\"From regex\"].values[0]\n            display(sample)\n\n        if not from_regex:\n            template_id = PreparedVariableName(name).template_id()\n            print(f\"--&gt; Template Information about {template_id}:\")\n            display(\n                self._parsed_templates[\n                    self._parsed_templates[\"TemplateId\"] == template_id\n                ]\n            )\n\n        print(\"--&gt; Causal Unit Partial Information:\")\n        if row_limit == None:\n            row_limit = len(self._prepared_log)\n        col_list = [name]\n        col_list.extend([ref_var] if ref_var is not None else [])\n        sample = self._prepared_log[col_list].head(row_limit)\n        col_names = [f\"{name} (candidate)\"]\n        col_names.extend([f\"{ref_var} (outcome)\"] if ref_var is not None else [])\n        sample.columns = col_names\n        display(sample)\n\n    def clear_graph(self, clear_edge_states: bool = True) -&gt; None:\n        \"\"\"\n        Clear the graph and possibly edge states.\n\n        Parameters:\n            clear_edge_states: Whether to also clear the edge states.\n        \"\"\"\n        self._graph = nx.DiGraph()\n        if clear_edge_states:\n            self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n\n    def display_graph(self) -&gt; None:\n        \"\"\"\n        Display the current graph.\n        \"\"\"\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    def accept(\n        self,\n        src: str,\n        dst: str,\n        interactive: bool = True,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark a causal graph edge as accepted.\n\n        This will also reject the edge from `dst` to `src` and remove any other variables with the\n        same base variable as either `src` or `dst` from consideration for the partial causal graph.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            interactive: Whether to display the graph interactively after accepting the edge.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge addition,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n\n        src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        to_drop = self._edge_states.mark_edge(src_name, dst_name, \"Accepted\")\n        for node in to_drop:\n            if node in self._graph.nodes:\n                self._graph.remove_node(node)\n\n        self._graph.add_node(src_name)\n        self._graph.add_node(dst_name)\n        self._graph.add_edge(src_name, dst_name)\n        if (dst_name, src_name) in self._graph.edges:\n            self._graph.remove_edge(dst_name, src_name)\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\",\n        )\n\n    def reject(\n        self,\n        src: str,\n        dst: str,\n        interactive: bool = True,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark a causal graph edge as rejected.\n\n        Parameters:\n            src: The name or tag of the source variable.\n            dst: The name or tag of the destination variable.\n            interactive: Whether to display the graph interactively after rejecting the edge.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejection,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n\n        src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        self._edge_states.mark_edge(src_name, dst_name, \"Rejected\")\n\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\",\n        )\n\n    def reject_undecided_incoming(\n        self, dst: str, interactive: bool = True\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        Mark all undecided incoming edges to a variable as rejected.\n\n        Parameters:\n            dst: The name or tag of the destination variable.\n            interactive: Whether to display the graph interactively after rejecting the edges.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejections,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n        dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n        for v in self.prepared_variable_names:\n            if self._edge_states.get_edge_state(v, dst_name) == \"Undecided\":\n                self._edge_states.mark_edge(v, dst_name, \"Rejected\")\n\n        if interactive:\n            GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n            if not interactive\n            else \"\",\n        )\n\n    def reject_all_prunable_edges(\n        self,\n        lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER,\n    ) -&gt; Tuple[float, Optional[str], Optional[str]]:\n        \"\"\"\n        For every prepared variable, reject all incoming edges that start at a variable\n        that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.\n\n        Returns:\n            A tuple containing:\n                (1) the exploration score after the edge rejections,\n                (2) the max-impact variable to explore next, if any,\n                (3) optionally a string representation of the graph, if `interactive` is False.\n        \"\"\"\n        num_processors = multiprocessing.cpu_count()\n        with multiprocessing.Pool(processes=num_processors) as pool:\n            all_candidates = pool.starmap(\n                Pruner.prune_with_lasso,\n                tqdm(\n                    [\n                        (self._prepared_log, [target], lasso_alpha, lasso_max_iter)\n                        for target in self.prepared_variable_names\n                    ],\n                    total=self.num_prepared_variables,\n                    desc=\"Finding pruned variables...\",\n                ),\n            )\n\n        Printer.printv(all_candidates)\n\n        for candidates, target in zip(all_candidates, self.prepared_variable_names):\n            non_candidates = (\n                set(self._prepared_log.columns) - set(candidates) - set([target])\n            )\n            for nc in non_candidates:\n                self._edge_states.mark_edge(nc, target, \"Rejected\")\n\n        return (\n            self.exploration_score,\n            self.suggest_next_exploration(),\n            GraphRenderer.draw_graph(self._graph, self._prepared_variables),\n        )\n\n    @property\n    def exploration_score(self) -&gt; float:\n        \"\"\"\n        Calculate the exploration score of the current partial causal graph,\n        based on the edge state matrix.\n\n        Returns:\n            The exploration score of the current partial causal graph.\n        \"\"\"\n        # Number of edges incident to a node in the current partial graph\n        M = self._graph.number_of_nodes()\n        N = self.num_prepared_variables\n        incident = M * (2 * N - M - 1)\n        if incident == 0:\n            return 0\n\n        # Number of edges among the incident that have been considered\n        graph_var_indices = [self._edge_states.idx(x) for x in list(self._graph.nodes)]\n        other_indices = list(np.setdiff1d(np.arange(N), graph_var_indices))\n        considered = np.sum(\n            self._edge_states.m[graph_var_indices][:, graph_var_indices] != 0\n        )\n        considered -= M  # subtract self-edges\n        considered += np.sum(\n            self._edge_states.m[graph_var_indices][:, other_indices] != 0\n        )\n        considered += np.sum(\n            self._edge_states.m[other_indices][:, graph_var_indices] != 0\n        )\n\n        Printer.printv(f\"Considered: {considered}\")\n        Printer.printv(f\"Incident: {incident}\")\n\n        return considered / incident\n\n    def explore_candidate_causes(\n        self,\n        target: Optional[str] = None,\n        ignore: Optional[list[str]] = None,\n        lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n        lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Present the user with candidate causal graph neighbors for `target`. If no `target`\n        is specified, the most recent sugestion of `suggest_next_exploration()` is used, if any.\n        If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n        Parameters:\n            target: The name or tag of the target variable.\n            ignore: A list of variables to ignore.\n            lasso_alpha: The alpha parameter to be used for Lasso regression.\n            lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n        Returns:\n            A dataframe containing the candidate causal graph neighbors for `target`.\n        \"\"\"\n\n        column_order = [\n            \"Candidate\",\n            \"Candidate Tag\",\n            \"Target\",\n            \"Slope\",\n            \"P-value\",\n            \"Candidate-&gt;Target Edge Status\",\n            \"Target-&gt;Candidate Edge Status\",\n        ]\n\n        # Handle the case where the user has not specified a target.\n        if target is None and self._next_exploration is None:\n            print(\"No target specified.\")\n            return pd.DataFrame(columns=column_order)\n        elif target is None:\n            target = self._next_exploration\n\n        # If the user provided the target as a tag, retrieve its name\n        target = TagUtils.name_of(self._prepared_variables, target, \"prepared\")\n\n        # Use Lasso to get a pruned list of neighbors\n        candidates = Pruner.prune_with_lasso(\n            self._prepared_log,\n            [target],\n            ignore=ignore,\n            alpha=lasso_alpha,\n            max_iter=lasso_max_iter,\n        )\n        Printer.printv(f\"Candidates: {candidates}\")\n\n        # Stop if there are no candidates\n\n        if len(candidates) == 0:\n            print(\"No candidates found.\")\n            return pd.DataFrame(columns=column_order)\n\n        # Mark the edges rejected by the pruning step\n        non_candidates = (\n            set(self._prepared_log.columns) - set(candidates) - set([target])\n        )\n        for var in non_candidates:\n            self._edge_states.mark_edge(var, target, \"Rejected\")\n\n        # For each candidate, calculate the slope and p-value of a linear regression with target (in parallel)\n        num_processors = multiprocessing.cpu_count()\n        X_columns = [c for c in self._prepared_log.columns if c in candidates]\n        with multiprocessing.Pool(processes=num_processors) as pool:\n            results = pool.starmap(\n                Regression.ols,\n                [\n                    (col, self._prepared_log[col], self.prepared_log[target])\n                    for col in X_columns\n                ],\n            )\n\n        # Return the results in a dataframe\n        result_df = pd.DataFrame(results)\n\n        result_df[\"Target\"] = target\n\n        result_df[\"Candidate-&gt;Target Edge Status\"] = result_df[\"Candidate\"].apply(\n            lambda x: self._edge_states.get_edge_state(x, target)\n        )\n        result_df[\"Target-&gt;Candidate Edge Status\"] = result_df[\"Candidate\"].apply(\n            lambda x: self._edge_states.get_edge_state(target, x)\n        )\n        result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n            lambda x: self.get_tag_of_prepared(x)\n        )\n\n        return (\n            result_df[column_order]\n            .sort_values(by=\"P-value\", ascending=True)\n            .reset_index(drop=True)\n        )\n\n    def suggest_next_exploration(self) -&gt; Optional[str]:\n        \"\"\"\n        Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph\n        that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable\n        with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.\n\n        If all edges are decided, return None.\n\n        Returns:\n            The name of the variable to explore next.\n        \"\"\"\n\n        # Try to find a suggestion from the partial causal graph.\n        node_names = list(self._graph.nodes)\n        graph_var_indices = [self._edge_states.idx(x) for x in node_names]\n        graph_var_incoming_edge_states = self._edge_states.m[:, graph_var_indices]\n        undecided_edges_per_col = (\n            np.sum(graph_var_incoming_edge_states == 0, axis=0)\n            if len(graph_var_incoming_edge_states) &gt; 0\n            else []\n        )\n        max_undecided = (\n            np.max(undecided_edges_per_col) if len(undecided_edges_per_col) &gt; 0 else 0\n        )\n\n        if max_undecided &gt; 0:\n            max_undecided_idx = np.argmax(undecided_edges_per_col)\n            self._next_exploration = node_names[max_undecided_idx]\n            return self._next_exploration\n\n        # If no suggestion was found, try to find a suggestion from the entire collection of prepared variables.\n        undecided_edges_per_col = np.sum(self._edge_states.m == 0, axis=0)\n        max_undecided = np.max(undecided_edges_per_col)\n\n        if max_undecided &gt; 0:\n            max_undecided_idx = np.argmax(undecided_edges_per_col)\n            self._next_exploration = self._prepared_variables.loc[\n                max_undecided_idx, \"Name\"\n            ]\n            return self._next_exploration\n\n        # If no suggestion was found, return None.\n        self._next_exploration = None\n        return None\n\n    def discover_graph(\n        self,\n        method: str = \"hill_climb\",\n        max_cond_vars: int = 3,\n        model: str = \"gpt-3.5-turbo\",\n    ) -&gt; None:\n        \"\"\"\n        Discover a causal graph based on the prepared table automatically.\n\n        Parameters:\n            method: The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".\n            max_cond_vars: The maximum number of conditioning variables to be used for PC.\n            model: The model to be used for GPT-based graph discovery.\n\n        \"\"\"\n\n        if method == \"PC\":\n            self._graph = CausalDiscoverer.pc(\n                self._prepared_log, max_cond_vars=max_cond_vars\n            )\n        elif method == \"hill_climb\":\n            self._graph = CausalDiscoverer.hill_climb(self._prepared_log)\n        elif method == \"exhaustive\":\n            self._graph = CausalDiscoverer.exhaustive(self._prepared_log)\n        elif method == \"GPT\":\n            self._graph = CausalDiscoverer.gpt(self._prepared_log, model=model)\n        else:\n            raise ValueError(f\"Invalid graph discovery method {method}\")\n\n        self._edge_states.clear_and_set_from_graph(self._graph)\n\n    def get_adjusted_ate(\n        self,\n        treatment: str,\n        outcome: str,\n        confounder: Optional[str] = None,\n    ) -&gt; float:\n        \"\"\"\n        Calculate the adjusted ATE of `treatment` on `outcome`, given the current partial causal graph.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n                causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n\n        Returns:\n            The adjusted ATE of `treatment` on `outcome`.\n        \"\"\"\n        return ATECalculator.get_ate_and_confidence(\n            self.prepared_log,\n            self.prepared_variables,\n            treatment,\n            outcome,\n            confounder,\n            graph=self._graph,\n            calculate_p_value=False,\n            calculate_std_error=False,\n        )[\"ATE\"]\n\n    def get_unadjusted_ate(\n        self,\n        treatment: str,\n        outcome: str,\n    ) -&gt; float:\n        \"\"\"\n        Calculate the unadjusted ATE of `treatment` on `outcome`, ignoring the current partial causal graph\n        in favor of a two-node graph with just `treatment` and `outcome`.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n\n        Returns:\n            The unadjusted ATE of `treatment` on `outcome`.\n        \"\"\"\n        return ATECalculator.get_ate_and_confidence(\n            self.prepared_log,\n            self.prepared_variables,\n            treatment,\n            outcome,\n            calculate_p_value=False,\n            calculate_std_error=False,\n        )[\"ATE\"]\n\n    def challenge_ate(\n        self,\n        treatment: str,\n        outcome: str,\n        num_outputs: int = 10,\n        method: str = \"step\",\n        ignore_current_graph: bool = False,\n        cp: Optional[ClusteringParams] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Identify a ranked list of up to `num_outputs` possible edges among variables in the prepared log\n        which, if set to a different state (i.e. included, reversed or omitted) would most noticeably\n        impact the ATE of `treatment` on `outcome`.\n\n        Parameters:\n            treatment: The name or tag of the treatment variable.\n            outcome: The name or tag of the outcome variable.\n            num_outputs: The number of edges to return.\n            method: The method to be used for edge ranking, among \"step\" and \"clustering\".\n            ignore_current_graph: Whether to ignore the current partial causal graph for the \"clustering\" method.\n            cp: The clustering parameters to be used for \"clustering\" method.\n\n        Returns:\n            A dataframe containing the ranked list of edges.\n        \"\"\"\n\n        return ATECalculator.challenge_ate(\n            self._prepared_log,\n            self._prepared_variables,\n            self._graph if not ignore_current_graph else None,\n            treatment,\n            outcome,\n            self._workdir,\n            num_outputs,\n            method,\n            cp,\n        )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.parsed_log","title":"<code>parsed_log: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed log as a dataframe.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.parsed_variables","title":"<code>parsed_variables: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed variables as a dataframe.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.parsed_templates","title":"<code>parsed_templates: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the parsed templates as a dataframe.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.prepared_log","title":"<code>prepared_log: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the prepared log as a dataframe.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.prepared_variables","title":"<code>prepared_variables: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the prepared variables as a dataframe.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.prepared_variable_names","title":"<code>prepared_variable_names: list[str]</code>  <code>property</code>","text":"<p>Get the names of the prepared variables.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.num_prepared_variables","title":"<code>num_prepared_variables: int</code>  <code>property</code>","text":"<p>Get the number of prepared variables.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.exploration_score","title":"<code>exploration_score: float</code>  <code>property</code>","text":"<p>Calculate the exploration score of the current partial causal graph, based on the edge state matrix.</p> <p>Returns:</p> Type Description <code>float</code> <p>The exploration score of the current partial causal graph.</p>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._set_vars_to_defaults","title":"<code>_set_vars_to_defaults()</code>","text":"<p>Set some of the variables to their default values.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _set_vars_to_defaults(self) -&gt; None:\n    \"\"\"\n    Set some of the variables to their default values.\n    \"\"\"\n    # The parsed log as a dataframe, and metadata about the parsed variables.\n    self._parsed_log: pd.DataFrame = pd.DataFrame()\n    self._parsed_variables: pd.DataFrame = pd.DataFrame()\n    self._parsed_templates: pd.DataFrame = pd.DataFrame()\n\n    # The variable used to define causal units and the number of causal units.\n    self._causal_unit_var: Optional[str] = None\n    self._num_causal_units: Optional[int] = None\n\n    # The prepared log as a dataframe, and metadata about the prepared variables.\n    self._prepared_log: pd.DataFrame = pd.DataFrame()\n    self._prepared_variables: pd.DataFrame = pd.DataFrame()\n\n    # The available aggregation and imputation functions.\n    agg_module = importlib.import_module(\"src.sawmill.aggimp.agg_funcs\")\n    self._agg_funcs: dict[str, Callable] = {\n        n: f for n, f in inspect.getmembers(agg_module, inspect.isfunction)\n    }\n\n    imp_module = importlib.import_module(\"src.sawmill.aggimp.imp_funcs\")\n    self._imp_funcs: dict[str, Callable] = {\n        n: f for n, f in inspect.getmembers(imp_module, inspect.isfunction)\n    }\n\n    # The graph of causal relationships.\n    self._graph: nx.DiGraph = nx.DiGraph()\n\n    # The exploration progress matrix, indicating which edges have been explored.\n    self._edge_states: Optional[EdgeStateMatrix] = None\n\n    # The most recent next exploration suggestion.\n    self._next_exploration: Optional[str] = None\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.prepared_variable_names_with_base_x_and_no_pre_post_agg","title":"<code>prepared_variable_names_with_base_x_and_no_pre_post_agg(x)</code>","text":"<p>Get all prepared variables with the given base variable and no pre- or post-aggregate values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[str, PreparedVariableName]</code> <p>The base variable to check.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of variables with the given base variable and no pre-</p> <code>list[str]</code> <p>or post-aggregate values.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def prepared_variable_names_with_base_x_and_no_pre_post_agg(\n    self, x: Union[str, PreparedVariableName]\n) -&gt; list[str]:\n    \"\"\"\n    Get all prepared variables with the given base variable and no pre-\n    or post-aggregate values.\n\n    Parameters:\n        x: The base variable to check.\n\n    Returns:\n        A list of variables with the given base variable and no pre-\n        or post-aggregate values.\n    \"\"\"\n    return [\n        var\n        for var in self.prepared_variable_names\n        if PreparedVariableName(var).has_base_var(x)\n        and PreparedVariableName(var).no_pre_post_aggs()\n    ]\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.__init__","title":"<code>__init__(filename, workdir=None, skip_writeout=False)</code>","text":"<p>Initialize a Sawmill instance, giving it the full path to the log file that will be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The full path to the log file that will be analyzed.</p> required <code>workdir</code> <code>str</code> <p>The directory where the parsed and prepared dataframes will be stored.</p> <code>None</code> <code>skip_writeout</code> <code>bool</code> <p>Whether to skip writing out the parsed and prepared dataframes.</p> <code>False</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def __init__(\n    self, filename: str, workdir: str = None, skip_writeout: bool = False\n) -&gt; None:\n    \"\"\"\n    Initialize a Sawmill instance, giving it the full path to the log file that will be analyzed.\n\n    Parameters:\n        filename: The full path to the log file that will be analyzed.\n        workdir: The directory where the parsed and prepared dataframes will be stored.\n        skip_writeout: Whether to skip writing out the parsed and prepared dataframes.\n    \"\"\"\n\n    self._set_vars_to_defaults()\n    self._filename = filename\n    print(f\"Initialized Sawmill with log file {filename}\")\n\n    # Set and create working directory\n    if workdir != None:\n        self._workdir = workdir\n    else:\n        self._workdir = os.path.join(\n            \"../../datasets\",\n            \"xyzw_logs\",\n            self._filename.split(\"/\")[-1].split(\".\")[0],\n        )\n    self._skip_writeout = skip_writeout\n\n    if not os.path.exists(self._workdir):\n        os.makedirs(self._workdir, exist_ok=True)\n    print(f\"Work directory set to {self._workdir}\")\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._get_filename","title":"<code>_get_filename(var_name)</code>","text":"<p>Create the file name string for dumping/loading pkl files.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>The name of the variable to be dumped/loaded.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The file name string.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _get_filename(self, var_name: str) -&gt; str:\n    \"\"\"\n    Create the file name string for dumping/loading pkl files.\n\n    Parameters:\n        var_name: The name of the variable to be dumped/loaded.\n\n    Returns:\n        The file name string.\n    \"\"\"\n    return os.path.join(\n        self._workdir,\n        os.path.basename(self._filename)\n        + f\"{var_name}_{self._causal_unit_var}_{self._num_causal_units}.pkl\",\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._find_type","title":"<code>_find_type(row)</code>","text":"<p>Identify the type of a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row of the parsed variables dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _find_type(self, row: pd.Series) -&gt; str:\n    \"\"\"\n    Identify the type of a parsed variable.\n\n    Parameters:\n        row: A row of the parsed variables dataframe.\n\n    Returns:\n        The type of the parsed variable as a string. Options are \"date\", \"time\", \"num\" and \"str\".\n    \"\"\"\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"error\", category=UserWarning)\n\n        try:\n            y = pd.to_numeric(row[\"Examples\"], errors=\"raise\")\n            return \"num\"\n        except Exception as e:\n            try:\n                y = pd.to_timedelta(row[\"Examples\"], errors=\"raise\")\n                return \"time\"\n            except Exception as e:\n                try:\n                    y = pd.to_datetime(row[\"Examples\"], errors=\"raise\")\n                    return \"date\"\n                except Exception as e:\n                    return \"str\"\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._find_uninteresting","title":"<code>_find_uninteresting(row)</code>","text":"<p>Identify whether a parsed variable is likely to be uninteresting.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row of the parsed variables dataframe.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the variable is likely to be uninteresting, False otherwise.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _find_uninteresting(self, row: pd.Series) -&gt; bool:\n    \"\"\"\n    Identify whether a parsed variable is likely to be uninteresting.\n\n    Parameters:\n        row: A row of the parsed variables dataframe.\n\n    Returns:\n        True if the variable is likely to be uninteresting, False otherwise.\n    \"\"\"\n    return (\n        row[\"Type\"] != \"num\"\n        and (self._parsed_log[row[\"Name\"]].nunique() &gt;= 0.15 * row[\"Occurrences\"])\n    ) or (self._parsed_log[row[\"Name\"]].nunique() == 1)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.parse","title":"<code>parse(regex_dict=DEFAULT_REGEX_DICT, sim_thresh=0.65, depth=5, force=False, message_prefix='.*', enable_gpt_tagging=False, gpt_model='gpt-3.5-turbo')</code>","text":"<p>Parse the log file into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>regex_dict</code> <code>dict[str, str]</code> <p>(for Drain) A dictionary of regular expressions to be used for parsing.</p> <code>DEFAULT_REGEX_DICT</code> <code>sim_thresh</code> <code>float</code> <p>(for Drain) The similarity threshold to be used for parsing.</p> <code>0.65</code> <code>depth</code> <code>int</code> <p>(for Drain) The parse tree depth to be used for parsing.</p> <code>5</code> <code>force</code> <code>bool</code> <p>Whether to force re-parsing of the log file.</p> <code>False</code> <code>message_prefix</code> <code>str</code> <p>A prefix used to identify the beginning of each log message. Can be used to collapse multiple lines into a single message. Each line that doesn't start with this prefix will be concatenated to the previous log message.</p> <code>'.*'</code> <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT tagging should be enabled.</p> <code>False</code> <code>gpt_model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> <p>Returns:</p> Type Description <code>str</code> <p>The time elapsed for parsing, as a string.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def parse(\n    self,\n    regex_dict: dict[str, str] = DEFAULT_REGEX_DICT,\n    sim_thresh: float = 0.65,\n    depth: int = 5,\n    force: bool = False,\n    message_prefix: str = r\".*\",\n    enable_gpt_tagging: bool = False,\n    gpt_model: str = \"gpt-3.5-turbo\",\n) -&gt; str:\n    \"\"\"\n    Parse the log file into a dataframe.\n\n    Parameters:\n        regex_dict: (for Drain) A dictionary of regular expressions to be used for parsing.\n        sim_thresh: (for Drain) The similarity threshold to be used for parsing.\n        depth: (for Drain) The parse tree depth to be used for parsing.\n        force: Whether to force re-parsing of the log file.\n        message_prefix: A prefix used to identify the beginning of each log message.\n            Can be used to collapse multiple lines into a single message. Each line that doesn't start with this\n            prefix will be concatenated to the previous log message.\n        enable_gpt_tagging: A boolean indicating whether GPT tagging should be enabled.\n        gpt_model: The GPT model to use.\n\n    Returns:\n        The time elapsed for parsing, as a string.\n    \"\"\"\n    start_time = datetime.now()\n    parser = Drain(\n        indir=os.path.dirname(self._filename),\n        depth=depth,\n        st=sim_thresh,\n        rex=regex_dict,\n        skip_writeout=self._skip_writeout,\n        message_prefix=message_prefix,\n    )\n\n    # Check if the parsed files already exist.\n    files_exist = not force\n    parsed_df_names = [\n        nameof(self._parsed_log),\n        nameof(self._parsed_templates),\n        nameof(self._parsed_variables),\n    ]\n    for var_name in parsed_df_names:\n        if not os.path.isfile(self._get_filename(var_name)):\n            files_exist = False\n            break\n\n    if files_exist:\n        self._parsed_log = Pickler.load(self._get_filename(parsed_df_names[0]))\n        self._parsed_templates = Pickler.load(\n            self._get_filename(parsed_df_names[1])\n        )\n        self._parsed_variables = Pickler.load(\n            self._get_filename(parsed_df_names[2])\n        )\n    else:\n        (\n            self._parsed_log,\n            self._parsed_templates,\n            self._parsed_variables,\n        ) = parser.parse(self._filename.split(\"/\")[-1])\n        tqdm.pandas(desc=\"Determining variable types...\")\n        self._parsed_variables[\"Type\"] = self._parsed_variables.progress_apply(\n            self._find_type, axis=1\n        )\n\n        # Cast and convert date columns\n        is_date = self._parsed_variables[\"Type\"] == \"date\"\n        date_cols = self._parsed_variables.loc[is_date, \"Name\"]\n        tqdm.pandas(desc=\"Casting date variables...\")\n        self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_apply(\n            pd.to_datetime, errors=\"coerce\"\n        )\n        tqdm.pandas(desc=\"Casting date variables round 2...\")\n        self._parsed_log[date_cols] = self._parsed_log[date_cols].progress_applymap(\n            lambda x: x.timestamp() if not pd.isnull(x) else None\n        )\n        self._parsed_variables.loc[is_date, \"Type\"] = \"num\"\n\n        # Cast and convert time columns\n        is_time = self._parsed_variables[\"Type\"] == \"time\"\n        time_cols = self._parsed_variables.loc[is_time, \"Name\"]\n        tqdm.pandas(desc=\"Casting time variables...\")\n        self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_apply(\n            pd.to_timedelta, errors=\"coerce\"\n        )\n        tqdm.pandas(desc=\"Casting time variables round 2...\")\n        self._parsed_log[time_cols] = self._parsed_log[time_cols].progress_applymap(\n            lambda x: x.total_seconds() if not pd.isnull(x) else None\n        )\n        self._parsed_variables.loc[is_time, \"Type\"] = \"num\"\n\n        # Cast numeric columns\n        is_num = self._parsed_variables[\"Type\"] == \"num\"\n        numeric_cols = self._parsed_variables.loc[is_num, \"Name\"]\n        tqdm.pandas(desc=\"Casting numerical variables...\")\n        self._parsed_log[numeric_cols] = self._parsed_log[\n            numeric_cols\n        ].progress_apply(pd.to_numeric, errors=\"coerce\")\n\n        # Tag variables.\n        tqdm.pandas(desc=\"Tagging variables...\")\n        tag, tag_from_gpt = zip(\n            *self._parsed_variables.progress_apply(\n                lambda x: TagUtils.best_effort_tag(\n                    self.parsed_templates, x, enable_gpt_tagging, gpt_model\n                ),\n                axis=1,\n            )\n        )\n        self._parsed_variables[\"Tag\"] = tag\n        self._parsed_variables[\"TagFromGPT\"] = tag_from_gpt\n        TagUtils.deduplicate_tags(self._parsed_variables)\n\n        # Detect identifiers.\n        tqdm.pandas(desc=\"Detecting identifiers...\")\n        self._parsed_variables[\n            \"IsUninteresting\"\n        ] = self._parsed_variables.progress_apply(self._find_uninteresting, axis=1)\n\n        # Reorder columns.\n        self._parsed_variables = self._parsed_variables[\n            [\n                \"Name\",\n                \"Tag\",\n                \"Type\",\n                \"IsUninteresting\",\n                \"Occurrences\",\n                \"Preceding 3 tokens\",\n                \"Examples\",\n                \"From regex\",\n            ]\n        ]\n\n    # Write out files if appropriate.\n    if not self._skip_writeout and not files_exist:\n        Pickler.dump(self._parsed_log, self._get_filename(parsed_df_names[0]))\n        Pickler.dump(self._parsed_templates, self._get_filename(parsed_df_names[1]))\n        Pickler.dump(self._parsed_variables, self._get_filename(parsed_df_names[2]))\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(f\"Parsing complete in {elapsed} seconds!\")\n    return elapsed\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.include_in_template","title":"<code>include_in_template(name, enable_gpt_tagging=False, gpt_model='gpt-3.5-turbo')</code>","text":"<p>Treat a certain parsed variable as part of its template and regenerate parsed dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to be included in its template.</p> required <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT-3.5 tagging should be enabled.</p> <code>False</code> <code>gpt_model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def include_in_template(\n    self,\n    name: str,\n    enable_gpt_tagging: bool = False,\n    gpt_model: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Treat a certain parsed variable as part of its template and regenerate parsed dataframes.\n\n    Parameters:\n        name: The name of the variable to be included in its template.\n        enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n        gpt_model: The GPT model to use.\n    \"\"\"\n\n    old_template_id = ParsedVariableName(name).template_id()\n    idx = ParsedVariableName(name).index()\n    value_counts = self._parsed_log[name].value_counts().to_dict()\n\n    ### Modify _parsed_templates\n    old_template_row = (\n        self._parsed_templates.loc[\n            self._parsed_templates[\"TemplateId\"] == old_template_id\n        ]\n        .iloc[0]\n        .copy()\n    )\n    toks = old_template_row[\"TemplateText\"].split(\" \")\n    new_template_ids = {}\n    new_variable_indices = old_template_row[\"VariableIndices\"]\n    new_variable_indices.remove(idx)\n\n    for value, occurences in value_counts.items():\n        new_template_row = old_template_row.copy()\n        toks[idx] = value\n\n        new_template_row[\"TemplateText\"] = \" \".join(toks)\n        new_template_row[\"TemplateId\"] = hashlib.md5(\n            new_template_row[\"TemplateText\"].encode(\"utf-8\")\n        ).hexdigest()[0:8]\n        new_template_row[\"Occurrences\"] = occurences\n        new_template_row[\"VariableIndices\"] = new_variable_indices\n        new_template_row[\"RegexIndices\"] = old_template_row[\"RegexIndices\"]\n\n        self._parsed_templates.loc[len(self._parsed_templates)] = new_template_row\n        new_template_ids[value] = new_template_row[\"TemplateId\"]\n\n    self._parsed_templates = self._parsed_templates[\n        self._parsed_templates[\"TemplateId\"] != old_template_id\n    ].reset_index(drop=True)\n\n    ### Modify _parsed_log\n\n    # Update the template ids of all rows that belonged to the old template\n    self._parsed_log[\"TemplateId\"] = self._parsed_log.apply(\n        lambda x: new_template_ids[x[name]]\n        if (x[\"TemplateId\"] == old_template_id)\n        else x[\"TemplateId\"],\n        axis=1,\n    )\n\n    # Create new variables for each new template id and assign the value of the old variables to them\n    new_variables = []\n    for new_template_id in new_template_ids.values():\n        for other_idx in new_variable_indices:\n            new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n            new_variables.append(new_var_name)\n            self._parsed_log[new_var_name] = self._parsed_log.apply(\n                lambda x: x[f\"{old_template_id}_{other_idx}\"]\n                if (x[\"TemplateId\"] == new_template_id)\n                else None,\n                axis=1,\n            )\n\n    # Drop variable columns associated with old template id\n    variables_to_drop = [\n        v for v in self._parsed_log.columns if v.startswith(old_template_id)\n    ]\n    self._parsed_log.drop(columns=variables_to_drop, inplace=True)\n\n    ### Modify _parsed_variables\n\n    # Add variable rows for each new variable\n    for value, occurrences in value_counts.items():\n        for other_idx in new_variable_indices:\n            new_template_id = new_template_ids[value]\n            new_var_name = f\"{new_template_id}_{str(other_idx)}\"\n\n            x = {}\n            x[\"Name\"] = new_var_name\n            x[\"Occurrences\"] = occurrences\n            x[\"Preceding 3 tokens\"] = (\n                self._parsed_templates[\n                    self._parsed_templates[\"TemplateId\"] == new_template_id\n                ][\"TemplateText\"]\n                .values[0]\n                .split()[max(0, other_idx - 3) : other_idx]\n            )\n            x[\"Examples\"] = (\n                self._parsed_log[new_var_name]\n                .loc[self._parsed_log[new_var_name].notna()]\n                .unique()[:5]\n                .tolist()\n            )\n            x[\"From regex\"] = False\n            x[\"Tag\"], x[\"TagFromGPT\"] = TagUtils.best_effort_tag(\n                self.parsed_templates, pd.Series(x), enable_gpt_tagging, gpt_model\n            )\n            x[\"Type\"] = self._find_type(pd.Series(x))\n            x[\"IsUninteresting\"] = self._find_uninteresting(pd.Series(x))\n\n            self._parsed_variables.loc[len(self._parsed_variables)] = x\n\n    # Drop variable rows associated with old template id\n    self._parsed_variables = self._parsed_variables[\n        ~self._parsed_variables[\"Name\"].isin(variables_to_drop)\n    ].reset_index(drop=True)\n\n    # Deduplicate tags again\n    TagUtils.deduplicate_tags(self._parsed_variables)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.tag_parsed_variable","title":"<code>tag_parsed_variable(name, tag)</code>","text":"<p>Tag a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to be tagged.</p> required <code>tag</code> <code>str</code> <p>The tag to be assigned to the variable.</p> required Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def tag_parsed_variable(self, name: str, tag: str) -&gt; None:\n    \"\"\"\n    Tag a parsed variable.\n\n    Parameters:\n        name: The name of the variable to be tagged.\n        tag: The tag to be assigned to the variable.\n    \"\"\"\n    TagUtils.set_tag(self._parsed_variables, name, tag, \"parsed\")\n    TagUtils.deduplicate_tags(self._parsed_variables)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.get_tag_of_parsed","title":"<code>get_tag_of_parsed(name)</code>","text":"<p>Get the tag of a parsed variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The tag of the variable.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def get_tag_of_parsed(self, name: str) -&gt; str:\n    \"\"\"\n    Get the tag of a parsed variable.\n\n    Parameters:\n        name: The name of the variable.\n\n    Returns:\n        The tag of the variable.\n    \"\"\"\n    return TagUtils.get_tag(self._parsed_variables, name, \"parsed\")\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.tag_prepared_variable","title":"<code>tag_prepared_variable(name, tag)</code>","text":"<p>Tag a prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to be tagged.</p> required <code>tag</code> <code>str</code> <p>The tag to be assigned to the variable.</p> required Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def tag_prepared_variable(self, name: str, tag: str) -&gt; None:\n    \"\"\"\n    Tag a prepared variable.\n\n    Parameters:\n        name: The name of the variable to be tagged.\n        tag: The tag to be assigned to the variable.\n    \"\"\"\n    TagUtils.set_tag(self._prepared_variables, name, tag, \"prepared\")\n    TagUtils.deduplicate_tags(self._prepared_variables)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.get_tag_of_prepared","title":"<code>get_tag_of_prepared(name)</code>","text":"<p>Get the tag of a prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The tag of the variable.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def get_tag_of_prepared(self, name: str) -&gt; str:\n    \"\"\"\n    Get the tag of a prepared variable.\n\n    Parameters:\n        name: The name of the variable.\n\n    Returns:\n        The tag of the variable.\n    \"\"\"\n    return TagUtils.get_tag(self._prepared_variables, name, \"prepared\")\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.get_causal_unit_info","title":"<code>get_causal_unit_info()</code>","text":"<p>Get the variable used to define causal units and the number of causal units.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the variable used to define causal units</p> <code>int</code> <p>and the number of causal units.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def get_causal_unit_info(self) -&gt; Tuple[str, int]:\n    \"\"\"\n    Get the variable used to define causal units and the number of\n    causal units.\n\n    Returns:\n        The name of the variable used to define causal units\n        and the number of causal units.\n    \"\"\"\n    return self._causal_unit_var, self._num_causal_units\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.suggest_causal_unit_defs","title":"<code>suggest_causal_unit_defs(min_causal_units=4, num_suggestions=10)</code>","text":"<p>Suggest at most <code>num_suggestions</code> causal unit definitions based on IUS maximization, while returning at least <code>min_causal_units</code> causal units.</p> <p>Parameters:</p> Name Type Description Default <code>min_causal_units</code> <code>int</code> <p>The minimum number of causal units that a suggested definition should create.</p> <code>4</code> <code>num_suggestions</code> <code>int</code> <p>The maximum number of causal unit definitions to suggest.</p> <code>10</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame with one row for each suggested causal unit definition, or <code>None</code> if no suggestions were made.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def suggest_causal_unit_defs(\n    self,\n    min_causal_units: int = 4,\n    num_suggestions: int = 10,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Suggest at most `num_suggestions` causal unit definitions based on IUS maximization,\n    while returning at least `min_causal_units` causal units.\n\n    Parameters:\n        min_causal_units: The minimum number of causal units that a suggested\n            definition should create.\n        num_suggestions: The maximum number of causal unit definitions to suggest.\n\n    Returns:\n        A DataFrame with one row for each suggested causal unit definition, or `None`\n            if no suggestions were made.\n    \"\"\"\n\n    return CausalUnitSuggester.suggest_causal_unit_defs(\n        self._parsed_log[self._parsed_variables[\"Name\"].values],\n        self._parsed_variables,\n        min_causal_units=min_causal_units,\n        num_suggestions=num_suggestions,\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.set_causal_unit","title":"<code>set_causal_unit(var, num_units=None)</code>","text":"<p>Set the variable used to define causal units and optionally the number of causal units. The latter will be ignored if the variable is categorical, but it must be specified if the variable is numerical.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable to be used as the causal unit.</p> required <code>num_units</code> <code>Optional[int]</code> <p>The number of causal units to be created.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the variable is numerical and <code>num_units</code> is not specified.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def set_causal_unit(\n    self,\n    var: str,\n    num_units: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Set the variable used to define causal units and optionally the number of\n    causal units. The latter will be ignored if the variable is categorical, but it\n    must be specified if the variable is numerical.\n\n    Parameters:\n        var: The name or tag of the variable to be used as the causal unit.\n        num_units: The number of causal units to be created.\n\n    Raises:\n        ValueError: If the variable is numerical and `num_units` is not specified.\n    \"\"\"\n    var_name = TagUtils.name_of(self._parsed_variables, var, \"parsed\")\n    var_type = self._parsed_variables.loc[\n        self._parsed_variables[\"Name\"] == var_name, \"Type\"\n    ].values[0]\n\n    if var_type == \"num\" and num_units is None:\n        raise ValueError(\n            \"The number of causal units must be specified if the causal unit is numerical.\"\n        )\n\n    self._causal_unit_var = var_name\n    self._num_causal_units = num_units\n\n    print(\n        f\"Causal unit set to {var_name} (tag: {self.get_tag_of_parsed(var_name)}) \"\n        + (\n            \"\"\n            if not self._num_causal_units\n            else f\" with {self._num_causal_units} causal units.\"\n        )\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.prepare","title":"<code>prepare(custom_agg={}, custom_imp={}, count_occurences=False, ignore_uninteresting=True, force=False, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER, drop_bad_aggs=True, reject_prunable_edges=True)</code>","text":"<p>Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.</p> <p>Parameters:</p> Name Type Description Default <code>custom_agg</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom aggregation functions to be used for specific variables.</p> <code>{}</code> <code>custom_imp</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom imputation functions to be used for specific variables.</p> <code>{}</code> <code>count_occurences</code> <code>bool</code> <p>Whether to include extra variables counting the occurence of each template.</p> <code>False</code> <code>ignore_uninteresting</code> <code>bool</code> <p>Whether to ignore uninteresting variables.</p> <code>True</code> <code>force</code> <code>bool</code> <p>Whether to force re-preparation of the log.</p> <code>False</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for LASSO regression.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for LASSO regression.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <code>drop_bad_aggs</code> <code>bool</code> <p>Whether to drop prepared variables that do not add information compared to other variables based on the same base variable but using a different aggregation function.</p> <code>True</code> <code>reject_prunable_edges</code> <code>bool</code> <p>Whether to reject edges that are prunable based on LASSO applciation.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The time elapsed for preparation, as a string.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def prepare(\n    self,\n    custom_agg: dict[str, list[str]] = {},\n    custom_imp: dict[str, list[str]] = {},\n    count_occurences: bool = False,\n    ignore_uninteresting: bool = True,\n    force: bool = False,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n    drop_bad_aggs: bool = True,\n    reject_prunable_edges: bool = True,\n) -&gt; str:\n    \"\"\"\n    Prepare the log parsed from the table for causal analysis, using aggregation and imputation as needed.\n\n    Parameters:\n        custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n        custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n        count_occurences: Whether to include extra variables counting the occurence of each template.\n        ignore_uninteresting: Whether to ignore uninteresting variables.\n        force: Whether to force re-preparation of the log.\n        lasso_alpha: The alpha parameter to be used for LASSO regression.\n        lasso_max_iter: The maximum number of iterations to be used for LASSO regression.\n        drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n            variables based on the same base variable but using a different aggregation function.\n        reject_prunable_edges: Whether to reject edges that are prunable based on LASSO applciation.\n\n    Returns:\n        The time elapsed for preparation, as a string.\n    \"\"\"\n\n    start_time = datetime.now()\n    # Ensure causal unit is set. TODO: make IUS maximizer the default\n    if self._causal_unit_var is None:\n        print(\"Causal unit not defined. Aborting.\")\n        return None\n\n    # Check if the prepared files already exist.\n    files_exist = not force\n    prepared_df_names = [\n        nameof(self._prepared_log),\n        nameof(self._prepared_variables),\n    ]\n    for var_name in prepared_df_names:\n        if not os.path.isfile(self._get_filename(var_name)):\n            files_exist = False\n            break\n\n    if files_exist:\n        self._prepared_log = Pickler.load(self._get_filename(prepared_df_names[0]))\n        self._prepared_variables = Pickler.load(\n            self._get_filename(prepared_df_names[1])\n        )\n    else:\n        self._prepare_anew(\n            custom_agg,\n            custom_imp,\n            count_occurences=count_occurences,\n            ignore_uninteresting=ignore_uninteresting,\n            drop_bad_aggs=drop_bad_aggs,\n        )\n\n    self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n    if reject_prunable_edges:\n        print(f\"Pruning edges...\")\n        self.reject_all_prunable_edges(\n            lasso_alpha=lasso_alpha, lasso_max_iter=lasso_max_iter\n        )\n\n    end_time = datetime.now()\n    elapsed = \"{:.6f}\".format((end_time - start_time).total_seconds())\n    print(\n        f\"\"\"Preparation complete in {elapsed} seconds! \"\"\"\n        f\"\"\"{np.count_nonzero(self._edge_states.m == -1)} of the {self.num_prepared_variables ** 2} possible edges were auto-rejected.\"\"\"\n    )\n\n    return elapsed\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._prepare_anew","title":"<code>_prepare_anew(custom_agg={}, custom_imp={}, count_occurences=False, ignore_uninteresting=True, drop_bad_aggs=True)</code>","text":"<p>Prepare the log anew.</p> <p>Parameters:</p> Name Type Description Default <code>custom_agg</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom aggregation functions to be used for specific variables.</p> <code>{}</code> <code>custom_imp</code> <code>dict[str, list[str]]</code> <p>A dictionary of custom imputation functions to be used for specific variables.</p> <code>{}</code> <code>count_occurences</code> <code>bool</code> <p>Whether to include extra variables counting the occurence of each template.</p> <code>False</code> <code>ignore_uninteresting</code> <code>bool</code> <p>Whether to ignore uninteresting variables.</p> <code>True</code> <code>drop_bad_aggs</code> <code>bool</code> <p>Whether to drop prepared variables that do not add information compared to other variables based on the same base variable but using a different aggregation function.</p> <code>True</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _prepare_anew(\n    self,\n    custom_agg: dict[str, list[str]] = {},\n    custom_imp: dict[str, list[str]] = {},\n    count_occurences: bool = False,\n    ignore_uninteresting: bool = True,\n    drop_bad_aggs: bool = True,\n) -&gt; None:\n    \"\"\"\n    Prepare the log anew.\n\n    Parameters:\n        custom_agg: A dictionary of custom aggregation functions to be used for specific variables.\n        custom_imp: A dictionary of custom imputation functions to be used for specific variables.\n        count_occurences: Whether to include extra variables counting the occurence of each template.\n        ignore_uninteresting: Whether to ignore uninteresting variables.\n        drop_bad_aggs: Whether to drop prepared variables that do not add information compared to other\n            variables based on the same base variable but using a different aggregation function.\n    \"\"\"\n\n    print(f\"Determining the causal unit assignment...\")\n    causal_unit_assignment = CausalUnitSuggester._discretize(\n        self._parsed_log[self._causal_unit_var],\n        self._parsed_variables[\n            self._parsed_variables[\"Name\"] == self._causal_unit_var\n        ][\"Type\"].values[0],\n        self._num_causal_units,\n    )\n\n    # Convert keys in custom_agg and custom_imp to the names of the variables, if they are tags.\n    custom_agg = {\n        TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n        for k, v in custom_agg.items()\n    }\n    custom_imp = {\n        TagUtils.name_of(self._parsed_variables, k, \"parsed\"): v\n        for k, v in custom_imp.items()\n    }\n\n    # Start with the parsed log, optionally with extra variables counting the occurence of each template.\n    if count_occurences:\n        print(f\"Adding template occurrence count variables...\")\n        self._prepared_log = pd.concat(\n            [\n                self._parsed_log,\n                pd.get_dummies(\n                    self._parsed_log[\"TemplateId\"],\n                    prefix=\"TemplateId\",\n                    prefix_sep=\"=\",\n                ),\n            ],\n            axis=1,\n        )\n    else:\n        self._prepared_log = self._parsed_log.copy(deep=True)\n\n    # No longer need the column storing the actual template IDs\n    self._prepared_log.drop(columns=\"TemplateId\", inplace=True)\n\n    # Build dictionary of aggregation functions\n    agg_dict: dict[str, str] = {\n        variable.Name: (\n            custom_agg[variable.Name]\n            if variable.Name in custom_agg\n            else AggregateSelector.DEFAULT_AGGREGATES[variable.Type]\n        )\n        for variable in self._parsed_variables.itertuples()\n    }\n\n    # Add aggregations for template counts\n    for col in self._prepared_log.columns:\n        if PreparedVariableName(col).base_var() == \"TemplateId\":\n            agg_dict[col] = [\"sum\"]\n\n    # Drop uninteresting columns if requested, except if they are the causal unit.\n    ui_cols = self._parsed_variables.loc[\n        self._parsed_variables[\"IsUninteresting\"], \"Name\"\n    ].values\n    ui_cols = [x for x in ui_cols if x != self._causal_unit_var]\n    if ignore_uninteresting:\n        self._prepared_log.drop(\n            columns=ui_cols,\n            inplace=True,\n        )\n        for col in ui_cols:\n            agg_dict.pop(col, None)\n        print(\n            f\"Dropped {len(ui_cols)} uninteresting columns, out of an original total of {len(self.parsed_variables)}.\"\n        )\n\n    # Ensure the causal unit variable only has one aggregation function\n    agg_dict[self._causal_unit_var] = agg_dict[self._causal_unit_var][:1]\n\n    # Perform the aggregation\n    print(\"Calculating aggregates for each causal unit...\")\n    agg_func_dict: dict[str, list[Callable]] = {\n        name: [self._agg_funcs[f] for f in funcs]\n        for name, funcs in agg_dict.items()\n    }\n    self._prepared_log = self._prepared_log.groupby(\n        causal_unit_assignment\n    ).aggregate(agg_func_dict)\n    self._prepared_log.columns = [\n        \"+\".join(col) for col in self._prepared_log.columns.values\n    ]\n    self._parsed_variables[\"Aggregates\"] = self._parsed_variables[\"Name\"].map(\n        lambda x: agg_dict.get(x, [])\n    )\n    self._prepared_log.set_index(\n        f\"{self._causal_unit_var}+{self._parsed_variables[self._parsed_variables['Name'] == self._causal_unit_var]['Aggregates'].values[0][0]}\",\n        inplace=True,\n    )\n    self._prepared_log.sort_index(inplace=True)\n    self._prepared_log.index = self._prepared_log.index.astype(str)\n\n    # Perform the imputation\n    for col in tqdm(self._prepared_log.columns, desc=\"Imputing missing values...\"):\n        if self._prepared_log[col].isnull().values.any():\n            base_var = PreparedVariableName(col).base_var()\n            func_name: str = (\n                custom_imp[base_var] if base_var in custom_imp else \"no_imp\"\n            )\n            self._prepared_log[col] = (self._imp_funcs[func_name])(\n                self._prepared_log[col]\n            )\n    self._prepared_log.dropna(inplace=True)\n\n    # Drop variables that do not add information compared to other variables based on the same base variable\n    # but using a different aggregation function.\n    if drop_bad_aggs:\n        print(f\"Dropping aggregates that do not add information...\")\n        cols_to_drop = AggregateSelector.find_uninformative_aggregates(\n            self._prepared_log, self._parsed_variables, self._causal_unit_var\n        )\n        self._prepared_log.drop(columns=cols_to_drop, inplace=True)\n\n    # Identify the categorical variables and one-hot encode them\n    categorical_vars = self._prepared_log.select_dtypes(\n        include=\"object\"\n    ).columns.tolist()\n    for col in tqdm(\n        categorical_vars, desc=\"One-hot encoding categorical variables...\"\n    ):\n        self._prepared_log = pd.concat(\n            [\n                self._prepared_log,\n                pd.get_dummies(\n                    self._prepared_log[col], prefix=col, prefix_sep=\"=\", dtype=int\n                ),\n            ],\n            axis=1,\n        )\n        self._prepared_log.drop(col, axis=1, inplace=True)\n    # Deal with https://github.com/pydot/pydot/issues/258\n    self._prepared_log.columns = [\n        x.replace(\":\", \";\") for x in self._prepared_log.columns\n    ]\n\n    # Generate dataframe of prepared variables for later tagging etc.\n    self._generate_prepared_variables_df()\n\n    # Convert any date columns to Unix timestamps in milliseconds\n    date_cols = self._prepared_variables.loc[\n        self._prepared_variables[\"Type\"] == \"date\", \"Name\"\n    ].values\n    self._prepared_log[date_cols] = self._prepared_log[date_cols].map(\n        lambda x: x.timestamp() * 1000.0\n    )\n\n    # Convert any time columns to milliseconds\n    time_cols = self._prepared_variables.loc[\n        self._prepared_variables[\"Type\"] == \"time\", \"Name\"\n    ].values\n    self._prepared_log[time_cols] = self._prepared_log[time_cols].map(\n        lambda x: x.total_seconds() * 1000.0\n    )\n\n    # Write out prepared log and variables\n    if not self._skip_writeout:\n        Pickler.dump(\n            self._prepared_log, self._get_filename(nameof(self._prepared_log))\n        )\n        Pickler.dump(\n            self._prepared_variables,\n            self._get_filename(nameof(self._prepared_variables)),\n        )\n\n    print(\n        f\"\"\"Successfully prepared the log with causal unit {self._causal_unit_var} \"\"\"\n        f\"\"\"(tag: {self.get_tag_of_parsed(self._causal_unit_var)})\"\"\"\n        + (\n            \"\"\n            if not self._num_causal_units\n            else f\" with {self._num_causal_units} causal units.\"\n        )\n    )\n\n    return\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill._generate_prepared_variables_df","title":"<code>_generate_prepared_variables_df()</code>","text":"<p>Generate dataframe of prepared variables for later tagging etc.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def _generate_prepared_variables_df(self) -&gt; None:\n    \"\"\"\n    Generate dataframe of prepared variables for later tagging etc.\n    \"\"\"\n\n    self._prepared_variables = pd.DataFrame()\n    self._prepared_variables[\"Name\"] = self._prepared_log.columns\n\n    # Bring in varable name components leveraging PreparedVariableName\n    self._prepared_variables[\"Base\"] = self._prepared_variables[\"Name\"].apply(\n        lambda x: PreparedVariableName(x).base_var()\n    )\n    self._prepared_variables[\"Pre-agg Value\"] = self._prepared_variables[\n        \"Name\"\n    ].apply(lambda x: PreparedVariableName(x).pre_agg_value())\n    self._prepared_variables[\"Agg\"] = self._prepared_variables[\"Name\"].apply(\n        lambda x: PreparedVariableName(x).aggregate()\n    )\n    self._prepared_variables[\"Post-agg Value\"] = self._prepared_variables[\n        \"Name\"\n    ].apply(lambda x: PreparedVariableName(x).post_agg_value())\n\n    # Bring in other info from self._parsed_variables\n    self._prepared_variables[\"Tag\"] = self._prepared_variables.apply(\n        lambda x: (\n            self._parsed_variables.loc[\n                self._parsed_variables[\"Name\"] == x[\"Base\"],\n                \"Tag\",\n            ].values[0]\n            if x[\"Base\"] != \"TemplateId\"\n            else \"TemplateId\"\n        )\n        + (f\" {x['Pre-agg Value']}\" if x[\"Pre-agg Value\"] != \"\" else \"\")\n        + (f\" {x['Agg']}\" if x[\"Agg\"] != \"\" else \"\")\n        + (f\" {x['Post-agg Value']}\" if x[\"Post-agg Value\"] != \"\" else \"\"),\n        axis=1,\n    )\n    self._prepared_variables[\"Base Variable Occurences\"] = self._prepared_variables[\n        \"Base\"\n    ].apply(\n        lambda x: self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == x, \"Occurrences\"\n        ].values[0]\n        if x != \"TemplateId\"\n        else \"\"\n    )\n    self._prepared_variables[\"Type\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == x, \"Type\"\n        ].values[0]\n        if x != \"TemplateId\"\n        else \"\"\n    )\n    self._prepared_variables[\"Examples\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == x, \"Examples\"\n        ].values[0]\n        if x != \"TemplateId\"\n        else \"\"\n    )\n    self._prepared_variables[\"From regex\"] = self._prepared_variables[\"Base\"].apply(\n        lambda x: self._parsed_variables.loc[\n            self._parsed_variables[\"Name\"] == x, \"From regex\"\n        ].values[0]\n        if x != \"TemplateId\"\n        else \"\"\n    )\n\n    # Bring in template text, only for appropriate base variables.\n    self._prepared_variables[\"TemplateText\"] = self._prepared_variables.apply(\n        lambda x: self._parsed_templates.loc[\n            self._parsed_templates[\"TemplateId\"]\n            == PreparedVariableName(x[\"Name\"]).template_id(),\n            \"TemplateText\",\n        ].values[0]\n        if x[\"From regex\"] == False\n        else \"\",\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.inspect","title":"<code>inspect(var, ref_var=None, row_limit=10)</code>","text":"<p>Print information about a specific prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>The name or tag of the variable.</p> required <code>ref_var</code> <code>Optional[str]</code> <p>The name or tag of a reference variable.</p> <code>None</code> <code>row_limit</code> <code>Optional[int]</code> <p>The number of rows of the prepared log to print out, to illustrate example values of this variable.</p> <code>10</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def inspect(\n    self,\n    var: str,\n    ref_var: Optional[str] = None,\n    row_limit: Optional[int] = 10,\n) -&gt; None:\n    \"\"\"\n    Print information about a specific prepared variable.\n\n    Parameters:\n        var: The name or tag of the variable.\n        ref_var: The name or tag of a reference variable.\n        row_limit: The number of rows of the prepared log to print out,\n            to illustrate example values of this variable.\n    \"\"\"\n\n    # Retrieve the name of this variable, if a tag was passed in.\n    name = TagUtils.name_of(self._prepared_variables, var, \"prepared\")\n\n    print(f\"Information about prepared variable {name}:\\n\")\n    base_var = PreparedVariableName(name).base_var()\n    from_regex = False\n\n    if base_var != \"TemplateId\":\n        print(f\"--&gt; Variable Information about {base_var}:\")\n        sample = self._parsed_variables[self._parsed_variables[\"Name\"] == base_var]\n        from_regex = sample[\"From regex\"].values[0]\n        display(sample)\n\n    if not from_regex:\n        template_id = PreparedVariableName(name).template_id()\n        print(f\"--&gt; Template Information about {template_id}:\")\n        display(\n            self._parsed_templates[\n                self._parsed_templates[\"TemplateId\"] == template_id\n            ]\n        )\n\n    print(\"--&gt; Causal Unit Partial Information:\")\n    if row_limit == None:\n        row_limit = len(self._prepared_log)\n    col_list = [name]\n    col_list.extend([ref_var] if ref_var is not None else [])\n    sample = self._prepared_log[col_list].head(row_limit)\n    col_names = [f\"{name} (candidate)\"]\n    col_names.extend([f\"{ref_var} (outcome)\"] if ref_var is not None else [])\n    sample.columns = col_names\n    display(sample)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.clear_graph","title":"<code>clear_graph(clear_edge_states=True)</code>","text":"<p>Clear the graph and possibly edge states.</p> <p>Parameters:</p> Name Type Description Default <code>clear_edge_states</code> <code>bool</code> <p>Whether to also clear the edge states.</p> <code>True</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def clear_graph(self, clear_edge_states: bool = True) -&gt; None:\n    \"\"\"\n    Clear the graph and possibly edge states.\n\n    Parameters:\n        clear_edge_states: Whether to also clear the edge states.\n    \"\"\"\n    self._graph = nx.DiGraph()\n    if clear_edge_states:\n        self._edge_states = EdgeStateMatrix(self.prepared_variable_names)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.display_graph","title":"<code>display_graph()</code>","text":"<p>Display the current graph.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def display_graph(self) -&gt; None:\n    \"\"\"\n    Display the current graph.\n    \"\"\"\n    GraphRenderer.display_graph(self._graph, self._prepared_variables)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.accept","title":"<code>accept(src, dst, interactive=True)</code>","text":"<p>Mark a causal graph edge as accepted.</p> <p>This will also reject the edge from <code>dst</code> to <code>src</code> and remove any other variables with the same base variable as either <code>src</code> or <code>dst</code> from consideration for the partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after accepting the edge.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge addition, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def accept(\n    self,\n    src: str,\n    dst: str,\n    interactive: bool = True,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark a causal graph edge as accepted.\n\n    This will also reject the edge from `dst` to `src` and remove any other variables with the\n    same base variable as either `src` or `dst` from consideration for the partial causal graph.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        interactive: Whether to display the graph interactively after accepting the edge.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge addition,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n\n    src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    to_drop = self._edge_states.mark_edge(src_name, dst_name, \"Accepted\")\n    for node in to_drop:\n        if node in self._graph.nodes:\n            self._graph.remove_node(node)\n\n    self._graph.add_node(src_name)\n    self._graph.add_node(dst_name)\n    self._graph.add_edge(src_name, dst_name)\n    if (dst_name, src_name) in self._graph.edges:\n        self._graph.remove_edge(dst_name, src_name)\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n        if not interactive\n        else \"\",\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.reject","title":"<code>reject(src, dst, interactive=True)</code>","text":"<p>Mark a causal graph edge as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>The name or tag of the source variable.</p> required <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after rejecting the edge.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejection, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def reject(\n    self,\n    src: str,\n    dst: str,\n    interactive: bool = True,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark a causal graph edge as rejected.\n\n    Parameters:\n        src: The name or tag of the source variable.\n        dst: The name or tag of the destination variable.\n        interactive: Whether to display the graph interactively after rejecting the edge.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejection,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n\n    src_name = TagUtils.name_of(self._prepared_variables, src, \"prepared\")\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    self._edge_states.mark_edge(src_name, dst_name, \"Rejected\")\n\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n        if not interactive\n        else \"\",\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.reject_undecided_incoming","title":"<code>reject_undecided_incoming(dst, interactive=True)</code>","text":"<p>Mark all undecided incoming edges to a variable as rejected.</p> <p>Parameters:</p> Name Type Description Default <code>dst</code> <code>str</code> <p>The name or tag of the destination variable.</p> required <code>interactive</code> <code>bool</code> <p>Whether to display the graph interactively after rejecting the edges.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejections, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def reject_undecided_incoming(\n    self, dst: str, interactive: bool = True\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    Mark all undecided incoming edges to a variable as rejected.\n\n    Parameters:\n        dst: The name or tag of the destination variable.\n        interactive: Whether to display the graph interactively after rejecting the edges.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejections,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n    dst_name = TagUtils.name_of(self._prepared_variables, dst, \"prepared\")\n    for v in self.prepared_variable_names:\n        if self._edge_states.get_edge_state(v, dst_name) == \"Undecided\":\n            self._edge_states.mark_edge(v, dst_name, \"Rejected\")\n\n    if interactive:\n        GraphRenderer.display_graph(self._graph, self._prepared_variables)\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        GraphRenderer.draw_graph(self._graph, self._prepared_variables)\n        if not interactive\n        else \"\",\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.reject_all_prunable_edges","title":"<code>reject_all_prunable_edges(lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER)</code>","text":"<p>For every prepared variable, reject all incoming edges that start at a variable that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.</p> <p>Returns:</p> Type Description <code>Tuple[float, Optional[str], Optional[str]]</code> <p>A tuple containing: (1) the exploration score after the edge rejections, (2) the max-impact variable to explore next, if any, (3) optionally a string representation of the graph, if <code>interactive</code> is False.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def reject_all_prunable_edges(\n    self,\n    lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER,\n) -&gt; Tuple[float, Optional[str], Optional[str]]:\n    \"\"\"\n    For every prepared variable, reject all incoming edges that start at a variable\n    that is pruned by our pruning approach. This may be time-consuming depending on the number of variables.\n\n    Returns:\n        A tuple containing:\n            (1) the exploration score after the edge rejections,\n            (2) the max-impact variable to explore next, if any,\n            (3) optionally a string representation of the graph, if `interactive` is False.\n    \"\"\"\n    num_processors = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=num_processors) as pool:\n        all_candidates = pool.starmap(\n            Pruner.prune_with_lasso,\n            tqdm(\n                [\n                    (self._prepared_log, [target], lasso_alpha, lasso_max_iter)\n                    for target in self.prepared_variable_names\n                ],\n                total=self.num_prepared_variables,\n                desc=\"Finding pruned variables...\",\n            ),\n        )\n\n    Printer.printv(all_candidates)\n\n    for candidates, target in zip(all_candidates, self.prepared_variable_names):\n        non_candidates = (\n            set(self._prepared_log.columns) - set(candidates) - set([target])\n        )\n        for nc in non_candidates:\n            self._edge_states.mark_edge(nc, target, \"Rejected\")\n\n    return (\n        self.exploration_score,\n        self.suggest_next_exploration(),\n        GraphRenderer.draw_graph(self._graph, self._prepared_variables),\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.explore_candidate_causes","title":"<code>explore_candidate_causes(target=None, ignore=None, lasso_alpha=Pruner.LASSO_DEFAULT_ALPHA, lasso_max_iter=Pruner.LASSO_DEFAULT_MAX_ITER)</code>","text":"<p>Present the user with candidate causal graph neighbors for <code>target</code>. If no <code>target</code> is specified, the most recent sugestion of <code>suggest_next_exploration()</code> is used, if any. If <code>ignore</code> is specified, the variables in <code>ignore</code> are not considered as candidate causes.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>The name or tag of the target variable.</p> <code>None</code> <code>ignore</code> <code>Optional[list[str]]</code> <p>A list of variables to ignore.</p> <code>None</code> <code>lasso_alpha</code> <code>float</code> <p>The alpha parameter to be used for Lasso regression.</p> <code>LASSO_DEFAULT_ALPHA</code> <code>lasso_max_iter</code> <code>int</code> <p>The maximum number of iterations to be used for Lasso regression.</p> <code>LASSO_DEFAULT_MAX_ITER</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the candidate causal graph neighbors for <code>target</code>.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def explore_candidate_causes(\n    self,\n    target: Optional[str] = None,\n    ignore: Optional[list[str]] = None,\n    lasso_alpha: float = Pruner.LASSO_DEFAULT_ALPHA,\n    lasso_max_iter: int = Pruner.LASSO_DEFAULT_MAX_ITER,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Present the user with candidate causal graph neighbors for `target`. If no `target`\n    is specified, the most recent sugestion of `suggest_next_exploration()` is used, if any.\n    If `ignore` is specified, the variables in `ignore` are not considered as candidate causes.\n\n    Parameters:\n        target: The name or tag of the target variable.\n        ignore: A list of variables to ignore.\n        lasso_alpha: The alpha parameter to be used for Lasso regression.\n        lasso_max_iter: The maximum number of iterations to be used for Lasso regression.\n\n    Returns:\n        A dataframe containing the candidate causal graph neighbors for `target`.\n    \"\"\"\n\n    column_order = [\n        \"Candidate\",\n        \"Candidate Tag\",\n        \"Target\",\n        \"Slope\",\n        \"P-value\",\n        \"Candidate-&gt;Target Edge Status\",\n        \"Target-&gt;Candidate Edge Status\",\n    ]\n\n    # Handle the case where the user has not specified a target.\n    if target is None and self._next_exploration is None:\n        print(\"No target specified.\")\n        return pd.DataFrame(columns=column_order)\n    elif target is None:\n        target = self._next_exploration\n\n    # If the user provided the target as a tag, retrieve its name\n    target = TagUtils.name_of(self._prepared_variables, target, \"prepared\")\n\n    # Use Lasso to get a pruned list of neighbors\n    candidates = Pruner.prune_with_lasso(\n        self._prepared_log,\n        [target],\n        ignore=ignore,\n        alpha=lasso_alpha,\n        max_iter=lasso_max_iter,\n    )\n    Printer.printv(f\"Candidates: {candidates}\")\n\n    # Stop if there are no candidates\n\n    if len(candidates) == 0:\n        print(\"No candidates found.\")\n        return pd.DataFrame(columns=column_order)\n\n    # Mark the edges rejected by the pruning step\n    non_candidates = (\n        set(self._prepared_log.columns) - set(candidates) - set([target])\n    )\n    for var in non_candidates:\n        self._edge_states.mark_edge(var, target, \"Rejected\")\n\n    # For each candidate, calculate the slope and p-value of a linear regression with target (in parallel)\n    num_processors = multiprocessing.cpu_count()\n    X_columns = [c for c in self._prepared_log.columns if c in candidates]\n    with multiprocessing.Pool(processes=num_processors) as pool:\n        results = pool.starmap(\n            Regression.ols,\n            [\n                (col, self._prepared_log[col], self.prepared_log[target])\n                for col in X_columns\n            ],\n        )\n\n    # Return the results in a dataframe\n    result_df = pd.DataFrame(results)\n\n    result_df[\"Target\"] = target\n\n    result_df[\"Candidate-&gt;Target Edge Status\"] = result_df[\"Candidate\"].apply(\n        lambda x: self._edge_states.get_edge_state(x, target)\n    )\n    result_df[\"Target-&gt;Candidate Edge Status\"] = result_df[\"Candidate\"].apply(\n        lambda x: self._edge_states.get_edge_state(target, x)\n    )\n    result_df[\"Candidate Tag\"] = result_df[\"Candidate\"].apply(\n        lambda x: self.get_tag_of_prepared(x)\n    )\n\n    return (\n        result_df[column_order]\n        .sort_values(by=\"P-value\", ascending=True)\n        .reset_index(drop=True)\n    )\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.suggest_next_exploration","title":"<code>suggest_next_exploration()</code>","text":"<p>Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.</p> <p>If all edges are decided, return None.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The name of the variable to explore next.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def suggest_next_exploration(self) -&gt; Optional[str]:\n    \"\"\"\n    Suggest the variable that should be explored next. Suggest the prepared variable in the partial causal graph\n    that has the most (nonzero) Unexplored incoming edges, if any; otherwise suggest the prepared variable\n    with the most (nonzero) Undecided incoming edges, even if it is not in the partial causal graph.\n\n    If all edges are decided, return None.\n\n    Returns:\n        The name of the variable to explore next.\n    \"\"\"\n\n    # Try to find a suggestion from the partial causal graph.\n    node_names = list(self._graph.nodes)\n    graph_var_indices = [self._edge_states.idx(x) for x in node_names]\n    graph_var_incoming_edge_states = self._edge_states.m[:, graph_var_indices]\n    undecided_edges_per_col = (\n        np.sum(graph_var_incoming_edge_states == 0, axis=0)\n        if len(graph_var_incoming_edge_states) &gt; 0\n        else []\n    )\n    max_undecided = (\n        np.max(undecided_edges_per_col) if len(undecided_edges_per_col) &gt; 0 else 0\n    )\n\n    if max_undecided &gt; 0:\n        max_undecided_idx = np.argmax(undecided_edges_per_col)\n        self._next_exploration = node_names[max_undecided_idx]\n        return self._next_exploration\n\n    # If no suggestion was found, try to find a suggestion from the entire collection of prepared variables.\n    undecided_edges_per_col = np.sum(self._edge_states.m == 0, axis=0)\n    max_undecided = np.max(undecided_edges_per_col)\n\n    if max_undecided &gt; 0:\n        max_undecided_idx = np.argmax(undecided_edges_per_col)\n        self._next_exploration = self._prepared_variables.loc[\n            max_undecided_idx, \"Name\"\n        ]\n        return self._next_exploration\n\n    # If no suggestion was found, return None.\n    self._next_exploration = None\n    return None\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.discover_graph","title":"<code>discover_graph(method='hill_climb', max_cond_vars=3, model='gpt-3.5-turbo')</code>","text":"<p>Discover a causal graph based on the prepared table automatically.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".</p> <code>'hill_climb'</code> <code>max_cond_vars</code> <code>int</code> <p>The maximum number of conditioning variables to be used for PC.</p> <code>3</code> <code>model</code> <code>str</code> <p>The model to be used for GPT-based graph discovery.</p> <code>'gpt-3.5-turbo'</code> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def discover_graph(\n    self,\n    method: str = \"hill_climb\",\n    max_cond_vars: int = 3,\n    model: str = \"gpt-3.5-turbo\",\n) -&gt; None:\n    \"\"\"\n    Discover a causal graph based on the prepared table automatically.\n\n    Parameters:\n        method: The method to be used for graph discovery, among \"PC\", \"hill_climb\", \"exhaustive\" and \"GPT\".\n        max_cond_vars: The maximum number of conditioning variables to be used for PC.\n        model: The model to be used for GPT-based graph discovery.\n\n    \"\"\"\n\n    if method == \"PC\":\n        self._graph = CausalDiscoverer.pc(\n            self._prepared_log, max_cond_vars=max_cond_vars\n        )\n    elif method == \"hill_climb\":\n        self._graph = CausalDiscoverer.hill_climb(self._prepared_log)\n    elif method == \"exhaustive\":\n        self._graph = CausalDiscoverer.exhaustive(self._prepared_log)\n    elif method == \"GPT\":\n        self._graph = CausalDiscoverer.gpt(self._prepared_log, model=model)\n    else:\n        raise ValueError(f\"Invalid graph discovery method {method}\")\n\n    self._edge_states.clear_and_set_from_graph(self._graph)\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.get_adjusted_ate","title":"<code>get_adjusted_ate(treatment, outcome, confounder=None)</code>","text":"<p>Calculate the adjusted ATE of <code>treatment</code> on <code>outcome</code>, given the current partial causal graph.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>confounder</code> <code>Optional[str]</code> <p>The name or tag of a confounder variable. If specified, overrides the current partial causal graph in favor of a three-node graph with <code>treatment</code>, <code>outcome</code> and <code>confounder</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The adjusted ATE of <code>treatment</code> on <code>outcome</code>.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def get_adjusted_ate(\n    self,\n    treatment: str,\n    outcome: str,\n    confounder: Optional[str] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate the adjusted ATE of `treatment` on `outcome`, given the current partial causal graph.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        confounder: The name or tag of a confounder variable. If specified, overrides the current partial\n            causal graph in favor of a three-node graph with `treatment`, `outcome` and `confounder`.\n\n    Returns:\n        The adjusted ATE of `treatment` on `outcome`.\n    \"\"\"\n    return ATECalculator.get_ate_and_confidence(\n        self.prepared_log,\n        self.prepared_variables,\n        treatment,\n        outcome,\n        confounder,\n        graph=self._graph,\n        calculate_p_value=False,\n        calculate_std_error=False,\n    )[\"ATE\"]\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.get_unadjusted_ate","title":"<code>get_unadjusted_ate(treatment, outcome)</code>","text":"<p>Calculate the unadjusted ATE of <code>treatment</code> on <code>outcome</code>, ignoring the current partial causal graph in favor of a two-node graph with just <code>treatment</code> and <code>outcome</code>.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The unadjusted ATE of <code>treatment</code> on <code>outcome</code>.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def get_unadjusted_ate(\n    self,\n    treatment: str,\n    outcome: str,\n) -&gt; float:\n    \"\"\"\n    Calculate the unadjusted ATE of `treatment` on `outcome`, ignoring the current partial causal graph\n    in favor of a two-node graph with just `treatment` and `outcome`.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n\n    Returns:\n        The unadjusted ATE of `treatment` on `outcome`.\n    \"\"\"\n    return ATECalculator.get_ate_and_confidence(\n        self.prepared_log,\n        self.prepared_variables,\n        treatment,\n        outcome,\n        calculate_p_value=False,\n        calculate_std_error=False,\n    )[\"ATE\"]\n</code></pre>"},{"location":"reference/src/sawmill/sawmill/#src.sawmill.sawmill.Sawmill.challenge_ate","title":"<code>challenge_ate(treatment, outcome, num_outputs=10, method='step', ignore_current_graph=False, cp=None)</code>","text":"<p>Identify a ranked list of up to <code>num_outputs</code> possible edges among variables in the prepared log which, if set to a different state (i.e. included, reversed or omitted) would most noticeably impact the ATE of <code>treatment</code> on <code>outcome</code>.</p> <p>Parameters:</p> Name Type Description Default <code>treatment</code> <code>str</code> <p>The name or tag of the treatment variable.</p> required <code>outcome</code> <code>str</code> <p>The name or tag of the outcome variable.</p> required <code>num_outputs</code> <code>int</code> <p>The number of edges to return.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to be used for edge ranking, among \"step\" and \"clustering\".</p> <code>'step'</code> <code>ignore_current_graph</code> <code>bool</code> <p>Whether to ignore the current partial causal graph for the \"clustering\" method.</p> <code>False</code> <code>cp</code> <code>Optional[ClusteringParams]</code> <p>The clustering parameters to be used for \"clustering\" method.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the ranked list of edges.</p> Source code in <code>src/sawmill/sawmill.py</code> <pre><code>def challenge_ate(\n    self,\n    treatment: str,\n    outcome: str,\n    num_outputs: int = 10,\n    method: str = \"step\",\n    ignore_current_graph: bool = False,\n    cp: Optional[ClusteringParams] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Identify a ranked list of up to `num_outputs` possible edges among variables in the prepared log\n    which, if set to a different state (i.e. included, reversed or omitted) would most noticeably\n    impact the ATE of `treatment` on `outcome`.\n\n    Parameters:\n        treatment: The name or tag of the treatment variable.\n        outcome: The name or tag of the outcome variable.\n        num_outputs: The number of edges to return.\n        method: The method to be used for edge ranking, among \"step\" and \"clustering\".\n        ignore_current_graph: Whether to ignore the current partial causal graph for the \"clustering\" method.\n        cp: The clustering parameters to be used for \"clustering\" method.\n\n    Returns:\n        A dataframe containing the ranked list of edges.\n    \"\"\"\n\n    return ATECalculator.challenge_ate(\n        self._prepared_log,\n        self._prepared_variables,\n        self._graph if not ignore_current_graph else None,\n        treatment,\n        outcome,\n        self._workdir,\n        num_outputs,\n        method,\n        cp,\n    )\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/","title":"TagUtils","text":""},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagOrigin","title":"<code>TagOrigin</code>","text":"<p>             Bases: <code>Enum</code></p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>class TagOrigin(Enum):\n    PRECEDING: str = 'preceding'\n    \"\"\"Indicates that the tag was derived from the preceding tokens in the corresponding template.\"\"\"\n\n    GPT_3POINT5_TURBO: str = 'gpt-3.5-turbo'\n    \"\"\"Indicates that the tag was derived using gpt-3.5-turbo.\"\"\"\n\n    GPT_4: str = 'gpt-4'\n    \"\"\"Indicates that the tag was derived using gpt-4.\"\"\"\n\n    NAME: str = 'name'\n    \"\"\"Indicates that the tag was derived from the name of the variable.\"\"\"\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagOrigin.PRECEDING","title":"<code>PRECEDING: str = 'preceding'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived from the preceding tokens in the corresponding template.</p>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagOrigin.GPT_3POINT5_TURBO","title":"<code>GPT_3POINT5_TURBO: str = 'gpt-3.5-turbo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived using gpt-3.5-turbo.</p>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagOrigin.GPT_4","title":"<code>GPT_4: str = 'gpt-4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived using gpt-4.</p>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagOrigin.NAME","title":"<code>NAME: str = 'name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that the tag was derived from the name of the variable.</p>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils","title":"<code>TagUtils</code>","text":"<p>A class for managing tags of parsed and prepared variables.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>class TagUtils:\n    \"\"\"\n    A class for managing tags of parsed and prepared variables.\n    \"\"\"\n\n    @staticmethod\n    def check_columns(df: pd.DataFrame, columns: list) -&gt; None:\n        \"\"\"\n        Check that the specified columns exist in the dataframe.\n\n        Parameters:\n            df: The dataframe to be checked.\n            columns: The columns to be checked.\n\n        Raises:\n            ValueError: If any of the columns are not present in the dataframe.\n        \"\"\"\n        if not set(columns).issubset(set(df.columns)):\n            raise ValueError(f\"Columns {columns} are not all present in the dataframe.\")\n\n    @staticmethod\n    def check_fields(series: pd.Series, fields: list) -&gt; None:\n        \"\"\"\n        Check that the specified fields exist in the specified series.\n\n        Parameters:\n            series: The series to be checked.\n            fields: The fields to be checked.\n\n        Raises:\n            ValueError: If any of the fields are not present in the series.\n        \"\"\"\n        if not set(fields).issubset(set(series.index)):\n            raise ValueError(f\"Fields {fields} are not all present in the series.\")\n\n    @staticmethod\n    def best_effort_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n        enable_gpt_tagging: bool,\n        gpt_model: str,\n    ) -&gt; tuple[str, bool]:\n        \"\"\"\n        Apply `gpt_tag` to `variable_row`, if possible, and return the result. If there is\n        no environment variable called OPENAI_API_KEY, or if `enable_gpt_tagging` is False,\n        apply `preceding_tokens_tag` instead.\n\n        Parameters:\n            templates_df: The dataframe containing information about the log templates.\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n            gpt_model: The GPT model to use.\n\n        Returns:\n            A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the\n            best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.\n        \"\"\"\n        if enable_gpt_tagging:\n            try:\n                return (TagUtils.gpt_tag(templates_df, variable_row, gpt_model), True)\n            except:\n                return (TagUtils.preceding_tokens_tag(variable_row), False)\n        else:\n            return (TagUtils.preceding_tokens_tag(variable_row), False)\n\n    @staticmethod\n    def waterfall_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n    ) -&gt; tuple[str, TagOrigin]:\n        return\n\n\n    @staticmethod\n    def preceding_tokens_tag(variable_row: pd.Series) -&gt; str:\n        \"\"\"\n        Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.\n\n        Parameters:\n            variable_row: The row of the dataframe containing information about the parsed variable.\n\n        Returns:\n            The best-effort tag for the parsed variable name.\n        \"\"\"\n\n        TagUtils.check_fields(variable_row, [\"Preceding 3 tokens\", \"Name\"])\n        pr = variable_row[\"Preceding 3 tokens\"]\n        name = variable_row[\"Name\"]\n        if len(pr) &gt;= 2 and (pr[-1] in \":=\") and (pr[-2][0] != \"&lt;\"):\n            return pr[-2]\n        elif (\n            len(pr) == 3\n            and (pr[2] in \"\"\"\"'\"\"\")\n            and (pr[1] in \":=\")\n            and (pr[0][0] != \"&lt;\")\n        ):\n            return pr[0]\n        else:\n            return name\n\n    @staticmethod\n    def gpt_tag(\n        templates_df: pd.DataFrame,\n        variable_row: pd.Series,\n        model: str = \"gpt-3.5-turbo\",\n    ) -&gt; str:\n        \"\"\"\n        Use GPT to derive a tag the variable described in `variable_row`,\n        using information about the corresponding log template, retrieved from `templates_df`.\n\n        Parameters:\n            templates_df: The dataframe containing information about the log templates.\n            variable_row: The row of the dataframe containing information about the parsed variable.\n            model: The GPT model to use.\n\n        Returns:\n            The GPT-generated tag for the parsed variable name.\n        \"\"\"\n\n        TagUtils.check_fields(variable_row, [\"Name\", \"Examples\"])\n        TagUtils.check_columns(templates_df, [\"TemplateId\", \"TemplateExample\"])\n\n        template_id = ParsedVariableName(variable_row[\"Name\"]).template_id()\n        idx = ParsedVariableName(variable_row[\"Name\"]).index()\n\n        line = templates_df[templates_df[\"TemplateId\"] == template_id][\n            \"TemplateExample\"\n        ].values[0]\n        line_toks = line.split()\n\n        # Define the messages to send to the model\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a backend engineer that knows all about the logging infrastructure of a distributed system.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate a tag for the variable that takes the value {line_toks[idx]} \"\"\"\n                f\"\"\"in the following log line:\\n {line}\\n\"\"\"\n                f\"\"\"Here are the 3 tokens that precede the variable: [{', '.join(line_toks[max(idx-3, 0):idx])} ]\\n\"\"\"\n                f\"\"\"Here are some more example values for this variable: [{', '.join(variable_row['Examples'])} ]\\n\"\"\"\n                \"\"\"Return only the tag as a single word or underscore-separated string. Keep it short and informative.\\n\"\"\",\n            },\n        ]\n\n        client = OpenAI()\n\n        tag = (\n            client.chat.completions.create(model=model, messages=messages)\n            .choices[0]\n            .message.content\n        )\n\n        return tag\n\n    @staticmethod\n    def deduplicate_tags(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Ensure that the tags in df are unique, by making the tag column of any row\n        with a seen-before tag equal to the name column of that row.\n\n        Parameters:\n            df: The dataframe to be deduplicated.\n\n        Returns:\n            The deduplicated dataframe.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        seen_tags = set()\n        for i, row in df.iterrows():\n            if row[\"Tag\"] in seen_tags:\n                df.loc[i, \"Tag\"] = row[\"Name\"]\n            else:\n                seen_tags.add(row[\"Tag\"])\n\n    @staticmethod\n    def set_tag(df: pd.DataFrame, name: str, tag: str, info: str = \"\") -&gt; None:\n        \"\"\"\n        Tag a parsed or prepared variable for easier access.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name: The name of the parsed or prepared variable.\n            tag: The tag to be set.\n            info: A string describing the type of variable being tagged (parsed or prepared).\n\n        Raises:\n            ValueError: If the name is not the name of a parsed or prepared variable.\n        \"\"\"\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        if name in df[\"Name\"].values:\n            df.loc[df[\"Name\"] == name, \"Tag\"] = tag\n            print(f\"Variable {name} tagged as {tag}\")\n        else:\n            raise ValueError(f\"{name} is not the name of a {info} variable.\")\n\n    @staticmethod\n    def get_tag(df: pd.DataFrame, name: str, info: str = \"\") -&gt; str:\n        \"\"\"\n        Retrieve the tag of a parsed or prepared variable.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name: The name of the parsed or prepared variable.\n            info: A string describing the type of variable being tagged (parsed or prepared).\n\n        Raises:\n            ValueError: If the name is not the name of a parsed or prepared variable.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        if name in df[\"Name\"].values:\n            return df.loc[df[\"Name\"] == name, \"Tag\"].values[0]\n        else:\n            raise ValueError(f\"{name} is not the name of a {info} variable.\")\n\n    @staticmethod\n    def name_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n        \"\"\"\n        Determine the name of a parsed or prepared variable, given either itself or its tag.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name_or_tag: The name or tag of the parsed or prepared variable.\n            info: A string describing the type of variable in question (parsed or prepared).\n\n        Returns:\n            The name of the parsed or prepared variable.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        name_or_tag = name_or_tag.strip()\n        if name_or_tag in df[\"Name\"].values:\n            return name_or_tag\n        elif name_or_tag in df[\"Tag\"].values:\n            return df.loc[df[\"Tag\"] == name_or_tag, \"Name\"].values[0]\n        else:\n            raise ValueError(\n                f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n            )\n\n    @staticmethod\n    def tag_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n        \"\"\"\n        Determine the tag of a parsed or prepared variable, given either itself or its name.\n\n        Parameters:\n            df: The dataframe containing the parsed or prepared variables.\n            name_or_tag: The name or tag of the parsed or prepared variable.\n            info: A string describing the type of variable in question (parsed or prepared).\n\n        Returns:\n            The tag of the parsed or prepared variable.\n        \"\"\"\n\n        TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n        name_or_tag = name_or_tag.strip()\n        if name_or_tag in df[\"Tag\"].values:\n            return name_or_tag\n        elif name_or_tag in df[\"Name\"].values:\n            return df.loc[df[\"Name\"] == name_or_tag, \"Tag\"].values[0]\n        else:\n            raise ValueError(\n                f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n            )\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.check_columns","title":"<code>check_columns(df, columns)</code>  <code>staticmethod</code>","text":"<p>Check that the specified columns exist in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be checked.</p> required <code>columns</code> <code>list</code> <p>The columns to be checked.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the columns are not present in the dataframe.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef check_columns(df: pd.DataFrame, columns: list) -&gt; None:\n    \"\"\"\n    Check that the specified columns exist in the dataframe.\n\n    Parameters:\n        df: The dataframe to be checked.\n        columns: The columns to be checked.\n\n    Raises:\n        ValueError: If any of the columns are not present in the dataframe.\n    \"\"\"\n    if not set(columns).issubset(set(df.columns)):\n        raise ValueError(f\"Columns {columns} are not all present in the dataframe.\")\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.check_fields","title":"<code>check_fields(series, fields)</code>  <code>staticmethod</code>","text":"<p>Check that the specified fields exist in the specified series.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>The series to be checked.</p> required <code>fields</code> <code>list</code> <p>The fields to be checked.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the fields are not present in the series.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef check_fields(series: pd.Series, fields: list) -&gt; None:\n    \"\"\"\n    Check that the specified fields exist in the specified series.\n\n    Parameters:\n        series: The series to be checked.\n        fields: The fields to be checked.\n\n    Raises:\n        ValueError: If any of the fields are not present in the series.\n    \"\"\"\n    if not set(fields).issubset(set(series.index)):\n        raise ValueError(f\"Fields {fields} are not all present in the series.\")\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.best_effort_tag","title":"<code>best_effort_tag(templates_df, variable_row, enable_gpt_tagging, gpt_model)</code>  <code>staticmethod</code>","text":"<p>Apply <code>gpt_tag</code> to <code>variable_row</code>, if possible, and return the result. If there is no environment variable called OPENAI_API_KEY, or if <code>enable_gpt_tagging</code> is False, apply <code>preceding_tokens_tag</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>templates_df</code> <code>DataFrame</code> <p>The dataframe containing information about the log templates.</p> required <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>enable_gpt_tagging</code> <code>bool</code> <p>A boolean indicating whether GPT-3.5 tagging should be enabled.</p> required <code>gpt_model</code> <code>str</code> <p>The GPT model to use.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the</p> <code>bool</code> <p>best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef best_effort_tag(\n    templates_df: pd.DataFrame,\n    variable_row: pd.Series,\n    enable_gpt_tagging: bool,\n    gpt_model: str,\n) -&gt; tuple[str, bool]:\n    \"\"\"\n    Apply `gpt_tag` to `variable_row`, if possible, and return the result. If there is\n    no environment variable called OPENAI_API_KEY, or if `enable_gpt_tagging` is False,\n    apply `preceding_tokens_tag` instead.\n\n    Parameters:\n        templates_df: The dataframe containing information about the log templates.\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        enable_gpt_tagging: A boolean indicating whether GPT-3.5 tagging should be enabled.\n        gpt_model: The GPT model to use.\n\n    Returns:\n        A tuple containing (i) the GPT-3.5 tag for the parsed variable name, if possible, or the\n        best-effort tag otherwise, and (ii) a boolean indicating whether the GPT-3.5 tag was used.\n    \"\"\"\n    if enable_gpt_tagging:\n        try:\n            return (TagUtils.gpt_tag(templates_df, variable_row, gpt_model), True)\n        except:\n            return (TagUtils.preceding_tokens_tag(variable_row), False)\n    else:\n        return (TagUtils.preceding_tokens_tag(variable_row), False)\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.preceding_tokens_tag","title":"<code>preceding_tokens_tag(variable_row)</code>  <code>staticmethod</code>","text":"<p>Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.</p> <p>Parameters:</p> Name Type Description Default <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The best-effort tag for the parsed variable name.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef preceding_tokens_tag(variable_row: pd.Series) -&gt; str:\n    \"\"\"\n    Try to derive a tag for a parsed variable name based on the preceding tokens in the corresponding template.\n\n    Parameters:\n        variable_row: The row of the dataframe containing information about the parsed variable.\n\n    Returns:\n        The best-effort tag for the parsed variable name.\n    \"\"\"\n\n    TagUtils.check_fields(variable_row, [\"Preceding 3 tokens\", \"Name\"])\n    pr = variable_row[\"Preceding 3 tokens\"]\n    name = variable_row[\"Name\"]\n    if len(pr) &gt;= 2 and (pr[-1] in \":=\") and (pr[-2][0] != \"&lt;\"):\n        return pr[-2]\n    elif (\n        len(pr) == 3\n        and (pr[2] in \"\"\"\"'\"\"\")\n        and (pr[1] in \":=\")\n        and (pr[0][0] != \"&lt;\")\n    ):\n        return pr[0]\n    else:\n        return name\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.gpt_tag","title":"<code>gpt_tag(templates_df, variable_row, model='gpt-3.5-turbo')</code>  <code>staticmethod</code>","text":"<p>Use GPT to derive a tag the variable described in <code>variable_row</code>, using information about the corresponding log template, retrieved from <code>templates_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>templates_df</code> <code>DataFrame</code> <p>The dataframe containing information about the log templates.</p> required <code>variable_row</code> <code>Series</code> <p>The row of the dataframe containing information about the parsed variable.</p> required <code>model</code> <code>str</code> <p>The GPT model to use.</p> <code>'gpt-3.5-turbo'</code> <p>Returns:</p> Type Description <code>str</code> <p>The GPT-generated tag for the parsed variable name.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef gpt_tag(\n    templates_df: pd.DataFrame,\n    variable_row: pd.Series,\n    model: str = \"gpt-3.5-turbo\",\n) -&gt; str:\n    \"\"\"\n    Use GPT to derive a tag the variable described in `variable_row`,\n    using information about the corresponding log template, retrieved from `templates_df`.\n\n    Parameters:\n        templates_df: The dataframe containing information about the log templates.\n        variable_row: The row of the dataframe containing information about the parsed variable.\n        model: The GPT model to use.\n\n    Returns:\n        The GPT-generated tag for the parsed variable name.\n    \"\"\"\n\n    TagUtils.check_fields(variable_row, [\"Name\", \"Examples\"])\n    TagUtils.check_columns(templates_df, [\"TemplateId\", \"TemplateExample\"])\n\n    template_id = ParsedVariableName(variable_row[\"Name\"]).template_id()\n    idx = ParsedVariableName(variable_row[\"Name\"]).index()\n\n    line = templates_df[templates_df[\"TemplateId\"] == template_id][\n        \"TemplateExample\"\n    ].values[0]\n    line_toks = line.split()\n\n    # Define the messages to send to the model\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a backend engineer that knows all about the logging infrastructure of a distributed system.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate a tag for the variable that takes the value {line_toks[idx]} \"\"\"\n            f\"\"\"in the following log line:\\n {line}\\n\"\"\"\n            f\"\"\"Here are the 3 tokens that precede the variable: [{', '.join(line_toks[max(idx-3, 0):idx])} ]\\n\"\"\"\n            f\"\"\"Here are some more example values for this variable: [{', '.join(variable_row['Examples'])} ]\\n\"\"\"\n            \"\"\"Return only the tag as a single word or underscore-separated string. Keep it short and informative.\\n\"\"\",\n        },\n    ]\n\n    client = OpenAI()\n\n    tag = (\n        client.chat.completions.create(model=model, messages=messages)\n        .choices[0]\n        .message.content\n    )\n\n    return tag\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.deduplicate_tags","title":"<code>deduplicate_tags(df)</code>  <code>staticmethod</code>","text":"<p>Ensure that the tags in df are unique, by making the tag column of any row with a seen-before tag equal to the name column of that row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to be deduplicated.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The deduplicated dataframe.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef deduplicate_tags(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Ensure that the tags in df are unique, by making the tag column of any row\n    with a seen-before tag equal to the name column of that row.\n\n    Parameters:\n        df: The dataframe to be deduplicated.\n\n    Returns:\n        The deduplicated dataframe.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    seen_tags = set()\n    for i, row in df.iterrows():\n        if row[\"Tag\"] in seen_tags:\n            df.loc[i, \"Tag\"] = row[\"Name\"]\n        else:\n            seen_tags.add(row[\"Tag\"])\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.set_tag","title":"<code>set_tag(df, name, tag, info='')</code>  <code>staticmethod</code>","text":"<p>Tag a parsed or prepared variable for easier access.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name</code> <code>str</code> <p>The name of the parsed or prepared variable.</p> required <code>tag</code> <code>str</code> <p>The tag to be set.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable being tagged (parsed or prepared).</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is not the name of a parsed or prepared variable.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef set_tag(df: pd.DataFrame, name: str, tag: str, info: str = \"\") -&gt; None:\n    \"\"\"\n    Tag a parsed or prepared variable for easier access.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name: The name of the parsed or prepared variable.\n        tag: The tag to be set.\n        info: A string describing the type of variable being tagged (parsed or prepared).\n\n    Raises:\n        ValueError: If the name is not the name of a parsed or prepared variable.\n    \"\"\"\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    if name in df[\"Name\"].values:\n        df.loc[df[\"Name\"] == name, \"Tag\"] = tag\n        print(f\"Variable {name} tagged as {tag}\")\n    else:\n        raise ValueError(f\"{name} is not the name of a {info} variable.\")\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.get_tag","title":"<code>get_tag(df, name, info='')</code>  <code>staticmethod</code>","text":"<p>Retrieve the tag of a parsed or prepared variable.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name</code> <code>str</code> <p>The name of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable being tagged (parsed or prepared).</p> <code>''</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is not the name of a parsed or prepared variable.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef get_tag(df: pd.DataFrame, name: str, info: str = \"\") -&gt; str:\n    \"\"\"\n    Retrieve the tag of a parsed or prepared variable.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name: The name of the parsed or prepared variable.\n        info: A string describing the type of variable being tagged (parsed or prepared).\n\n    Raises:\n        ValueError: If the name is not the name of a parsed or prepared variable.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    if name in df[\"Name\"].values:\n        return df.loc[df[\"Name\"] == name, \"Tag\"].values[0]\n    else:\n        raise ValueError(f\"{name} is not the name of a {info} variable.\")\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.name_of","title":"<code>name_of(df, name_or_tag, info='')</code>  <code>staticmethod</code>","text":"<p>Determine the name of a parsed or prepared variable, given either itself or its tag.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name_or_tag</code> <code>str</code> <p>The name or tag of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable in question (parsed or prepared).</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The name of the parsed or prepared variable.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef name_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n    \"\"\"\n    Determine the name of a parsed or prepared variable, given either itself or its tag.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name_or_tag: The name or tag of the parsed or prepared variable.\n        info: A string describing the type of variable in question (parsed or prepared).\n\n    Returns:\n        The name of the parsed or prepared variable.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    name_or_tag = name_or_tag.strip()\n    if name_or_tag in df[\"Name\"].values:\n        return name_or_tag\n    elif name_or_tag in df[\"Tag\"].values:\n        return df.loc[df[\"Tag\"] == name_or_tag, \"Name\"].values[0]\n    else:\n        raise ValueError(\n            f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n        )\n</code></pre>"},{"location":"reference/src/sawmill/tag_utils/#src.sawmill.tag_utils.TagUtils.tag_of","title":"<code>tag_of(df, name_or_tag, info='')</code>  <code>staticmethod</code>","text":"<p>Determine the tag of a parsed or prepared variable, given either itself or its name.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the parsed or prepared variables.</p> required <code>name_or_tag</code> <code>str</code> <p>The name or tag of the parsed or prepared variable.</p> required <code>info</code> <code>str</code> <p>A string describing the type of variable in question (parsed or prepared).</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The tag of the parsed or prepared variable.</p> Source code in <code>src/sawmill/tag_utils.py</code> <pre><code>@staticmethod\ndef tag_of(df: pd.DataFrame, name_or_tag: str, info: str = \"\") -&gt; str:\n    \"\"\"\n    Determine the tag of a parsed or prepared variable, given either itself or its name.\n\n    Parameters:\n        df: The dataframe containing the parsed or prepared variables.\n        name_or_tag: The name or tag of the parsed or prepared variable.\n        info: A string describing the type of variable in question (parsed or prepared).\n\n    Returns:\n        The tag of the parsed or prepared variable.\n    \"\"\"\n\n    TagUtils.check_columns(df, [\"Name\", \"Tag\"])\n    name_or_tag = name_or_tag.strip()\n    if name_or_tag in df[\"Tag\"].values:\n        return name_or_tag\n    elif name_or_tag in df[\"Name\"].values:\n        return df.loc[df[\"Name\"] == name_or_tag, \"Tag\"].values[0]\n    else:\n        raise ValueError(\n            f\"{name_or_tag} is not the name or tag of a {info} variable.\"\n        )\n</code></pre>"},{"location":"reference/src/sawmill/types/","title":"Types","text":""},{"location":"reference/src/sawmill/types/#src.sawmill.types.Types","title":"<code>Types</code>","text":"Source code in <code>src/sawmill/types.py</code> <pre><code>class Types:\n    Edge = tuple[str, str]\n    \"\"\"Type alias for a directed edge.\"\"\"\n\n    LeafLabelingFunction = Callable[[int], str]\n    \"\"\"Type alias for a leaf labeling function in `ATE`.\"\"\"\n\n    EdgeCountDict = defaultdict[Edge, int]\n    \"\"\"Type alias for a dictionary counting edge occurrences.\"\"\"\n</code></pre>"},{"location":"reference/src/sawmill/types/#src.sawmill.types.Types.Edge","title":"<code>Edge = tuple[str, str]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a directed edge.</p>"},{"location":"reference/src/sawmill/types/#src.sawmill.types.Types.LeafLabelingFunction","title":"<code>LeafLabelingFunction = Callable[[int], str]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a leaf labeling function in <code>ATE</code>.</p>"},{"location":"reference/src/sawmill/types/#src.sawmill.types.Types.EdgeCountDict","title":"<code>EdgeCountDict = defaultdict[Edge, int]</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type alias for a dictionary counting edge occurrences.</p>"},{"location":"reference/src/sawmill/aggimp/","title":"Index","text":""},{"location":"reference/src/sawmill/aggimp/agg_funcs/","title":"Aggregation Functions","text":""},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.mean","title":"<code>mean(x)</code>","text":"<p>Calculates the mean of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mean will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mean of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def mean(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mean of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mean will be calculated.\n\n    Returns:\n        The mean of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mean(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.min","title":"<code>min(x)</code>","text":"<p>Calculates the minimum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the minimum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The minimum of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def min(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the minimum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the minimum will be calculated.\n\n    Returns:\n        The minimum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.min(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.max","title":"<code>max(x)</code>","text":"<p>Calculates the maximum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the maximum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The maximum of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def max(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the maximum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the maximum will be calculated.\n\n    Returns:\n        The maximum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.max(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.median","title":"<code>median(x)</code>","text":"<p>Calculates the median of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the median will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The median of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def median(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the median of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the median will be calculated.\n\n    Returns:\n        The median of the series, or None if the series is all NA.\n    \"\"\"\n    return x.median(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.mode","title":"<code>mode(x)</code>","text":"<p>Calculates the mode of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the mode will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The mode of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def mode(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the mode of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the mode will be calculated.\n\n    Returns:\n        The mode of the series, or None if the series is all NA.\n    \"\"\"\n    return x.mode(dropna=True)[0] if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.std","title":"<code>std(x)</code>","text":"<p>Calculates the standard deviation of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the standard deviation will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The standard deviation of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def std(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the standard deviation of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the standard deviation will be calculated.\n\n    Returns:\n        The standard deviation of the series, or None if the series is all NA.\n    \"\"\"\n    return x.std(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.last","title":"<code>last(x)</code>","text":"<p>Returns the last non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the last non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The last non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def last(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the last non-NA value in a series.\n\n    Parameters:\n        x: The series for which the last non-NA value will be returned.\n\n    Returns:\n        The last non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().tail(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.first","title":"<code>first(x)</code>","text":"<p>Returns the first non-NA value in a series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the first non-NA value will be returned.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The first non-NA value of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def first(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Returns the first non-NA value in a series.\n\n    Parameters:\n        x: The series for which the first non-NA value will be returned.\n\n    Returns:\n        The first non-NA value of the series, or None if the series is all NA.\n    \"\"\"\n    return x.dropna().head(1) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/agg_funcs/#src.sawmill.aggimp.agg_funcs.sum","title":"<code>sum(x)</code>","text":"<p>Calculates the sum of a series, ignoring NA values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the sum will be calculated.</p> required <p>Returns:</p> Type Description <code>Optional[Series]</code> <p>The sum of the series, or None if the series is all NA.</p> Source code in <code>src/sawmill/aggimp/agg_funcs.py</code> <pre><code>def sum(x: pd.Series) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Calculates the sum of a series, ignoring NA values.\n\n    Parameters:\n        x: The series for which the sum will be calculated.\n\n    Returns:\n        The sum of the series, or None if the series is all NA.\n    \"\"\"\n    return x.sum(skipna=True) if x.isna().sum() &lt; len(x) else None\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/imp_funcs/","title":"Imputation Functions","text":""},{"location":"reference/src/sawmill/aggimp/imp_funcs/#src.sawmill.aggimp.imp_funcs.ffill_imp","title":"<code>ffill_imp(x)</code>","text":"<p>Impute the NA values in a series by forward-filling and return the series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the NA values will be imputed.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series, with NA values imputed.</p> Source code in <code>src/sawmill/aggimp/imp_funcs.py</code> <pre><code>def ffill_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Impute the NA values in a series by forward-filling and return the series.\n\n    Parameters:\n        x: The series for which the NA values will be imputed.\n\n    Returns:\n        The series, with NA values imputed.\n    \"\"\"\n    return x.ffill()\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/imp_funcs/#src.sawmill.aggimp.imp_funcs.zero_imp","title":"<code>zero_imp(x)</code>","text":"<p>Impute the NA values in a series with zeroes and return the series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series for which the NA values will be imputed.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series, with NA values imputed.</p> Source code in <code>src/sawmill/aggimp/imp_funcs.py</code> <pre><code>def zero_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Impute the NA values in a series with zeroes and return the series.\n\n    Parameters:\n        x: The series for which the NA values will be imputed.\n\n    Returns:\n        The series, with NA values imputed.\n    \"\"\"\n    return x.fillna(0)\n</code></pre>"},{"location":"reference/src/sawmill/aggimp/imp_funcs/#src.sawmill.aggimp.imp_funcs.no_imp","title":"<code>no_imp(x)</code>","text":"<p>No-op.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The series to be returned.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The series passed as a parameter.</p> Source code in <code>src/sawmill/aggimp/imp_funcs.py</code> <pre><code>def no_imp(x: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    No-op.\n\n    Parameters:\n        x: The series to be returned.\n\n    Returns:\n        The series passed as a parameter.\n    \"\"\"\n    return x\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/","title":"Index","text":""},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/","title":"ParsedVariableName","text":""},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/#src.sawmill.variable_name.parsed_variable_name.ParsedVariableName","title":"<code>ParsedVariableName</code>","text":"<p>Performs operations on a atring interpreted as a parsed variable name.</p> <p>The relevant string format is {template_id}[_{index}].</p> Source code in <code>src/sawmill/variable_name/parsed_variable_name.py</code> <pre><code>class ParsedVariableName:\n    \"\"\"\n    Performs operations on a atring interpreted as a parsed variable name.\n\n    The relevant string format is {template_id}[_{index}].\n    \"\"\"\n    def __init__(self, s: str) -&gt; None:\n        \"\"\"\n        Initializes a ParsedVariableName object.\n\n        Parameters:\n            s: The string interpretation of the parsed variable name.\n        \"\"\"\n        toks = s.split(\"_\")\n        self._s = s\n        self._template_id = toks[0]\n        self._index = int(toks[1]) if len(toks) &gt; 1 else -1\n\n    def template_id(self) -&gt; str:\n        \"\"\"\n        Returns the template ID of the parsed variable name.\n\n        Returns:\n            The template ID of the parsed variable name.\n        \"\"\"\n        return self._template_id\n\n    def index(self) -&gt; Optional[int]:\n        \"\"\"\n        Returns the index of the parsed variable name.\n\n        Returns:\n            The index of the parsed variable name, or None if the index is not\n            present.\n        \"\"\"\n        return self._index if self._index != -1 else None\n\n    def str(self) -&gt; str:\n        \"\"\"\n        Returns the string representation of the parsed variable name.\n\n        Returns:\n            The string representation of the parsed variable name.\n        \"\"\"\n        return self._s\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/#src.sawmill.variable_name.parsed_variable_name.ParsedVariableName.__init__","title":"<code>__init__(s)</code>","text":"<p>Initializes a ParsedVariableName object.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string interpretation of the parsed variable name.</p> required Source code in <code>src/sawmill/variable_name/parsed_variable_name.py</code> <pre><code>def __init__(self, s: str) -&gt; None:\n    \"\"\"\n    Initializes a ParsedVariableName object.\n\n    Parameters:\n        s: The string interpretation of the parsed variable name.\n    \"\"\"\n    toks = s.split(\"_\")\n    self._s = s\n    self._template_id = toks[0]\n    self._index = int(toks[1]) if len(toks) &gt; 1 else -1\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/#src.sawmill.variable_name.parsed_variable_name.ParsedVariableName.template_id","title":"<code>template_id()</code>","text":"<p>Returns the template ID of the parsed variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The template ID of the parsed variable name.</p> Source code in <code>src/sawmill/variable_name/parsed_variable_name.py</code> <pre><code>def template_id(self) -&gt; str:\n    \"\"\"\n    Returns the template ID of the parsed variable name.\n\n    Returns:\n        The template ID of the parsed variable name.\n    \"\"\"\n    return self._template_id\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/#src.sawmill.variable_name.parsed_variable_name.ParsedVariableName.index","title":"<code>index()</code>","text":"<p>Returns the index of the parsed variable name.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The index of the parsed variable name, or None if the index is not</p> <code>Optional[int]</code> <p>present.</p> Source code in <code>src/sawmill/variable_name/parsed_variable_name.py</code> <pre><code>def index(self) -&gt; Optional[int]:\n    \"\"\"\n    Returns the index of the parsed variable name.\n\n    Returns:\n        The index of the parsed variable name, or None if the index is not\n        present.\n    \"\"\"\n    return self._index if self._index != -1 else None\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/parsed_variable_name/#src.sawmill.variable_name.parsed_variable_name.ParsedVariableName.str","title":"<code>str()</code>","text":"<p>Returns the string representation of the parsed variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the parsed variable name.</p> Source code in <code>src/sawmill/variable_name/parsed_variable_name.py</code> <pre><code>def str(self) -&gt; str:\n    \"\"\"\n    Returns the string representation of the parsed variable name.\n\n    Returns:\n        The string representation of the parsed variable name.\n    \"\"\"\n    return self._s\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/","title":"PreparedVariableName","text":""},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName","title":"<code>PreparedVariableName</code>","text":"<p>Performs operations on a string interpreted as a prepared variable name.</p> <p>The relevant string format is {template_id}[_{index}][={pre-agg value}]+{aggregate}[={post_agg value}].</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>class PreparedVariableName:\n    \"\"\"\n    Performs operations on a string interpreted as a prepared variable name.\n\n    The relevant string format is {template_id}[_{index}][={pre-agg value}]+{aggregate}[={post_agg value}].\n    \"\"\"\n\n    def __init__(self, s: str) -&gt; None:\n        \"\"\"\n        Initializes a PreparedVariableName object.\n\n        Parameters:\n            s: The string representation of the prepared variable name.\n        \"\"\"\n        mid_split = s.split(\"+\")\n\n        left_split = mid_split[0].split(\"=\")\n        right_split = mid_split[1].split(\"=\") if len(mid_split) &gt; 1 else [\"\", \"\"]\n\n        self._base_var = left_split[0]\n        self._pre_agg_value = left_split[1] if len(left_split) &gt; 1 else \"\"\n        self._aggregate = right_split[0]\n        self._post_agg_value = right_split[1] if len(right_split) &gt; 1 else \"\"\n\n    def base_var(self) -&gt; str:\n        \"\"\"\n        Returns the base variable of the prepared variable name.\n\n        Returns:\n            The base variable of the prepared variable name.\n        \"\"\"\n        return self._base_var\n\n    def template_id(self) -&gt; str:\n        \"\"\"\n        Returns the template ID of the prepared variable name. If the base variable\n        is 'TemplateId', then this will match the pre_agg_value.\n\n        Returns:\n            The template ID of the prepared variable name.\n        \"\"\"\n        if self._base_var == \"TemplateId\":\n            return self._pre_agg_value\n        else:\n            return ParsedVariableName(self._base_var).template_id()\n\n    def index(self) -&gt; Optional[int]:\n        \"\"\"\n        Returns the index of the prepared variable name.\n\n        Returns:\n            The index of the prepared variable name, or None if the index is not\n            present.\n        \"\"\"\n        return ParsedVariableName(self._base_var).index()\n\n    def pre_agg_value(self) -&gt; str:\n        \"\"\"\n        Returns the pre-aggregate value of the prepared variable name.\n\n        Returns:\n            The pre-aggregate value of the prepared variable name.\n        \"\"\"\n        return self._pre_agg_value\n\n    def aggregate(self) -&gt; str:\n        \"\"\"\n        Returns the aggregate of the prepared variable name.\n\n        Returns:\n            The aggregation function implied by the prepared variable name.\n        \"\"\"\n        return self._aggregate\n\n    def post_agg_value(self) -&gt; str:\n        \"\"\"\n        Returns the post-aggregate value of the prepared variable name.\n\n        Returns:\n            The post-aggregate value of the prepared variable name.\n        \"\"\"\n        return self._post_agg_value\n\n    def no_pre_post_aggs(self) -&gt; bool:\n        \"\"\"\n        Check whether the prepared variable has no pre- or post-aggregates.\n\n        Returns:\n            Whether the prepared variable has no pre- or post-aggregates.\n        \"\"\"\n        return self.pre_agg_value() == \"\" and self.post_agg_value() == \"\"\n\n    def has_base_var(self, x: str | Self) -&gt; bool:\n        \"\"\"\n        Check whether the prepared variable has the given base variable.\n\n        Parameters:\n            x: The base variable to check.\n\n        Returns:\n            Whether the prepared variable has the given base variable.\n        \"\"\"\n        return PreparedVariableName.same_base_var(self, x)\n\n    @staticmethod\n    def same_base_var(var1: str | Self, var2: str | Self) -&gt; bool:\n        \"\"\"\n        Check whether two prepared variables have the same base variable.\n\n        Parameters:\n            var1: The first variable to check.\n            var2: The second variable to check.\n\n        Returns:\n            Whether the two variables have the same base variable.\n        \"\"\"\n\n        if isinstance(var1, str):\n            var1 = PreparedVariableName(var1)\n        if isinstance(var2, str):\n            var2 = PreparedVariableName(var2)\n\n        return var1.base_var() == var2.base_var()\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.__init__","title":"<code>__init__(s)</code>","text":"<p>Initializes a PreparedVariableName object.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string representation of the prepared variable name.</p> required Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def __init__(self, s: str) -&gt; None:\n    \"\"\"\n    Initializes a PreparedVariableName object.\n\n    Parameters:\n        s: The string representation of the prepared variable name.\n    \"\"\"\n    mid_split = s.split(\"+\")\n\n    left_split = mid_split[0].split(\"=\")\n    right_split = mid_split[1].split(\"=\") if len(mid_split) &gt; 1 else [\"\", \"\"]\n\n    self._base_var = left_split[0]\n    self._pre_agg_value = left_split[1] if len(left_split) &gt; 1 else \"\"\n    self._aggregate = right_split[0]\n    self._post_agg_value = right_split[1] if len(right_split) &gt; 1 else \"\"\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.base_var","title":"<code>base_var()</code>","text":"<p>Returns the base variable of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The base variable of the prepared variable name.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def base_var(self) -&gt; str:\n    \"\"\"\n    Returns the base variable of the prepared variable name.\n\n    Returns:\n        The base variable of the prepared variable name.\n    \"\"\"\n    return self._base_var\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.template_id","title":"<code>template_id()</code>","text":"<p>Returns the template ID of the prepared variable name. If the base variable is 'TemplateId', then this will match the pre_agg_value.</p> <p>Returns:</p> Type Description <code>str</code> <p>The template ID of the prepared variable name.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def template_id(self) -&gt; str:\n    \"\"\"\n    Returns the template ID of the prepared variable name. If the base variable\n    is 'TemplateId', then this will match the pre_agg_value.\n\n    Returns:\n        The template ID of the prepared variable name.\n    \"\"\"\n    if self._base_var == \"TemplateId\":\n        return self._pre_agg_value\n    else:\n        return ParsedVariableName(self._base_var).template_id()\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.index","title":"<code>index()</code>","text":"<p>Returns the index of the prepared variable name.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The index of the prepared variable name, or None if the index is not</p> <code>Optional[int]</code> <p>present.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def index(self) -&gt; Optional[int]:\n    \"\"\"\n    Returns the index of the prepared variable name.\n\n    Returns:\n        The index of the prepared variable name, or None if the index is not\n        present.\n    \"\"\"\n    return ParsedVariableName(self._base_var).index()\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.pre_agg_value","title":"<code>pre_agg_value()</code>","text":"<p>Returns the pre-aggregate value of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The pre-aggregate value of the prepared variable name.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def pre_agg_value(self) -&gt; str:\n    \"\"\"\n    Returns the pre-aggregate value of the prepared variable name.\n\n    Returns:\n        The pre-aggregate value of the prepared variable name.\n    \"\"\"\n    return self._pre_agg_value\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.aggregate","title":"<code>aggregate()</code>","text":"<p>Returns the aggregate of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The aggregation function implied by the prepared variable name.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def aggregate(self) -&gt; str:\n    \"\"\"\n    Returns the aggregate of the prepared variable name.\n\n    Returns:\n        The aggregation function implied by the prepared variable name.\n    \"\"\"\n    return self._aggregate\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.post_agg_value","title":"<code>post_agg_value()</code>","text":"<p>Returns the post-aggregate value of the prepared variable name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The post-aggregate value of the prepared variable name.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def post_agg_value(self) -&gt; str:\n    \"\"\"\n    Returns the post-aggregate value of the prepared variable name.\n\n    Returns:\n        The post-aggregate value of the prepared variable name.\n    \"\"\"\n    return self._post_agg_value\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.no_pre_post_aggs","title":"<code>no_pre_post_aggs()</code>","text":"<p>Check whether the prepared variable has no pre- or post-aggregates.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the prepared variable has no pre- or post-aggregates.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def no_pre_post_aggs(self) -&gt; bool:\n    \"\"\"\n    Check whether the prepared variable has no pre- or post-aggregates.\n\n    Returns:\n        Whether the prepared variable has no pre- or post-aggregates.\n    \"\"\"\n    return self.pre_agg_value() == \"\" and self.post_agg_value() == \"\"\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.has_base_var","title":"<code>has_base_var(x)</code>","text":"<p>Check whether the prepared variable has the given base variable.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str | Self</code> <p>The base variable to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the prepared variable has the given base variable.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>def has_base_var(self, x: str | Self) -&gt; bool:\n    \"\"\"\n    Check whether the prepared variable has the given base variable.\n\n    Parameters:\n        x: The base variable to check.\n\n    Returns:\n        Whether the prepared variable has the given base variable.\n    \"\"\"\n    return PreparedVariableName.same_base_var(self, x)\n</code></pre>"},{"location":"reference/src/sawmill/variable_name/prepared_variable_name/#src.sawmill.variable_name.prepared_variable_name.PreparedVariableName.same_base_var","title":"<code>same_base_var(var1, var2)</code>  <code>staticmethod</code>","text":"<p>Check whether two prepared variables have the same base variable.</p> <p>Parameters:</p> Name Type Description Default <code>var1</code> <code>str | Self</code> <p>The first variable to check.</p> required <code>var2</code> <code>str | Self</code> <p>The second variable to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the two variables have the same base variable.</p> Source code in <code>src/sawmill/variable_name/prepared_variable_name.py</code> <pre><code>@staticmethod\ndef same_base_var(var1: str | Self, var2: str | Self) -&gt; bool:\n    \"\"\"\n    Check whether two prepared variables have the same base variable.\n\n    Parameters:\n        var1: The first variable to check.\n        var2: The second variable to check.\n\n    Returns:\n        Whether the two variables have the same base variable.\n    \"\"\"\n\n    if isinstance(var1, str):\n        var1 = PreparedVariableName(var1)\n    if isinstance(var2, str):\n        var2 = PreparedVariableName(var2)\n\n    return var1.base_var() == var2.base_var()\n</code></pre>"}]}